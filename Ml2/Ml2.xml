<?xml version="1.0"?>
<doc>
    <assembly>
        <name>Ml2</name>
    </assembly>
    <members>
        <member name="T:Ml2.Arff.IGetValue">
            <summary>
            Reflection is very slow, this is a simple way of avoiding reflection costs.
            </summary>
        </member>
        <member name="T:Ml2.Asstn.Apriori">
            <summary>
            Class implementing an Apriori-type algorithm. Iteratively reduces the
            minimum support until it finds the required number of rules with the given
            minimum confidence.<br/>The algorithm has an option to mine class association
            rules. It is adapted as explained in the second reference.<br/><br/>For
            more information see:<br/><br/>R. Agrawal, R. Srikant: Fast Algorithms for
            Mining Association Rules in Large Databases. In: 20th International Conference
            on Very Large Data Bases, 478-499, 1994.<br/><br/>Bing Liu, Wynne Hsu,
            Yiming Ma: Integrating Classification and Association Rule Mining. In: Fourth
            International Conference on Knowledge Discovery and Data Mining, 80-86,
            1998.<br/><br/>Options:<br/><br/>-N &lt;required number of rules output&gt; =
            	The required number of rules. (default = 10)<br/>-T &lt;0=confidence |
            1=lift | 2=leverage | 3=Conviction&gt; = 	The metric type by which to rank
            rules. (default = confidence)<br/>-C &lt;minimum metric score of a rule&gt; =
            	The minimum confidence of a rule. (default = 0.9)<br/>-D &lt;delta for
            minimum support&gt; = 	The delta by which the minimum support is decreased
            in<br/>	each iteration. (default = 0.05)<br/>-U &lt;upper bound for minimum
            support&gt; = 	Upper bound for minimum support. (default = 1.0)<br/>-M
            &lt;lower bound for minimum support&gt; = 	The lower bound for the minimum
            support. (default = 0.1)<br/>-S &lt;significance level&gt; = 	If used, rules are
            tested for significance at<br/>	the given level. Slower. (default = no
            significance testing)<br/>-I = 	If set the itemsets found are also output.
            (default = no)<br/>-R = 	Remove columns that contain all missing values
            (default = no)<br/>-V = 	Report progress iteratively. (default = no)<br/>-A = 	If
            set class association rules are mined. (default = no)<br/>-Z = 	Treat zero
            (i.e. first value of nominal attributes) as missing<br/>-B &lt;toString
            delimiters&gt; = 	If used, two characters to use as rule delimiters<br/>	in
            the result of toString: the first to delimit fields,<br/>	the second to
            delimit items within fields.<br/>	(default = traditional toString result)<br/>-c
            &lt;the class index&gt; = 	The class index. (default = last)
            </summary>
        </member>
        <member name="M:Ml2.Asstn.Apriori.MetricType(Ml2.Asstn.Apriori.EMetricType)">
            <summary>
            Set the type of metric by which to rank rules. Confidence is the
            proportion of the examples covered by the premise that are also covered by the
            consequence (Class association rules can only be mined using confidence). Lift
            is confidence divided by the proportion of all examples that are covered by
            the consequence. This is a measure of the importance of the association
            that is independent of support. Leverage is the proportion of additional
            examples covered by both the premise and consequence above those expected if
            the premise and consequence were independent of each other. The total number
            of examples that this represents is presented in brackets following the
            leverage. Conviction is another measure of departure from independence.
            Conviction is given by P(premise)P(!consequence) / P(premise, !consequence).
            </summary>    
        </member>
        <member name="M:Ml2.Asstn.Apriori.UpperBoundMinSupport(System.Double)">
            <summary>
            Upper bound for minimum support. Start iteratively decreasing minimum
            support from this value.
            </summary>    
        </member>
        <member name="M:Ml2.Asstn.Apriori.RemoveAllMissingCols(System.Boolean)">
            <summary>
            Remove columns with all missing values.
            </summary>    
        </member>
        <member name="M:Ml2.Asstn.Apriori.MinMetric(System.Double)">
            <summary>
            Minimum metric score. Consider only rules with scores higher than this
            value.
            </summary>    
        </member>
        <member name="M:Ml2.Asstn.Apriori.ClassIndex(System.Int32)">
            <summary>
            Index of the class attribute. If set to -1, the last attribute is taken
            as class attribute.
            </summary>    
        </member>
        <member name="M:Ml2.Asstn.Apriori.Car(System.Boolean)">
            <summary>
            If enabled class association rules are mined instead of (general)
            association rules.
            </summary>    
        </member>
        <member name="M:Ml2.Asstn.Apriori.LowerBoundMinSupport(System.Double)">
            <summary>
            Lower bound for minimum support.
            </summary>    
        </member>
        <member name="M:Ml2.Asstn.Apriori.NumRules(System.Int32)">
            <summary>
            Number of rules to find.
            </summary>    
        </member>
        <member name="M:Ml2.Asstn.Apriori.Delta(System.Double)">
            <summary>
            Iteratively decrease support by this factor. Reduces support until min
            support is reached or required number of rules has been generated.
            </summary>    
        </member>
        <member name="M:Ml2.Asstn.Apriori.SignificanceLevel(System.Double)">
            <summary>
            Significance level. Significance test (confidence metric only).
            </summary>    
        </member>
        <member name="M:Ml2.Asstn.Apriori.OutputItemSets(System.Boolean)">
            <summary>
            If enabled the itemsets are output as well.
            </summary>    
        </member>
        <member name="M:Ml2.Asstn.Apriori.Verbose(System.Boolean)">
            <summary>
            If enabled the algorithm will be run in verbose mode.
            </summary>    
        </member>
        <member name="M:Ml2.Asstn.Apriori.TreatZeroAsMissing(System.Boolean)">
            <summary>
            If enabled, zero (that is, the first value of a nominal) is treated in
            the same way as a missing value.
            </summary>    
        </member>
        <member name="M:Ml2.Asstn.Associations.Apriori">
            <summary>
            Class implementing an Apriori-type algorithm. Iteratively reduces the
            minimum support until it finds the required number of rules with the given
            minimum confidence.<br/>The algorithm has an option to mine class association
            rules. It is adapted as explained in the second reference.<br/><br/>For
            more information see:<br/><br/>R. Agrawal, R. Srikant: Fast Algorithms for
            Mining Association Rules in Large Databases. In: 20th International Conference
            on Very Large Data Bases, 478-499, 1994.<br/><br/>Bing Liu, Wynne Hsu,
            Yiming Ma: Integrating Classification and Association Rule Mining. In: Fourth
            International Conference on Knowledge Discovery and Data Mining, 80-86,
            1998.<br/><br/>Options:<br/><br/>-N &lt;required number of rules output&gt; =
            	The required number of rules. (default = 10)<br/>-T &lt;0=confidence |
            1=lift | 2=leverage | 3=Conviction&gt; = 	The metric type by which to rank
            rules. (default = confidence)<br/>-C &lt;minimum metric score of a rule&gt; =
            	The minimum confidence of a rule. (default = 0.9)<br/>-D &lt;delta for
            minimum support&gt; = 	The delta by which the minimum support is decreased
            in<br/>	each iteration. (default = 0.05)<br/>-U &lt;upper bound for minimum
            support&gt; = 	Upper bound for minimum support. (default = 1.0)<br/>-M
            &lt;lower bound for minimum support&gt; = 	The lower bound for the minimum
            support. (default = 0.1)<br/>-S &lt;significance level&gt; = 	If used, rules are
            tested for significance at<br/>	the given level. Slower. (default = no
            significance testing)<br/>-I = 	If set the itemsets found are also output.
            (default = no)<br/>-R = 	Remove columns that contain all missing values
            (default = no)<br/>-V = 	Report progress iteratively. (default = no)<br/>-A = 	If
            set class association rules are mined. (default = no)<br/>-Z = 	Treat zero
            (i.e. first value of nominal attributes) as missing<br/>-B &lt;toString
            delimiters&gt; = 	If used, two characters to use as rule delimiters<br/>	in
            the result of toString: the first to delimit fields,<br/>	the second to
            delimit items within fields.<br/>	(default = traditional toString result)<br/>-c
            &lt;the class index&gt; = 	The class index. (default = last)
            </summary>
        </member>
        <member name="M:Ml2.Asstn.Associations.FPGrowth">
            <summary>
            Class implementing the FP-growth algorithm for finding large item sets
            without candidate generation. Iteratively reduces the minimum support until
            it finds the required number of rules with the given minimum metric. For
            more information see:<br/><br/>J. Han, J.Pei, Y. Yin: Mining frequent patterns
            without candidate generation. In: Proceedings of the 2000 ACM-SIGMID
            International Conference on Management of Data, 1-12,
            2000.<br/><br/>Options:<br/><br/>-P &lt;attribute index of positive value&gt; = 	Set the index of the
            attribute value to consider as 'positive'<br/>	for binary attributes in
            normal dense instances. Index 2 is always<br/>	used for sparse instances.
            (default = 2)<br/>-I &lt;max items&gt; = 	The maximum number of items to
            include in large items sets (and rules). (default = -1, i.e. no limit.)<br/>-N
            &lt;require number of rules&gt; = 	The required number of rules. (default =
            10)<br/>-T &lt;0=confidence | 1=lift | 2=leverage | 3=Conviction&gt; = 	The
            metric by which to rank rules. (default = confidence)<br/>-C &lt;minimum
            metric score of a rule&gt; = 	The minimum metric score of a rule. (default =
            0.9)<br/>-U &lt;upper bound for minimum support&gt; = 	Upper bound for
            minimum support as a fraction or number of instances. (default = 1.0)<br/>-M
            &lt;lower bound for minimum support&gt; = 	The lower bound for the minimum
            support as a fraction or number of instances. (default = 0.1)<br/>-D
            &lt;delta for minimum support&gt; = 	The delta by which the minimum support is
            decreased in<br/>	each iteration as a fraction or number of instances.
            (default = 0.05)<br/>-S = 	Find all rules that meet the lower bound
            on<br/>	minimum support and the minimum metric constraint.<br/>	Turning this mode on will
            disable the iterative support reduction<br/>	procedure to find the
            specified number of rules.<br/>-transactions &lt;comma separated list of attribute
            names&gt; = 	Only consider transactions that contain these items (default
            = no restriction)<br/>-rules &lt;comma separated list of attribute
            names&gt; = 	Only print rules that contain these items. (default = no
            restriction)<br/>-use-or = 	Use OR instead of AND for must contain list(s). Use in
            conjunction<br/>	with -transactions and/or -rules
            </summary>
        </member>
        <member name="M:Ml2.Asstn.Associations.FilteredAssociator">
            <summary>
            Class for running an arbitrary associator on data that has been passed
            through an arbitrary filter. Like the associator, the structure of the filter
            is based exclusively on the training data and test instances will be
            processed by the filter without changing their
            structure.<br/><br/>Options:<br/><br/>-F &lt;filter specification&gt; = 	Full class name of filter to use,
            followed<br/>	by filter options.<br/>	eg:
            "weka.filters.unsupervised.attribute.Remove -V -R 1,2"<br/>	(default: weka.filters.MultiFilter
            with<br/>	weka.filters.unsupervised.attribute.ReplaceMissingValues)<br/>-c &lt;the class
            index&gt; = 	The class index.<br/>	(default: -1, i.e. unset)<br/>-W = 	Full
            name of base associator.<br/>	(default:
            weka.associations.Apriori)<br/><br/>Options specific to associator weka.associations.Apriori: = <br/>-N
            &lt;required number of rules output&gt; = 	The required number of rules.
            (default = 10)<br/>-T &lt;0=confidence | 1=lift | 2=leverage | 3=Conviction&gt; =
            	The metric type by which to rank rules. (default = confidence)<br/>-C
            &lt;minimum metric score of a rule&gt; = 	The minimum confidence of a rule.
            (default = 0.9)<br/>-D &lt;delta for minimum support&gt; = 	The delta by which
            the minimum support is decreased in<br/>	each iteration. (default =
            0.05)<br/>-U &lt;upper bound for minimum support&gt; = 	Upper bound for minimum
            support. (default = 1.0)<br/>-M &lt;lower bound for minimum support&gt; =
            	The lower bound for the minimum support. (default = 0.1)<br/>-S
            &lt;significance level&gt; = 	If used, rules are tested for significance at<br/>	the
            given level. Slower. (default = no significance testing)<br/>-I = 	If set the
            itemsets found are also output. (default = no)<br/>-R = 	Remove columns
            that contain all missing values (default = no)<br/>-V = 	Report progress
            iteratively. (default = no)<br/>-A = 	If set class association rules are mined.
            (default = no)<br/>-Z = 	Treat zero (i.e. first value of nominal
            attributes) as missing<br/>-B &lt;toString delimiters&gt; = 	If used, two characters
            to use as rule delimiters<br/>	in the result of toString: the first to
            delimit fields,<br/>	the second to delimit items within fields.<br/>	(default
            = traditional toString result)<br/>-c &lt;the class index&gt; = 	The class
            index. (default = last)
            </summary>
        </member>
        <member name="T:Ml2.Asstn.FilteredAssociator">
            <summary>
            Class for running an arbitrary associator on data that has been passed
            through an arbitrary filter. Like the associator, the structure of the filter
            is based exclusively on the training data and test instances will be
            processed by the filter without changing their
            structure.<br/><br/>Options:<br/><br/>-F &lt;filter specification&gt; = 	Full class name of filter to use,
            followed<br/>	by filter options.<br/>	eg:
            "weka.filters.unsupervised.attribute.Remove -V -R 1,2"<br/>	(default: weka.filters.MultiFilter
            with<br/>	weka.filters.unsupervised.attribute.ReplaceMissingValues)<br/>-c &lt;the class
            index&gt; = 	The class index.<br/>	(default: -1, i.e. unset)<br/>-W = 	Full
            name of base associator.<br/>	(default:
            weka.associations.Apriori)<br/><br/>Options specific to associator weka.associations.Apriori: = <br/>-N
            &lt;required number of rules output&gt; = 	The required number of rules.
            (default = 10)<br/>-T &lt;0=confidence | 1=lift | 2=leverage | 3=Conviction&gt; =
            	The metric type by which to rank rules. (default = confidence)<br/>-C
            &lt;minimum metric score of a rule&gt; = 	The minimum confidence of a rule.
            (default = 0.9)<br/>-D &lt;delta for minimum support&gt; = 	The delta by which
            the minimum support is decreased in<br/>	each iteration. (default =
            0.05)<br/>-U &lt;upper bound for minimum support&gt; = 	Upper bound for minimum
            support. (default = 1.0)<br/>-M &lt;lower bound for minimum support&gt; =
            	The lower bound for the minimum support. (default = 0.1)<br/>-S
            &lt;significance level&gt; = 	If used, rules are tested for significance at<br/>	the
            given level. Slower. (default = no significance testing)<br/>-I = 	If set the
            itemsets found are also output. (default = no)<br/>-R = 	Remove columns
            that contain all missing values (default = no)<br/>-V = 	Report progress
            iteratively. (default = no)<br/>-A = 	If set class association rules are mined.
            (default = no)<br/>-Z = 	Treat zero (i.e. first value of nominal
            attributes) as missing<br/>-B &lt;toString delimiters&gt; = 	If used, two characters
            to use as rule delimiters<br/>	in the result of toString: the first to
            delimit fields,<br/>	the second to delimit items within fields.<br/>	(default
            = traditional toString result)<br/>-c &lt;the class index&gt; = 	The class
            index. (default = last)
            </summary>
        </member>
        <member name="M:Ml2.Asstn.FilteredAssociator.Filter(Ml2.Fltr.IBaseFilter{weka.filters.Filter})">
            <summary>
            The filter to be used.
            </summary>    
        </member>
        <member name="M:Ml2.Asstn.FilteredAssociator.ClassIndex(System.Int32)">
            <summary>
            Index of the class attribute. If set to -1, the last attribute is taken
            as class attribute.
            </summary>    
        </member>
        <member name="M:Ml2.Asstn.FilteredAssociator.Associator(Ml2.Asstn.BaseAssociation{weka.associations.AbstractAssociator})">
            <summary>
            The base associator to be used.
            </summary>    
        </member>
        <member name="T:Ml2.Asstn.FPGrowth">
            <summary>
            Class implementing the FP-growth algorithm for finding large item sets
            without candidate generation. Iteratively reduces the minimum support until
            it finds the required number of rules with the given minimum metric. For
            more information see:<br/><br/>J. Han, J.Pei, Y. Yin: Mining frequent patterns
            without candidate generation. In: Proceedings of the 2000 ACM-SIGMID
            International Conference on Management of Data, 1-12,
            2000.<br/><br/>Options:<br/><br/>-P &lt;attribute index of positive value&gt; = 	Set the index of the
            attribute value to consider as 'positive'<br/>	for binary attributes in
            normal dense instances. Index 2 is always<br/>	used for sparse instances.
            (default = 2)<br/>-I &lt;max items&gt; = 	The maximum number of items to
            include in large items sets (and rules). (default = -1, i.e. no limit.)<br/>-N
            &lt;require number of rules&gt; = 	The required number of rules. (default =
            10)<br/>-T &lt;0=confidence | 1=lift | 2=leverage | 3=Conviction&gt; = 	The
            metric by which to rank rules. (default = confidence)<br/>-C &lt;minimum
            metric score of a rule&gt; = 	The minimum metric score of a rule. (default =
            0.9)<br/>-U &lt;upper bound for minimum support&gt; = 	Upper bound for
            minimum support as a fraction or number of instances. (default = 1.0)<br/>-M
            &lt;lower bound for minimum support&gt; = 	The lower bound for the minimum
            support as a fraction or number of instances. (default = 0.1)<br/>-D
            &lt;delta for minimum support&gt; = 	The delta by which the minimum support is
            decreased in<br/>	each iteration as a fraction or number of instances.
            (default = 0.05)<br/>-S = 	Find all rules that meet the lower bound
            on<br/>	minimum support and the minimum metric constraint.<br/>	Turning this mode on will
            disable the iterative support reduction<br/>	procedure to find the
            specified number of rules.<br/>-transactions &lt;comma separated list of attribute
            names&gt; = 	Only consider transactions that contain these items (default
            = no restriction)<br/>-rules &lt;comma separated list of attribute
            names&gt; = 	Only print rules that contain these items. (default = no
            restriction)<br/>-use-or = 	Use OR instead of AND for must contain list(s). Use in
            conjunction<br/>	with -transactions and/or -rules
            </summary>
        </member>
        <member name="M:Ml2.Asstn.FPGrowth.PositiveIndex(System.Int32)">
            <summary>
            Set the index of binary valued attributes that is to be considered the
            positive index. Has no effect for sparse data (in this case the first index
            (i.e. non-zero values) is always treated as positive. Also has no effect for
            unary valued attributes (i.e. when using the Weka Apriori-style format for
            market basket data, which uses missing value "?" to indicate absence of an
            item.
            </summary>    
        </member>
        <member name="M:Ml2.Asstn.FPGrowth.MaxNumberOfItems(System.Int32)">
            <summary>
            The maximum number of items to include in frequent item sets. -1 means no
            limit.
            </summary>    
        </member>
        <member name="M:Ml2.Asstn.FPGrowth.NumRulesToFind(System.Int32)">
            <summary>
            The number of rules to output
            </summary>    
        </member>
        <member name="M:Ml2.Asstn.FPGrowth.MinMetric(System.Double)">
            <summary>
            Minimum metric score. Consider only rules with scores higher than this
            value.
            </summary>    
        </member>
        <member name="M:Ml2.Asstn.FPGrowth.Delta(System.Double)">
            <summary>
            Iteratively decrease support by this factor. Reduces support until min
            support is reached or required number of rules has been generated.
            </summary>    
        </member>
        <member name="M:Ml2.Asstn.FPGrowth.LowerBoundMinSupport(System.Double)">
            <summary>
            Lower bound for minimum support as a fraction or number of instances.
            </summary>    
        </member>
        <member name="M:Ml2.Asstn.FPGrowth.UpperBoundMinSupport(System.Double)">
            <summary>
            Upper bound for minimum support as a fraction or number of instances.
            Start iteratively decreasing minimum support from this value.
            </summary>    
        </member>
        <member name="M:Ml2.Asstn.FPGrowth.TransactionsMustContain(System.String)">
            <summary>
            Limit input to FPGrowth to those transactions (instances) that contain
            these items. Provide a comma separated list of attribute names.
            </summary>    
        </member>
        <member name="M:Ml2.Asstn.FPGrowth.RulesMustContain(System.String)">
            <summary>
            Only print rules that contain these items. Provide a comma separated list
            of attribute names.
            </summary>    
        </member>
        <member name="M:Ml2.Asstn.FPGrowth.UseORForMustContainList(System.Boolean)">
            <summary>
            Use OR instead of AND for transactions/rules must contain lists.
            </summary>    
        </member>
        <member name="M:Ml2.Asstn.FPGrowth.FindAllRulesForSupportLevel(System.Boolean)">
            <summary>
            Find all rules that meet the lower bound on minimum support and the
            minimum metric constraint. Turning this mode on will disable the iterative
            support reduction procedure to find the specified number of rules.
            </summary>    
        </member>
        <member name="M:Ml2.Asstn.FPGrowth.OffDiskReportingFrequency(System.Int32)">
            <summary>
            
            </summary>    
        </member>
        <member name="T:Ml2.AttrSel.Evals.LatentSemanticAnalysis">
            <summary>
            Performs latent semantic analysis and transformation of the data. Use in
            conjunction with a Ranker search. A low-rank approximation of the full data
            is found by either specifying the number of singular values to use or
            specifying a proportion of the singular values to
            cover.<br/><br/>Options:<br/><br/>-N = 	Normalize input data.<br/>-R = 	Rank approximation used in LSA.
            <br/>	May be actual number of LSA attributes <br/>	to include (if greater
            than 1) or a <br/>	proportion of total singular values to <br/>	account for
            (if between 0 and 1). <br/>	A value less than or equal to zero means
            <br/>	use all latent variables.(default = 0.95)<br/>-A = 	Maximum number of
            attributes to include<br/>	in transformed attribute names.<br/>	(-1 = include
            all)
            </summary>
        </member>
        <member name="M:Ml2.AttrSel.Evals.LatentSemanticAnalysis.Rank(System.Double)">
            <summary>
            Matrix rank to use for data reduction. Can be a proportion to indicate
            desired coverage
            </summary>    
        </member>
        <member name="M:Ml2.AttrSel.Evals.LatentSemanticAnalysis.MaximumAttributeNames(System.Int32)">
            <summary>
            The maximum number of attributes to include in transformed attribute
            names.
            </summary>    
        </member>
        <member name="M:Ml2.AttrSel.Evals.LatentSemanticAnalysis.Normalize(System.Boolean)">
            <summary>
            Normalize input data.
            </summary>    
        </member>
        <member name="T:Ml2.Clss.DelegatingClassifier`1">
            <summary>
            Delegates classification to an instance of any weka Classifier.
            </summary>
            <typeparam name="T">The classifier to delegate classification to.</typeparam>
        </member>
        <member name="P:Ml2.Clss.ClassifiersBayes.BayesNet">
            <summary>
            Bayes Network learning using various search algorithms and quality
            measures.<br/>Base class for a Bayes Network classifier. Provides datastructures
            (network structure, conditional probability distributions, etc.) and
            facilities common to Bayes Network learning algorithms like K2 and
            B.<br/><br/>For more information
            see:<br/><br/>http://www.cs.waikato.ac.nz/~remco/weka.pdf<br/><br/>Options:<br/><br/>-D = 	Do not use ADTree data
            structure<br/><br/>-B &lt;BIF file&gt; = 	BIF file to compare with<br/><br/>-Q
            weka.classifiers.bayes.net.search.SearchAlgorithm = 	Search algorithm<br/><br/>-E
            weka.classifiers.bayes.net.estimate.SimpleEstimator = 	Estimator algorithm<br/>
            </summary>
        </member>
        <member name="P:Ml2.Clss.ClassifiersBayes.NaiveBayes">
            <summary>
            Class for a Naive Bayes classifier using estimator classes. Numeric
            estimator precision values are chosen based on analysis of the training data.
            For this reason, the classifier is not an UpdateableClassifier (which in
            typical usage are initialized with zero training instances) -- if you need the
            UpdateableClassifier functionality, use the NaiveBayesUpdateable
            classifier. The NaiveBayesUpdateable classifier will use a default precision of 0.1
            for numeric attributes when buildClassifier is called with zero training
            instances.<br/><br/>For more information on Naive Bayes classifiers,
            see<br/><br/>George H. John, Pat Langley: Estimating Continuous Distributions in
            Bayesian Classifiers. In: Eleventh Conference on Uncertainty in Artificial
            Intelligence, San Mateo, 338-345, 1995.<br/><br/>Options:<br/><br/>-K = 	Use
            kernel density estimator rather than normal<br/>	distribution for numeric
            attributes<br/>-D = 	Use supervised discretization to process numeric
            attributes<br/><br/>-O = 	Display model in old format (good when there are many
            classes)<br/>
            </summary>
        </member>
        <member name="P:Ml2.Clss.ClassifiersBayes.NaiveBayesMultinomial">
            <summary>
            Class for building and using a multinomial Naive Bayes classifier. For
            more information see,<br/><br/>Andrew Mccallum, Kamal Nigam: A Comparison of
            Event Models for Naive Bayes Text Classification. In: AAAI-98 Workshop on
            'Learning for Text Categorization', 1998.<br/><br/>The core equation for
            this classifier:<br/><br/>P[Ci|D] = (P[D|Ci] x P[Ci]) / P[D] (Bayes
            rule)<br/><br/>where Ci is class i and D is a document.<br/><br/>Options:<br/><br/>-D
            = 	If set, classifier is run in debug mode and<br/>	may output additional
            info to the console
            </summary>
        </member>
        <member name="P:Ml2.Clss.ClassifiersBayes.NaiveBayesMultinomialText">
            <summary>
            Multinomial naive bayes for text data. Operates directly (and only) on
            String attributes. Other types of input attributes are accepted but ignored
            during training and classification<br/><br/>Options:<br/><br/>-W = 	Use word
            frequencies instead of binary bag of words.<br/>-P &lt;# instances&gt; =
            	How often to prune the dictionary of low frequency words (default = 0, i.e.
            don't prune)<br/>-M &lt;double&gt; = 	Minimum word frequency. Words with
            less than this frequence are ignored.<br/>	If periodic pruning is turned on
            then this is also used to determine which<br/>	words to remove from the
            dictionary (default = 3).<br/>-normalize = 	Normalize document length (use in
            conjunction with -norm and -lnorm)<br/>-norm &lt;num&gt; = 	Specify the
            norm that each instance must have (default 1.0)<br/>-lnorm &lt;num&gt; =
            	Specify L-norm to use (default 2.0)<br/>-lowercase = 	Convert all tokens to
            lowercase before adding to the dictionary.<br/>-stoplist = 	Ignore words that
            are in the stoplist.<br/>-stopwords &lt;file&gt; = 	A file containing
            stopwords to override the default ones.<br/>	Using this option automatically
            sets the flag ('-stoplist') to use the<br/>	stoplist if the file
            exists.<br/>	Format: one stopword per line, lines starting with '#'<br/>	are interpreted
            as comments and ignored.<br/>-tokenizer &lt;spec&gt; = 	The tokenizing
            algorihtm (classname plus parameters) to use.<br/>	(default:
            weka.core.tokenizers.WordTokenizer)<br/>-stemmer &lt;spec&gt; = 	The stemmering algorihtm
            (classname plus parameters) to use.
            </summary>
        </member>
        <member name="P:Ml2.Clss.ClassifiersBayes.NaiveBayesMultinomialUpdateable">
            <summary>
            Class for building and using a multinomial Naive Bayes classifier. For
            more information see,<br/><br/>Andrew Mccallum, Kamal Nigam: A Comparison of
            Event Models for Naive Bayes Text Classification. In: AAAI-98 Workshop on
            'Learning for Text Categorization', 1998.<br/><br/>The core equation for
            this classifier:<br/><br/>P[Ci|D] = (P[D|Ci] x P[Ci]) / P[D] (Bayes
            rule)<br/><br/>where Ci is class i and D is a document.<br/><br/>Incremental version
            of the algorithm.<br/><br/>Options:<br/><br/>-D = 	If set, classifier is
            run in debug mode and<br/>	may output additional info to the console
            </summary>
        </member>
        <member name="P:Ml2.Clss.ClassifiersBayes.NaiveBayesUpdateable">
            <summary>
            Class for a Naive Bayes classifier using estimator classes. This is the
            updateable version of NaiveBayes.<br/>This classifier will use a default
            precision of 0.1 for numeric attributes when buildClassifier is called with
            zero training instances.<br/><br/>For more information on Naive Bayes
            classifiers, see<br/><br/>George H. John, Pat Langley: Estimating Continuous
            Distributions in Bayesian Classifiers. In: Eleventh Conference on Uncertainty in
            Artificial Intelligence, San Mateo, 338-345,
            1995.<br/><br/>Options:<br/><br/>-K = 	Use kernel density estimator rather than normal<br/>	distribution
            for numeric attributes<br/>-D = 	Use supervised discretization to process
            numeric attributes<br/><br/>-O = 	Display model in old format (good when
            there are many classes)<br/>
            </summary>
        </member>
        <member name="P:Ml2.Clss.ClassifiersBayesNet.BIFReader">
            <summary>
            Builds a description of a Bayes Net classifier stored in XML BIF 0.3
            format.<br/><br/>For more details on XML BIF see:<br/><br/>Fabio Cozman, Marek
            Druzdzel, Daniel Garcia (1998). XML BIF version 0.3. URL
            http://www-2.cs.cmu.edu/~fgcozman/Research/InterchangeFormat/.<br/><br/>Options:<br/><br/>-D
            = 	Do not use ADTree data structure<br/><br/>-B &lt;BIF file&gt; = 	BIF
            file to compare with<br/><br/>-Q
            weka.classifiers.bayes.net.search.SearchAlgorithm = 	Search algorithm<br/><br/>-E
            weka.classifiers.bayes.net.estimate.SimpleEstimator = 	Estimator algorithm<br/>
            </summary>
        </member>
        <member name="P:Ml2.Clss.ClassifiersBayesNet.BayesNetGenerator">
            <summary>
            Bayes Network learning using various search algorithms and quality
            measures.<br/>Base class for a Bayes Network classifier. Provides datastructures
            (network structure, conditional probability distributions, etc.) and
            facilities common to Bayes Network learning algorithms like K2 and
            B.<br/><br/>For more information
            see:<br/><br/>http://www.cs.waikato.ac.nz/~remco/weka.pdf<br/><br/>Options:<br/><br/>-B = 	Generate network (instead of
            instances)<br/><br/>-N &lt;integer&gt; = 	Nr of nodes<br/><br/>-A &lt;integer&gt; =
            	Nr of arcs<br/><br/>-M &lt;integer&gt; = 	Nr of instances<br/><br/>-C
            &lt;integer&gt; = 	Cardinality of the variables<br/><br/>-S &lt;integer&gt; =
            	Seed for random number generator<br/><br/>-F &lt;file&gt; = 	The BIF file to
            obtain the structure from.<br/>
            </summary>
        </member>
        <member name="P:Ml2.Clss.ClassifiersBayesNet.EditableBayesNet">
            <summary>
            Bayes Network learning using various search algorithms and quality
            measures.<br/>Base class for a Bayes Network classifier. Provides datastructures
            (network structure, conditional probability distributions, etc.) and
            facilities common to Bayes Network learning algorithms like K2 and
            B.<br/><br/>For more information
            see:<br/><br/>http://www.cs.waikato.ac.nz/~remco/weka.pdf<br/><br/>Options:<br/><br/>-D = 	Do not use ADTree data
            structure<br/><br/>-B &lt;BIF file&gt; = 	BIF file to compare with<br/><br/>-Q
            weka.classifiers.bayes.net.search.SearchAlgorithm = 	Search algorithm<br/><br/>-E
            weka.classifiers.bayes.net.estimate.SimpleEstimator = 	Estimator algorithm<br/>
            </summary>
        </member>
        <member name="P:Ml2.Clss.ClassifiersFunctions.GaussianProcesses">
            <summary>
            Implements Gaussian processes for regression without
            hyperparameter-tuning. To make choosing an appropriate noise level easier, this implementation
            applies normalization/standardization to the target attribute as well as
            the other attributes (if normalization/standardizaton is turned on). Missing
            values are replaced by the global mean/mode. Nominal attributes are
            converted to binary ones. Note that kernel caching is turned off if the kernel
            used implements CachedKernel.<br/><br/>Options:<br/><br/>-D = 	If set,
            classifier is run in debug mode and<br/>	may output additional info to the
            console<br/>-L &lt;double&gt; = 	Level of Gaussian Noise wrt transformed target.
            (default 1)<br/>-N = 	Whether to 0=normalize/1=standardize/2=neither.
            (default 0=normalize)<br/>-K &lt;classname and parameters&gt; = 	The Kernel to
            use.<br/>	(default:
            weka.classifiers.functions.supportVector.PolyKernel)<br/><br/>Options specific to kernel
            weka.classifiers.functions.supportVector.PolyKernel: = <br/>-D = 	Enables debugging output (if available) to be
            printed.<br/>	(default: off)<br/>-no-checks = 	Turns off all checks - use with
            caution!<br/>	(default: checks on)<br/>-C &lt;num&gt; = 	The size of the
            cache (a prime number), 0 for full cache and <br/>	-1 to turn it
            off.<br/>	(default: 250007)<br/>-E &lt;num&gt; = 	The Exponent to use.<br/>	(default:
            1.0)<br/>-L = 	Use lower-order terms.<br/>	(default: no)
            </summary>
        </member>
        <member name="P:Ml2.Clss.ClassifiersFunctions.LinearRegression">
            <summary>
            Class for using linear regression for prediction. Uses the Akaike
            criterion for model selection, and is able to deal with weighted
            instances.<br/><br/>Options:<br/><br/>-D = 	Produce debugging output.<br/>	(default no
            debugging output)<br/>-S &lt;number of selection method&gt; = 	Set the attribute
            selection method to use. 1 = None, 2 = Greedy.<br/>	(default 0 = M5'
            method)<br/>-C = 	Do not try to eliminate colinear attributes.<br/><br/>-R
            &lt;double&gt; = 	Set ridge parameter (default 1.0e-8).<br/><br/>-minimal =
            	Conserve memory, don't keep dataset header and means/stdevs.<br/>	Model cannot
            be printed out if this option is enabled.	(default: keep data)
            </summary>
        </member>
        <member name="P:Ml2.Clss.ClassifiersFunctions.Logistic">
            <summary>
            Class for building and using a multinomial logistic regression model with
            a ridge estimator.<br/><br/>There are some modifications, however,
            compared to the paper of leCessie and van Houwelingen(1992): <br/><br/>If there
            are k classes for n instances with m attributes, the parameter matrix B to be
            calculated will be an m*(k-1) matrix.<br/><br/>The probability for class j
            with the exception of the last class is<br/><br/>Pj(Xi) =
            exp(XiBj)/((sum[j=1..(k-1)]exp(Xi*Bj))+1) <br/><br/>The last class has
            probability<br/><br/>1-(sum[j=1..(k-1)]Pj(Xi)) <br/>	=
            1/((sum[j=1..(k-1)]exp(Xi*Bj))+1)<br/><br/>The (negative) multinomial log-likelihood is thus: <br/><br/>L =
            -sum[i=1..n]{<br/>	sum[j=1..(k-1)](Yij * ln(Pj(Xi)))<br/>	+(1 -
            (sum[j=1..(k-1)]Yij)) <br/>	* ln(1 - sum[j=1..(k-1)]Pj(Xi))<br/>	} + ridge *
            (B^2)<br/><br/>In order to find the matrix B for which L is minimised, a Quasi-Newton
            Method is used to search for the optimized values of the m*(k-1) variables. Note
            that before we use the optimization procedure, we 'squeeze' the matrix B
            into a m*(k-1) vector. For details of the optimization procedure, please
            check weka.core.Optimization class.<br/><br/>Although original Logistic
            Regression does not deal with instance weights, we modify the algorithm a little
            bit to handle the instance weights.<br/><br/>For more information
            see:<br/><br/>le Cessie, S., van Houwelingen, J.C. (1992). Ridge Estimators in
            Logistic Regression. Applied Statistics. 41(1):191-201.<br/><br/>Note: Missing
            values are replaced using a ReplaceMissingValuesFilter, and nominal
            attributes are transformed into numeric attributes using a
            NominalToBinaryFilter.<br/><br/>Options:<br/><br/>-D = 	Turn on debugging output.<br/>-C = 	Use
            conjugate gradient descent rather than BFGS updates.<br/>-R &lt;ridge&gt; =
            	Set the ridge in the log-likelihood.<br/>-M &lt;number&gt; = 	Set the
            maximum number of iterations (default -1, until convergence).
            </summary>
        </member>
        <member name="P:Ml2.Clss.ClassifiersFunctions.MultilayerPerceptron">
            <summary>
            A Classifier that uses backpropagation to classify instances.<br/>This
            network can be built by hand, created by an algorithm or both. The network
            can also be monitored and modified during training time. The nodes in this
            network are all sigmoid (except for when the class is numeric in which case
            the the output nodes become unthresholded linear
            units).<br/><br/>Options:<br/><br/>-L &lt;learning rate&gt; = 	Learning Rate for the backpropagation
            algorithm.<br/>	(Value should be between 0 - 1, Default = 0.3).<br/>-M
            &lt;momentum&gt; = 	Momentum Rate for the backpropagation algorithm.<br/>	(Value
            should be between 0 - 1, Default = 0.2).<br/>-N &lt;number of epochs&gt; =
            	Number of epochs to train through.<br/>	(Default = 500).<br/>-V
            &lt;percentage size of validation set&gt; = 	Percentage size of validation set to
            use to terminate<br/>	training (if this is non zero it can pre-empt num of
            epochs.<br/>	(Value should be between 0 - 100, Default = 0).<br/>-S
            &lt;seed&gt; = 	The value used to seed the random number generator<br/>	(Value
            should be >= 0 and and a long, Default = 0).<br/>-E &lt;threshold for number of
            consequetive errors&gt; = 	The consequetive number of errors allowed for
            validation<br/>	testing before the netwrok terminates.<br/>	(Value should be
            > 0, Default = 20).<br/>-G = 	GUI will be opened.<br/>	(Use this to bring
            up a GUI).<br/>-A = 	Autocreation of the network connections will NOT be
            done.<br/>	(This will be ignored if -G is NOT set)<br/>-B = 	A NominalToBinary
            filter will NOT automatically be used.<br/>	(Set this to not use a
            NominalToBinary filter).<br/>-H &lt;comma seperated numbers for nodes on each
            layer&gt; = 	The hidden layers to be created for the network.<br/>	(Value
            should be a list of comma separated Natural <br/>	numbers or the letters 'a' =
            (attribs + classes) / 2, <br/>	'i' = attribs, 'o' = classes, 't' = attribs
            .+ classes)<br/>	for wildcard values, Default = a).<br/>-C = 	Normalizing a
            numeric class will NOT be done.<br/>	(Set this to not normalize the class
            if it's numeric).<br/>-I = 	Normalizing the attributes will NOT be
            done.<br/>	(Set this to not normalize the attributes).<br/>-R = 	Reseting the
            network will NOT be allowed.<br/>	(Set this to not allow the network to
            reset).<br/>-D = 	Learning rate decay will occur.<br/>	(Set this to cause the
            learning rate to decay).
            </summary>
        </member>
        <member name="P:Ml2.Clss.ClassifiersFunctions.SGD">
            <summary>
            Implements stochastic gradient descent for learning various linear models
            (binary class SVM, binary class logistic regression and linear
            regression). Globally replaces all missing values and transforms nominal attributes
            into binary ones. It also normalizes all attributes, so the coefficients in
            the output are based on the normalized data.<br/>For numeric class
            attributes, the squared loss function (2) must be
            used.<br/><br/>Options:<br/><br/>-F = 	Set the loss function to minimize. 0 = hinge loss (SVM), 1 = log loss
            (logistic regression),<br/>	2 = squared loss (regression).<br/>	(default =
            0)<br/>-L = 	The learning rate. If normalization is<br/>	turned off (as it
            is automatically for streaming data), then the<br/>	default learning rate
            will need to be reduced (try 0.0001).<br/>	(default = 0.01).<br/>-R
            &lt;double&gt; = 	The lambda regularization constant (default = 0.0001)<br/>-E
            &lt;integer&gt; = 	The number of epochs to perform (batch learning only,
            default = 500)<br/>-N = 	Don't normalize the data<br/>-M = 	Don't replace missing
            values
            </summary>
        </member>
        <member name="P:Ml2.Clss.ClassifiersFunctions.SGDText">
            <summary>
            Implements stochastic gradient descent for learning a linear binary class
            SVM or binary class logistic regression on text data. Operates directly
            (and only) on String attributes. Other types of input attributes are accepted
            but ignored during training and
            classification.<br/><br/>Options:<br/><br/>-F = 	Set the loss function to minimize. 0 = hinge loss (SVM), 1 = log
            loss (logistic regression)<br/>	(default = 0)<br/>-outputProbs = 	Output
            probabilities for SVMs (fits a logsitic<br/>	model to the output of the
            SVM)<br/>-L = 	The learning rate (default = 0.01).<br/>-R &lt;double&gt; = 	The
            lambda regularization constant (default = 0.0001)<br/>-E &lt;integer&gt; =
            	The number of epochs to perform (batch learning only, default = 500)<br/>-W =
            	Use word frequencies instead of binary bag of words.<br/>-P &lt;#
            instances&gt; = 	How often to prune the dictionary of low frequency words (default
            = 0, i.e. don't prune)<br/>-M &lt;double&gt; = 	Minimum word frequency.
            Words with less than this frequence are ignored.<br/>	If periodic pruning is
            turned on then this is also used to determine which<br/>	words to remove
            from the dictionary (default = 3).<br/>-normalize = 	Normalize document
            length (use in conjunction with -norm and -lnorm)<br/>-norm &lt;num&gt; =
            	Specify the norm that each instance must have (default 1.0)<br/>-lnorm
            &lt;num&gt; = 	Specify L-norm to use (default 2.0)<br/>-lowercase = 	Convert all
            tokens to lowercase before adding to the dictionary.<br/>-stoplist = 	Ignore
            words that are in the stoplist.<br/>-stopwords &lt;file&gt; = 	A file
            containing stopwords to override the default ones.<br/>	Using this option
            automatically sets the flag ('-stoplist') to use the<br/>	stoplist if the file
            exists.<br/>	Format: one stopword per line, lines starting with '#'<br/>	are
            interpreted as comments and ignored.<br/>-tokenizer &lt;spec&gt; = 	The
            tokenizing algorihtm (classname plus parameters) to use.<br/>	(default:
            weka.core.tokenizers.WordTokenizer)<br/>-stemmer &lt;spec&gt; = 	The stemmering
            algorihtm (classname plus parameters) to use.
            </summary>
        </member>
        <member name="P:Ml2.Clss.ClassifiersFunctions.SMO">
            <summary>
            Implements John Platt's sequential minimal optimization algorithm for
            training a support vector classifier.<br/><br/>This implementation globally
            replaces all missing values and transforms nominal attributes into binary
            ones. It also normalizes all attributes by default. (In that case the
            coefficients in the output are based on the normalized data, not the original data
            --- this is important for interpreting the
            classifier.)<br/><br/>Multi-class problems are solved using pairwise classification (1-vs-1 and if logistic
            models are built pairwise coupling according to Hastie and Tibshirani,
            1998).<br/><br/>To obtain proper probability estimates, use the option that
            fits logistic regression models to the outputs of the support vector machine.
            In the multi-class case the predicted probabilities are coupled using
            Hastie and Tibshirani's pairwise coupling method.<br/><br/>Note: for improved
            speed normalization should be turned off when operating on
            SparseInstances.<br/><br/>For more information on the SMO algorithm, see<br/><br/>J. Platt:
            Fast Training of Support Vector Machines using Sequential Minimal
            Optimization. In B. Schoelkopf and C. Burges and A. Smola, editors, Advances in
            Kernel Methods - Support Vector Learning, 1998.<br/><br/>S.S. Keerthi, S.K.
            Shevade, C. Bhattacharyya, K.R.K. Murthy (2001). Improvements to Platt's SMO
            Algorithm for SVM Classifier Design. Neural Computation.
            13(3):637-649.<br/><br/>Trevor Hastie, Robert Tibshirani: Classification by Pairwise Coupling.
            In: Advances in Neural Information Processing Systems,
            1998.<br/><br/>Options:<br/><br/>-D = 	If set, classifier is run in debug mode and<br/>	may
            output additional info to the console<br/>-no-checks = 	Turns off all checks
            - use with caution!<br/>	Turning them off assumes that data is purely
            numeric, doesn't<br/>	contain any missing values, and has a nominal class.
            Turning them<br/>	off also means that no header information will be stored if
            the<br/>	machine is linear. Finally, it also assumes that no instance
            has<br/>	a weight equal to 0.<br/>	(default: checks on)<br/>-C &lt;double&gt; =
            	The complexity constant C. (default 1)<br/>-N = 	Whether to
            0=normalize/1=standardize/2=neither. (default 0=normalize)<br/>-L &lt;double&gt; = 	The
            tolerance parameter. (default 1.0e-3)<br/>-P &lt;double&gt; = 	The epsilon for
            round-off error. (default 1.0e-12)<br/>-M = 	Fit logistic models to SVM
            outputs. <br/>-V &lt;double&gt; = 	The number of folds for the
            internal<br/>	cross-validation. (default -1, use training data)<br/>-W &lt;double&gt; =
            	The random number seed. (default 1)<br/>-K &lt;classname and parameters&gt;
            = 	The Kernel to use.<br/>	(default:
            weka.classifiers.functions.supportVector.PolyKernel)<br/><br/>Options specific to kernel
            weka.classifiers.functions.supportVector.PolyKernel: = <br/>-D = 	Enables debugging output (if
            available) to be printed.<br/>	(default: off)<br/>-no-checks = 	Turns off all
            checks - use with caution!<br/>	(default: checks on)<br/>-C &lt;num&gt; =
            	The size of the cache (a prime number), 0 for full cache and <br/>	-1 to
            turn it off.<br/>	(default: 250007)<br/>-E &lt;num&gt; = 	The Exponent to
            use.<br/>	(default: 1.0)<br/>-L = 	Use lower-order terms.<br/>	(default: no)
            </summary>
        </member>
        <member name="P:Ml2.Clss.ClassifiersFunctions.SMOreg">
            <summary>
            SMOreg implements the support vector machine for regression. The
            parameters can be learned using various algorithms. The algorithm is selected by
            setting the RegOptimizer. The most popular algorithm (RegSMOImproved) is due
            to Shevade, Keerthi et al and this is the default
            RegOptimizer.<br/><br/>For more information see:<br/><br/>S.K. Shevade, S.S. Keerthi, C.
            Bhattacharyya, K.R.K. Murthy: Improvements to the SMO Algorithm for SVM Regression.
            In: IEEE Transactions on Neural Networks, 1999.<br/><br/>A.J. Smola, B.
            Schoelkopf (1998). A tutorial on support vector
            regression.<br/><br/>Options:<br/><br/>-C &lt;double&gt; = 	The complexity constant C.<br/>	(default
            1)<br/>-N = 	Whether to 0=normalize/1=standardize/2=neither.<br/>	(default
            0=normalize)<br/>-I &lt;classname and parameters&gt; = 	Optimizer class used for
            solving quadratic optimization problem<br/>	(default
            weka.classifiers.functions.supportVector.RegSMOImproved)<br/>-K &lt;classname and parameters&gt;
            = 	The Kernel to use.<br/>	(default:
            weka.classifiers.functions.supportVector.PolyKernel)<br/><br/>Options specific to optimizer ('-I')
            weka.classifiers.functions.supportVector.RegSMOImproved: = <br/>-T &lt;double&gt; = 	The
            tolerance parameter for checking the stopping criterion.<br/>	(default
            0.001)<br/>-V = 	Use variant 1 of the algorithm when true, otherwise use
            variant 2.<br/>	(default true)<br/>-P &lt;double&gt; = 	The epsilon for round-off
            error.<br/>	(default 1.0e-12)<br/>-L &lt;double&gt; = 	The epsilon
            parameter in epsilon-insensitive loss function.<br/>	(default 1.0e-3)<br/>-W
            &lt;double&gt; = 	The random number seed.<br/>	(default 1)<br/><br/>Options
            specific to kernel ('-K') weka.classifiers.functions.supportVector.PolyKernel:
            = <br/>-D = 	Enables debugging output (if available) to be
            printed.<br/>	(default: off)<br/>-no-checks = 	Turns off all checks - use with
            caution!<br/>	(default: checks on)<br/>-C &lt;num&gt; = 	The size of the cache (a prime
            number), 0 for full cache and <br/>	-1 to turn it off.<br/>	(default:
            250007)<br/>-E &lt;num&gt; = 	The Exponent to use.<br/>	(default: 1.0)<br/>-L =
            	Use lower-order terms.<br/>	(default: no)
            </summary>
        </member>
        <member name="P:Ml2.Clss.ClassifiersFunctions.SimpleLinearRegression">
            <summary>
            Learns a simple linear regression model. Picks the attribute that results
            in the lowest squared error. Missing values are not allowed. Can only deal
            with numeric attributes.<br/><br/>Options:<br/><br/>-D = 	If set,
            classifier is run in debug mode and<br/>	may output additional info to the console
            </summary>
        </member>
        <member name="P:Ml2.Clss.ClassifiersFunctions.SimpleLogistic">
            <summary>
            Classifier for building linear logistic regression models. LogitBoost
            with simple regression functions as base learners is used for fitting the
            logistic models. The optimal number of LogitBoost iterations to perform is
            cross-validated, which leads to automatic attribute selection. For more
            information see:<br/>Niels Landwehr, Mark Hall, Eibe Frank (2005). Logistic Model
            Trees.<br/><br/>Marc Sumner, Eibe Frank, Mark Hall: Speeding up Logistic
            Model Tree Induction. In: 9th European Conference on Principles and Practice
            of Knowledge Discovery in Databases, 675-683,
            2005.<br/><br/>Options:<br/><br/>-I &lt;iterations&gt; = 	Set fixed number of iterations for
            LogitBoost<br/>-S = 	Use stopping criterion on training set (instead
            of<br/>	cross-validation)<br/>-P = 	Use error on probabilities (rmse) instead
            of<br/>	misclassification error for stopping criterion<br/>-M &lt;iterations&gt; = 	Set
            maximum number of boosting iterations<br/>-H &lt;iterations&gt; = 	Set
            parameter for heuristic for early stopping of<br/>	LogitBoost.<br/>	If enabled,
            the minimum is selected greedily, stopping<br/>	if the current minimum has
            not changed for iter iterations.<br/>	By default, heuristic is enabled with
            value 50. Set to<br/>	zero to disable heuristic.<br/>-W &lt;beta&gt; =
            	Set beta for weight trimming for LogitBoost. Set to 0 for no weight
            trimming.<br/><br/>-A = 	The AIC is used to choose the best iteration (instead of CV
            or training error).<br/>
            </summary>
        </member>
        <member name="P:Ml2.Clss.ClassifiersFunctions.VotedPerceptron">
            <summary>
            Implementation of the voted perceptron algorithm by Freund and Schapire.
            Globally replaces all missing values, and transforms nominal attributes
            into binary ones.<br/><br/>For more information, see:<br/><br/>Y. Freund, R.
            E. Schapire: Large margin classification using the perceptron algorithm. In:
            11th Annual Conference on Computational Learning Theory, New York, NY,
            209-217, 1998.<br/><br/>Options:<br/><br/>-I &lt;int&gt; = 	The number of
            iterations to be performed.<br/>	(default 1)<br/>-E &lt;double&gt; = 	The
            exponent for the polynomial kernel.<br/>	(default 1)<br/>-S &lt;int&gt; = 	The
            seed for the random number generation.<br/>	(default 1)<br/>-M &lt;int&gt; =
            	The maximum number of alterations allowed.<br/>	(default 10000)
            </summary>
        </member>
        <!-- Badly formed XML comment ignored for member "P:Ml2.Clss.ClassifiersFunctions.LibLINEAR" -->
        <member name="P:Ml2.Clss.ClassifiersLazy.IBk">
            <summary>
            K-nearest neighbours classifier. Can select appropriate value of K based
            on cross-validation. Can also do distance weighting.<br/><br/>For more
            information, see<br/><br/>D. Aha, D. Kibler (1991). Instance-based learning
            algorithms. Machine Learning. 6:37-66.<br/><br/>Options:<br/><br/>-I = 	Weight
            neighbours by the inverse of their distance<br/>	(use when k > 1)<br/>-F =
            	Weight neighbours by 1 - their distance<br/>	(use when k > 1)<br/>-K
            &lt;number of neighbors&gt; = 	Number of nearest neighbours (k) used in
            classification.<br/>	(Default = 1)<br/>-E = 	Minimise mean squared error rather
            than mean absolute<br/>	error when using -X option with numeric
            prediction.<br/>-W &lt;window size&gt; = 	Maximum number of training instances
            maintained.<br/>	Training instances are dropped FIFO. (Default = no window)<br/>-X =
            	Select the number of nearest neighbours between 1<br/>	and the k value
            specified using hold-one-out evaluation<br/>	on the training data (use when k
            > 1)<br/>-A = 	The nearest neighbour search algorithm to use (default:
            weka.core.neighboursearch.LinearNNSearch).<br/>
            </summary>
        </member>
        <member name="P:Ml2.Clss.ClassifiersLazy.KStar">
            <summary>
            K* is an instance-based classifier, that is the class of a test instance
            is based upon the class of those training instances similar to it, as
            determined by some similarity function. It differs from other instance-based
            learners in that it uses an entropy-based distance function.<br/><br/>For more
            information on K*, see<br/><br/>John G. Cleary, Leonard E. Trigg: K*: An
            Instance-based Learner Using an Entropic Distance Measure. In: 12th
            International Conference on Machine Learning, 108-114,
            1995.<br/><br/>Options:<br/><br/>-B &lt;num&gt; = 	Manual blend setting (default 20%)<br/><br/>-E =
            	Enable entropic auto-blend setting (symbolic class only)<br/><br/>-M
            &lt;char&gt; = 	Specify the missing value treatment mode (default a)<br/>	Valid
            options are: a(verage), d(elete), m(axdiff), n(ormal)<br/>
            </summary>
        </member>
        <member name="P:Ml2.Clss.ClassifiersLazy.LWL">
            <summary>
            Locally weighted learning. Uses an instance-based algorithm to assign
            instance weights which are then used by a specified
            WeightedInstancesHandler.<br/>Can do classification (e.g. using naive Bayes) or regression (e.g.
            using linear regression).<br/><br/>For more info, see<br/><br/>Eibe Frank, Mark
            Hall, Bernhard Pfahringer: Locally Weighted Naive Bayes. In: 19th
            Conference in Uncertainty in Artificial Intelligence, 249-256, 2003.<br/><br/>C.
            Atkeson, A. Moore, S. Schaal (1996). Locally weighted learning. AI
            Review..<br/><br/>Options:<br/><br/>-A = 	The nearest neighbour search algorithm to
            use (default: weka.core.neighboursearch.LinearNNSearch).<br/><br/>-K
            &lt;number of neighbours&gt; = 	Set the number of neighbours used to set the
            kernel bandwidth.<br/>	(default all)<br/>-U &lt;number of weighting method&gt; =
            	Set the weighting kernel shape to use. 0=Linear,
            1=Epanechnikov,<br/>	2=Tricube, 3=Inverse, 4=Gaussian.<br/>	(default 0 = Linear)<br/>-D = 	If set,
            classifier is run in debug mode and<br/>	may output additional info to the
            console<br/>-W = 	Full name of base classifier.<br/>	(default:
            weka.classifiers.trees.DecisionStump)<br/><br/>Options specific to classifier
            weka.classifiers.trees.DecisionStump: = <br/>-D = 	If set, classifier is run in
            debug mode and<br/>	may output additional info to the console
            </summary>
        </member>
        <member name="P:Ml2.Clss.ClassifiersMeta.AdaBoostM1">
            <summary>
            Class for boosting a nominal class classifier using the Adaboost M1
            method. Only nominal class problems can be tackled. Often dramatically improves
            performance, but sometimes overfits.<br/><br/>For more information,
            see<br/><br/>Yoav Freund, Robert E. Schapire: Experiments with a new boosting
            algorithm. In: Thirteenth International Conference on Machine Learning, San
            Francisco, 148-156, 1996.<br/><br/>Options:<br/><br/>-P &lt;num&gt; =
            	Percentage of weight mass to base training on.<br/>	(default 100, reduce to around
            90 speed up)<br/>-Q = 	Use resampling for boosting.<br/>-S &lt;num&gt; =
            	Random number seed.<br/>	(default 1)<br/>-I &lt;num&gt; = 	Number of
            iterations.<br/>	(default 10)<br/>-D = 	If set, classifier is run in debug mode
            and<br/>	may output additional info to the console<br/>-W = 	Full name of
            base classifier.<br/>	(default:
            weka.classifiers.trees.DecisionStump)<br/><br/>Options specific to classifier weka.classifiers.trees.DecisionStump: =
            <br/>-D = 	If set, classifier is run in debug mode and<br/>	may output
            additional info to the console
            </summary>
        </member>
        <member name="P:Ml2.Clss.ClassifiersMeta.AdditiveRegression">
            <summary>
            Meta classifier that enhances the performance of a regression base
            classifier. Each iteration fits a model to the residuals left by the classifier
            on the previous iteration. Prediction is accomplished by adding the
            predictions of each classifier. Reducing the shrinkage (learning rate) parameter
            helps prevent overfitting and has a smoothing effect but increases the
            learning time.<br/><br/>For more information see:<br/><br/>J.H. Friedman (1999).
            Stochastic Gradient Boosting.<br/><br/>Options:<br/><br/>-S = 	Specify
            shrinkage rate. (default = 1.0, ie. no shrinkage)<br/><br/>-I &lt;num&gt; =
            	Number of iterations.<br/>	(default 10)<br/>-D = 	If set, classifier is run
            in debug mode and<br/>	may output additional info to the console<br/>-W =
            	Full name of base classifier.<br/>	(default:
            weka.classifiers.trees.DecisionStump)<br/><br/>Options specific to classifier
            weka.classifiers.trees.DecisionStump: = <br/>-D = 	If set, classifier is run in debug mode and<br/>	may
            output additional info to the console
            </summary>
        </member>
        <member name="P:Ml2.Clss.ClassifiersMeta.AttributeSelectedClassifier">
            <summary>
            Dimensionality of training and test data is reduced by attribute
            selection before being passed on to a classifier.<br/><br/>Options:<br/><br/>-E
            &lt;attribute evaluator specification&gt; = 	Full class name of attribute
            evaluator, followed<br/>	by its options.<br/>	eg:
            "weka.attributeSelection.CfsSubsetEval -L"<br/>	(default weka.attributeSelection.CfsSubsetEval)<br/>-S
            &lt;search method specification&gt; = 	Full class name of search method,
            followed<br/>	by its options.<br/>	eg: "weka.attributeSelection.BestFirst -D
            1"<br/>	(default weka.attributeSelection.BestFirst)<br/>-D = 	If set,
            classifier is run in debug mode and<br/>	may output additional info to the
            console<br/>-W = 	Full name of base classifier.<br/>	(default:
            weka.classifiers.trees.J48)<br/><br/>Options specific to classifier
            weka.classifiers.trees.J48: = <br/>-U = 	Use unpruned tree.<br/>-O = 	Do not collapse tree.<br/>-C
            &lt;pruning confidence&gt; = 	Set confidence threshold for
            pruning.<br/>	(default 0.25)<br/>-M &lt;minimum number of instances&gt; = 	Set minimum
            number of instances per leaf.<br/>	(default 2)<br/>-R = 	Use reduced error
            pruning.<br/>-N &lt;number of folds&gt; = 	Set number of folds for reduced
            error<br/>	pruning. One fold is used as pruning set.<br/>	(default 3)<br/>-B =
            	Use binary splits only.<br/>-S = 	Don't perform subtree raising.<br/>-L =
            	Do not clean up after the tree has been built.<br/>-A = 	Laplace smoothing
            for predicted probabilities.<br/>-J = 	Do not use MDL correction for info
            gain on numeric attributes.<br/>-Q &lt;seed&gt; = 	Seed for random data
            shuffling (default 1).
            </summary>
        </member>
        <member name="P:Ml2.Clss.ClassifiersMeta.Bagging">
            <summary>
            Class for bagging a classifier to reduce variance. Can do classification
            and regression depending on the base learner. <br/><br/>For more
            information, see<br/><br/>Leo Breiman (1996). Bagging predictors. Machine Learning.
            24(2):123-140.<br/><br/>Options:<br/><br/>-P = 	Size of each bag, as a
            percentage of the<br/>	training set size. (default 100)<br/>-O = 	Calculate the
            out of bag error.<br/>-S &lt;num&gt; = 	Random number seed.<br/>	(default
            1)<br/>-num-slots &lt;num&gt; = 	Number of execution slots.<br/>	(default 1
            - i.e. no parallelism)<br/>-I &lt;num&gt; = 	Number of
            iterations.<br/>	(default 10)<br/>-D = 	If set, classifier is run in debug mode and<br/>	may
            output additional info to the console<br/>-W = 	Full name of base
            classifier.<br/>	(default: weka.classifiers.trees.REPTree)<br/><br/>Options specific
            to classifier weka.classifiers.trees.REPTree: = <br/>-M &lt;minimum number
            of instances&gt; = 	Set minimum number of instances per leaf (default
            2).<br/>-V &lt;minimum variance for split&gt; = 	Set minimum numeric class
            variance proportion<br/>	of train variance for split (default 1e-3).<br/>-N
            &lt;number of folds&gt; = 	Number of folds for reduced error pruning (default
            3).<br/>-S &lt;seed&gt; = 	Seed for random data shuffling (default 1).<br/>-P
            = 	No pruning.<br/>-L = 	Maximum tree depth (default -1, no
            maximum)<br/>-I = 	Initial class value count (default 0)<br/>-R = 	Spread initial count
            over all class values (i.e. don't use 1 per value)
            </summary>
        </member>
        <member name="P:Ml2.Clss.ClassifiersMeta.CVParameterSelection">
            <summary>
            Class for performing parameter selection by cross-validation for any
            classifier.<br/><br/>For more information, see:<br/><br/>R. Kohavi (1995).
            Wrappers for Performance Enhancement and Oblivious Decision Graphs. Department
            of Computer Science, Stanford University.<br/><br/>Options:<br/><br/>-X
            &lt;number of folds&gt; = 	Number of folds used for cross validation (default
            10).<br/>-P &lt;classifier parameter&gt; = 	Classifier parameter
            options.<br/>	eg: "N 1 5 10" Sets an optimisation parameter for the<br/>	classifier
            with name -N, with lower bound 1, upper bound<br/>	5, and 10 optimisation
            steps. The upper bound may be the<br/>	character 'A' or 'I' to substitute the
            number of<br/>	attributes or instances in the training
            data,<br/>	respectively. This parameter may be supplied more than<br/>	once to optimise over
            several classifier options<br/>	simultaneously.<br/>-S &lt;num&gt; = 	Random
            number seed.<br/>	(default 1)<br/>-D = 	If set, classifier is run in debug
            mode and<br/>	may output additional info to the console<br/>-W = 	Full
            name of base classifier.<br/>	(default:
            weka.classifiers.rules.ZeroR)<br/><br/>Options specific to classifier weka.classifiers.rules.ZeroR: = <br/>-D =
            	If set, classifier is run in debug mode and<br/>	may output additional info
            to the console
            </summary>
        </member>
        <member name="P:Ml2.Clss.ClassifiersMeta.ClassificationViaRegression">
            <summary>
            Class for doing classification using regression methods. Class is
            binarized and one regression model is built for each class value. For more
            information, see, for example<br/><br/>E. Frank, Y. Wang, S. Inglis, G. Holmes,
            I.H. Witten (1998). Using model trees for classification. Machine Learning.
            32(1):63-76.<br/><br/>Options:<br/><br/>-D = 	If set, classifier is run in
            debug mode and<br/>	may output additional info to the console<br/>-W = 	Full
            name of base classifier.<br/>	(default:
            weka.classifiers.trees.M5P)<br/><br/>Options specific to classifier weka.classifiers.trees.M5P: = <br/>-N =
            	Use unpruned tree/rules<br/>-U = 	Use unsmoothed predictions<br/>-R =
            	Build regression tree/rule rather than a model tree/rule<br/>-M &lt;minimum
            number of instances&gt; = 	Set minimum number of instances per
            leaf<br/>	(default 4)<br/>-L = 	Save instances at the nodes in<br/>	the tree (for
            visualization purposes)
            </summary>
        </member>
        <member name="P:Ml2.Clss.ClassifiersMeta.CostSensitiveClassifier">
            <summary>
            A metaclassifier that makes its base classifier cost-sensitive. Two
            methods can be used to introduce cost-sensitivity: reweighting training
            instances according to the total cost assigned to each class; or predicting the
            class with minimum expected misclassification cost (rather than the most
            likely class). Performance can often be improved by using a Bagged classifier to
            improve the probability estimates of the base
            classifier.<br/><br/>Options:<br/><br/>-M = 	Minimize expected misclassification cost. Default is
            to<br/>	reweight training instances according to costs per class<br/>-C &lt;cost
            file name&gt; = 	File name of a cost matrix to use. If this is not
            supplied,<br/>	a cost matrix will be loaded on demand. The name of
            the<br/>	on-demand file is the relation name of the training data<br/>	plus ".cost", and
            the path to the on-demand file is<br/>	specified with the -N option.<br/>-N
            &lt;directory&gt; = 	Name of a directory to search for cost files when
            loading<br/>	costs on demand (default current directory).<br/>-cost-matrix
            &lt;matrix&gt; = 	The cost matrix in Matlab single line format.<br/>-S
            &lt;num&gt; = 	Random number seed.<br/>	(default 1)<br/>-D = 	If set, classifier is
            run in debug mode and<br/>	may output additional info to the
            console<br/>-W = 	Full name of base classifier.<br/>	(default:
            weka.classifiers.rules.ZeroR)<br/><br/>Options specific to classifier weka.classifiers.rules.ZeroR:
            = <br/>-D = 	If set, classifier is run in debug mode and<br/>	may output
            additional info to the console
            </summary>
        </member>
        <member name="P:Ml2.Clss.ClassifiersMeta.FilteredClassifier">
            <summary>
            Class for running an arbitrary classifier on data that has been passed
            through an arbitrary filter. Like the classifier, the structure of the filter
            is based exclusively on the training data and test instances will be
            processed by the filter without changing their
            structure.<br/><br/>Options:<br/><br/>-F &lt;filter specification&gt; = 	Full class name of filter to use,
            followed<br/>	by filter options.<br/>	eg:
            "weka.filters.unsupervised.attribute.Remove -V -R 1,2"<br/>-D = 	If set, classifier is run in debug mode
            and<br/>	may output additional info to the console<br/>-W = 	Full name of base
            classifier.<br/>	(default: weka.classifiers.trees.J48)<br/><br/>Options
            specific to classifier weka.classifiers.trees.J48: = <br/>-U = 	Use unpruned
            tree.<br/>-O = 	Do not collapse tree.<br/>-C &lt;pruning confidence&gt; =
            	Set confidence threshold for pruning.<br/>	(default 0.25)<br/>-M &lt;minimum
            number of instances&gt; = 	Set minimum number of instances per
            leaf.<br/>	(default 2)<br/>-R = 	Use reduced error pruning.<br/>-N &lt;number of
            folds&gt; = 	Set number of folds for reduced error<br/>	pruning. One fold is
            used as pruning set.<br/>	(default 3)<br/>-B = 	Use binary splits only.<br/>-S
            = 	Don't perform subtree raising.<br/>-L = 	Do not clean up after the tree
            has been built.<br/>-A = 	Laplace smoothing for predicted
            probabilities.<br/>-J = 	Do not use MDL correction for info gain on numeric
            attributes.<br/>-Q &lt;seed&gt; = 	Seed for random data shuffling (default 1).
            </summary>
        </member>
        <member name="P:Ml2.Clss.ClassifiersMeta.LogitBoost">
            <summary>
            Class for performing additive logistic regression. <br/>This class
            performs classification using a regression scheme as the base learner, and can
            handle multi-class problems. For more information, see<br/><br/>J. Friedman,
            T. Hastie, R. Tibshirani (1998). Additive Logistic Regression: a
            Statistical View of Boosting. Stanford University.<br/><br/>Can do efficient internal
            cross-validation to determine appropriate number of
            iterations.<br/><br/>Options:<br/><br/>-Q = 	Use resampling instead of reweighting for
            boosting.<br/>-P &lt;percent&gt; = 	Percentage of weight mass to base training
            on.<br/>	(default 100, reduce to around 90 speed up)<br/>-F &lt;num&gt; = 	Number
            of folds for internal cross-validation.<br/>	(default 0 -- no
            cross-validation)<br/>-R &lt;num&gt; = 	Number of runs for internal
            cross-validation.<br/>	(default 1)<br/>-L &lt;num&gt; = 	Threshold on the improvement of the
            likelihood.<br/>	(default -Double.MAX_VALUE)<br/>-H &lt;num&gt; = 	Shrinkage
            parameter.<br/>	(default 1)<br/>-S &lt;num&gt; = 	Random number
            seed.<br/>	(default 1)<br/>-I &lt;num&gt; = 	Number of iterations.<br/>	(default
            10)<br/>-D = 	If set, classifier is run in debug mode and<br/>	may output
            additional info to the console<br/>-W = 	Full name of base
            classifier.<br/>	(default: weka.classifiers.trees.DecisionStump)<br/><br/>Options specific to
            classifier weka.classifiers.trees.DecisionStump: = <br/>-D = 	If set,
            classifier is run in debug mode and<br/>	may output additional info to the console
            </summary>
        </member>
        <member name="P:Ml2.Clss.ClassifiersMeta.MultiClassClassifier">
            <summary>
            A metaclassifier for handling multi-class datasets with 2-class
            classifiers. This classifier is also capable of applying error correcting output
            codes for increased accuracy.<br/><br/>Options:<br/><br/>-M &lt;num&gt; =
            	Sets the method to use. Valid values are 0 (1-against-all),<br/>	1 (random
            codes), 2 (exhaustive code), and 3 (1-against-1). (default 0)<br/><br/>-R
            &lt;num&gt; = 	Sets the multiplier when using random codes. (default
            2.0)<br/>-P = 	Use pairwise coupling (only has an effect for 1-against1)<br/>-S
            &lt;num&gt; = 	Random number seed.<br/>	(default 1)<br/>-D = 	If set, classifier
            is run in debug mode and<br/>	may output additional info to the
            console<br/>-W = 	Full name of base classifier.<br/>	(default:
            weka.classifiers.functions.Logistic)<br/><br/>Options specific to classifier
            weka.classifiers.functions.Logistic: = <br/>-D = 	Turn on debugging output.<br/>-C = 	Use
            conjugate gradient descent rather than BFGS updates.<br/>-R &lt;ridge&gt; = 	Set
            the ridge in the log-likelihood.<br/>-M &lt;number&gt; = 	Set the maximum
            number of iterations (default -1, until convergence).
            </summary>
        </member>
        <member name="P:Ml2.Clss.ClassifiersMeta.MultiClassClassifierUpdateable">
            <summary>
            A metaclassifier for handling multi-class datasets with 2-class
            classifiers. This classifier is also capable of applying error correcting output
            codes for increased accuracy. The base classifier must be an updateable
            classifier<br/><br/>Options:<br/><br/>-M &lt;num&gt; = 	Sets the method to use.
            Valid values are 0 (1-against-all),<br/>	1 (random codes), 2 (exhaustive
            code), and 3 (1-against-1). (default 0)<br/><br/>-R &lt;num&gt; = 	Sets the
            multiplier when using random codes. (default 2.0)<br/>-P = 	Use pairwise
            coupling (only has an effect for 1-against1)<br/>-S &lt;num&gt; = 	Random
            number seed.<br/>	(default 1)<br/>-D = 	If set, classifier is run in debug mode
            and<br/>	may output additional info to the console<br/>-W = 	Full name of
            base classifier.<br/>	(default:
            weka.classifiers.functions.Logistic)<br/><br/>Options specific to classifier weka.classifiers.functions.SGD: = <br/>-F
            = 	Set the loss function to minimize. 0 = hinge loss (SVM), 1 = log loss
            (logistic regression),<br/>	2 = squared loss (regression).<br/>	(default =
            0)<br/>-L = 	The learning rate. If normalization is<br/>	turned off (as it is
            automatically for streaming data), then the<br/>	default learning rate
            will need to be reduced (try 0.0001).<br/>	(default = 0.01).<br/>-R
            &lt;double&gt; = 	The lambda regularization constant (default = 0.0001)<br/>-E
            &lt;integer&gt; = 	The number of epochs to perform (batch learning only, default
            = 500)<br/>-N = 	Don't normalize the data<br/>-M = 	Don't replace missing
            values
            </summary>
        </member>
        <member name="P:Ml2.Clss.ClassifiersMeta.MultiScheme">
            <summary>
            Class for selecting a classifier from among several using cross
            validation on the training data or the performance on the training data. Performance
            is measured based on percent correct (classification) or mean-squared
            error (regression).<br/><br/>Options:<br/><br/>-X &lt;number of folds&gt; =
            	Use cross validation for model selection using the<br/>	given number of
            folds. (default 0, is to<br/>	use training error)<br/>-S &lt;num&gt; = 	Random
            number seed.<br/>	(default 1)<br/>-B &lt;classifier specification&gt; =
            	Full class name of classifier to include, followed<br/>	by scheme options. May
            be specified multiple times.<br/>	(default:
            "weka.classifiers.rules.ZeroR")<br/>-D = 	If set, classifier is run in debug mode and<br/>	may output
            additional info to the console
            </summary>
        </member>
        <!-- Badly formed XML comment ignored for member "P:Ml2.Clss.ClassifiersMeta.RandomCommittee" -->
        <!-- Badly formed XML comment ignored for member "P:Ml2.Clss.ClassifiersMeta.RandomSubSpace" -->
        <member name="P:Ml2.Clss.ClassifiersMeta.RegressionByDiscretization">
            <summary>
            A regression scheme that employs any classifier on a copy of the data
            that has the class attribute discretized. The predicted value is the expected
            value of the mean class value for each discretized interval (based on the
            predicted probabilities for each interval). This class now also supports
            conditional density estimation by building a univariate density estimator from
            the target values in the training data, weighted by the class
            probabilities. <br/><br/>For more information on this process, see<br/><br/>Eibe Frank,
            Remco R. Bouckaert: Conditional Density Estimation with Class Probability
            Estimators. In: First Asian Conference on Machine Learning, Berlin, 65-81,
            2009.<br/><br/>Options:<br/><br/>-B &lt;int&gt; = 	Number of bins for
            equal-width discretization<br/>	(default 10).<br/><br/>-E = 	Whether to delete
            empty bins after discretization<br/>	(default false).<br/><br/>-A = 	Whether
            to minimize absolute error, rather than squared error.<br/>	(default
            false).<br/><br/>-F = 	Use equal-frequency instead of equal-width
            discretization.<br/>-K = 	What type of density estimator to use:
            0=histogram/1=kernel/2=normal (default: 0).<br/>-D = 	If set, classifier is run in debug mode
            and<br/>	may output additional info to the console<br/>-W = 	Full name of base
            classifier.<br/>	(default: weka.classifiers.trees.J48)<br/><br/>Options
            specific to classifier weka.classifiers.trees.J48: = <br/>-U = 	Use unpruned
            tree.<br/>-O = 	Do not collapse tree.<br/>-C &lt;pruning confidence&gt; =
            	Set confidence threshold for pruning.<br/>	(default 0.25)<br/>-M &lt;minimum
            number of instances&gt; = 	Set minimum number of instances per
            leaf.<br/>	(default 2)<br/>-R = 	Use reduced error pruning.<br/>-N &lt;number of
            folds&gt; = 	Set number of folds for reduced error<br/>	pruning. One fold is used
            as pruning set.<br/>	(default 3)<br/>-B = 	Use binary splits only.<br/>-S
            = 	Don't perform subtree raising.<br/>-L = 	Do not clean up after the tree
            has been built.<br/>-A = 	Laplace smoothing for predicted
            probabilities.<br/>-J = 	Do not use MDL correction for info gain on numeric
            attributes.<br/>-Q &lt;seed&gt; = 	Seed for random data shuffling (default 1).
            </summary>
        </member>
        <member name="P:Ml2.Clss.ClassifiersMeta.Stacking">
            <summary>
            Combines several classifiers using the stacking method. Can do
            classification or regression.<br/><br/>For more information, see<br/><br/>David H.
            Wolpert (1992). Stacked generalization. Neural Networks.
            5:241-259.<br/><br/>Options:<br/><br/>-M &lt;scheme specification&gt; = 	Full name of meta
            classifier, followed by options.<br/>	(default:
            "weka.classifiers.rules.Zero")<br/>-X &lt;number of folds&gt; = 	Sets the number of cross-validation
            folds.<br/>-S &lt;num&gt; = 	Random number seed.<br/>	(default 1)<br/>-num-slots
            &lt;num&gt; = 	Number of execution slots.<br/>	(default 1 - i.e. no
            parallelism)<br/>-B &lt;classifier specification&gt; = 	Full class name of
            classifier to include, followed<br/>	by scheme options. May be specified multiple
            times.<br/>	(default: "weka.classifiers.rules.ZeroR")<br/>-D = 	If set,
            classifier is run in debug mode and<br/>	may output additional info to the
            console
            </summary>
        </member>
        <member name="P:Ml2.Clss.ClassifiersMeta.Vote">
            <summary>
            Class for combining classifiers. Different combinations of probability
            estimates for classification are available.<br/><br/>For more information
            see:<br/><br/>Ludmila I. Kuncheva (2004). Combining Pattern Classifiers:
            Methods and Algorithms. John Wiley and Sons, Inc..<br/><br/>J. Kittler, M.
            Hatef, Robert P.W. Duin, J. Matas (1998). On combining classifiers. IEEE
            Transactions on Pattern Analysis and Machine Intelligence.
            20(3):226-239.<br/><br/>Options:<br/><br/>-S &lt;num&gt; = 	Random number seed.<br/>	(default
            1)<br/>-B &lt;classifier specification&gt; = 	Full class name of classifier to
            include, followed<br/>	by scheme options. May be specified multiple
            times.<br/>	(default: "weka.classifiers.rules.ZeroR")<br/>-D = 	If set, classifier
            is run in debug mode and<br/>	may output additional info to the
            console<br/>-P &lt;path to serialized classifier&gt; = 	Full path to serialized
            classifier to include.<br/>	May be specified multiple times to
            include<br/>	multiple serialized classifiers. Note: it does<br/>	not make sense to use
            pre-built classifiers in<br/>	a cross-validation.<br/>-R
            &lt;AVG|PROD|MAJ|MIN|MAX|MED&gt; = 	The combination rule to use<br/>	(default: AVG)
            </summary>
        </member>
        <member name="P:Ml2.Clss.ClassifiersMeta.MetaCost">
            <summary>
            This metaclassifier makes its base classifier cost-sensitive using the
            method specified in<br/><br/>Pedro Domingos: MetaCost: A general method for
            making classifiers cost-sensitive. In: Fifth International Conference on
            Knowledge Discovery and Data Mining, 155-164, 1999.<br/><br/>This classifier
            should produce similar results to one created by passing the base learner to
            Bagging, which is in turn passed to a CostSensitiveClassifier operating on
            minimum expected cost. The difference is that MetaCost produces a single
            cost-sensitive classifier of the base learner, giving the benefits of fast
            classification and interpretable output (if the base learner itself is
            interpretable). This implementation uses all bagging iterations when
            reclassifying training data (the MetaCost paper reports a marginal improvement when
            only those iterations containing each training instance are used in
            reclassifying that instance).<br/><br/>Options:<br/><br/>-I &lt;num&gt; = 	Number of
            bagging iterations.<br/>	(default 10)<br/>-C &lt;cost file name&gt; =
            	File name of a cost matrix to use. If this is not supplied,<br/>	a cost matrix
            will be loaded on demand. The name of the<br/>	on-demand file is the
            relation name of the training data<br/>	plus ".cost", and the path to the
            on-demand file is<br/>	specified with the -N option.<br/>-N &lt;directory&gt; =
            	Name of a directory to search for cost files when loading<br/>	costs on
            demand (default current directory).<br/>-cost-matrix &lt;matrix&gt; = 	The
            cost matrix in Matlab single line format.<br/>-P = 	Size of each bag, as a
            percentage of the<br/>	training set size. (default 100)<br/>-S &lt;num&gt; =
            	Random number seed.<br/>	(default 1)<br/>-D = 	If set, classifier is run in
            debug mode and<br/>	may output additional info to the console<br/>-W =
            	Full name of base classifier.<br/>	(default:
            weka.classifiers.rules.ZeroR)<br/><br/>Options specific to classifier weka.classifiers.rules.ZeroR: =
            <br/>-D = 	If set, classifier is run in debug mode and<br/>	may output
            additional info to the console
            </summary>
        </member>
        <member name="P:Ml2.Clss.ClassifiersMisc.InputMappedClassifier">
            <summary>
            Wrapper classifier that addresses incompatible training and test data by
            building a mapping between the training data that a classifier has been
            built with and the incoming test instances' structure. Model attributes that
            are not found in the incoming instances receive missing values, so do
            incoming nominal attribute values that the classifier has not seen before. A new
            classifier can be trained or an existing one loaded from a
            file.<br/><br/>Options:<br/><br/>-I = 	Ignore case when matching attribute names and
            nominal values.<br/>-M = 	Suppress the output of the mapping report.<br/>-trim =
            	Trim white space from either end of names before matching.<br/>-L &lt;path
            to model to load&gt; = 	Path to a model to load. If set, this
            model<br/>	will be used for prediction and any base classifier<br/>	specification will
            be ignored. Environment variables<br/>	may be used in the path (e.g.
            ${HOME}/myModel.model)<br/>-D = 	If set, classifier is run in debug mode
            and<br/>	may output additional info to the console<br/>-W = 	Full name of base
            classifier.<br/>	(default: weka.classifiers.rules.ZeroR)<br/><br/>Options
            specific to classifier weka.classifiers.rules.ZeroR: = <br/>-D = 	If set,
            classifier is run in debug mode and<br/>	may output additional info to the
            console
            </summary>
        </member>
        <member name="P:Ml2.Clss.ClassifiersMisc.SerializedClassifier">
            <summary>
            A wrapper around a serialized classifier model. This classifier loads a
            serialized models and uses it to make predictions.<br/><br/>Warning: since
            the serialized model doesn't get changed, cross-validation cannot bet used
            with this classifier.<br/><br/>Options:<br/><br/>-D = 	If set, classifier is
            run in debug mode and<br/>	may output additional info to the
            console<br/>-model &lt;filename&gt; = 	The file containing the serialized
            model.<br/>	(required)
            </summary>
        </member>
        <member name="P:Ml2.Clss.ClassifiersRules.DecisionTable">
            <summary>
            Class for building and using a simple decision table majority
            classifier.<br/><br/>For more information see: <br/><br/>Ron Kohavi: The Power of
            Decision Tables. In: 8th European Conference on Machine Learning, 174-189,
            1995.<br/><br/>Options:<br/><br/>-S &lt;search method specification&gt; = 	Full
            class name of search method, followed<br/>	by its options.<br/>	eg:
            "weka.attributeSelection.BestFirst -D 1"<br/>	(default
            weka.attributeSelection.BestFirst)<br/>-X &lt;number of folds&gt; = 	Use cross validation to evaluate
            features.<br/>	Use number of folds = 1 for leave one out CV.<br/>	(Default
            = leave one out CV)<br/>-E &lt;acc | rmse | mae | auc&gt; = 	Performance
            evaluation measure to use for selecting attributes.<br/>	(Default = accuracy
            for discrete class and rmse for numeric class)<br/>-I = 	Use nearest
            neighbour instead of global table majority.<br/>-R = 	Display decision table
            rules.<br/><br/><br/>Options specific to search method
            weka.attributeSelection.BestFirst: = <br/>-P &lt;start set&gt; = 	Specify a starting set of
            attributes.<br/>	Eg. 1,3,5-7.<br/>-D &lt;0 = backward | 1 = forward | 2 =
            bi-directional&gt; = 	Direction of search. (default = 1).<br/>-N &lt;num&gt; =
            	Number of non-improving nodes to<br/>	consider before terminating
            search.<br/>-S &lt;num&gt; = 	Size of lookup cache for evaluated
            subsets.<br/>	Expressed as a multiple of the number of<br/>	attributes in the data set. (default
            = 1)
            </summary>
        </member>
        <member name="P:Ml2.Clss.ClassifiersRules.JRip">
            <summary>
            This class implements a propositional rule learner, Repeated Incremental
            Pruning to Produce Error Reduction (RIPPER), which was proposed by William
            W. Cohen as an optimized version of IREP. <br/><br/>The algorithm is
            briefly described as follows: <br/><br/>Initialize RS = {}, and for each class
            from the less prevalent one to the more frequent one, DO: <br/><br/>1.
            Building stage:<br/>Repeat 1.1 and 1.2 until the descrition length (DL) of the
            ruleset and examples is 64 bits greater than the smallest DL met so far, or
            there are no positive examples, or the error rate >= 50%. <br/><br/>1.1.
            Grow phase:<br/>Grow one rule by greedily adding antecedents (or conditions)
            to the rule until the rule is perfect (i.e. 100% accurate). The procedure
            tries every possible value of each attribute and selects the condition with
            highest information gain: p(log(p/t)-log(P/T)).<br/><br/>1.2. Prune
            phase:<br/>Incrementally prune each rule and allow the pruning of any final
            sequences of the antecedents;The pruning metric is (p-n)/(p+n) -- but it's
            actually 2p/(p+n) -1, so in this implementation we simply use p/(p+n) (actually
            (p+1)/(p+n+2), thus if p+n is 0, it's 0.5).<br/><br/>2. Optimization
            stage:<br/> after generating the initial ruleset {Ri}, generate and prune two
            variants of each rule Ri from randomized data using procedure 1.1 and 1.2. But
            one variant is generated from an empty rule while the other is generated by
            greedily adding antecedents to the original rule. Moreover, the pruning
            metric used here is (TP+TN)/(P+N).Then the smallest possible DL for each
            variant and the original rule is computed. The variant with the minimal DL is
            selected as the final representative of Ri in the ruleset.After all the rules
            in {Ri} have been examined and if there are still residual positives, more
            rules are generated based on the residual positives using Building Stage
            again. <br/>3. Delete the rules from the ruleset that would increase the DL
            of the whole ruleset if it were in it. and add resultant ruleset to RS.
            <br/>ENDDO<br/><br/>Note that there seem to be 2 bugs in the original ripper
            program that would affect the ruleset size and accuracy slightly. This
            implementation avoids these bugs and thus is a little bit different from Cohen's
            original implementation. Even after fixing the bugs, since the order of
            classes with the same frequency is not defined in ripper, there still seems
            to be some trivial difference between this implementation and the original
            ripper, especially for audiology data in UCI repository, where there are
            lots of classes of few instances.<br/><br/>Details please
            see:<br/><br/>William W. Cohen: Fast Effective Rule Induction. In: Twelfth International
            Conference on Machine Learning, 115-123, 1995.<br/><br/>PS. We have compared this
            implementation with the original ripper implementation in aspects of
            accuracy, ruleset size and running time on both artificial data "ab+bcd+defg"
            and UCI datasets. In all these aspects it seems to be quite comparable to the
            original ripper implementation. However, we didn't consider memory
            consumption optimization in this
            implementation.<br/><br/><br/><br/>Options:<br/><br/>-F &lt;number of folds&gt; = 	Set number of folds for REP<br/>	One fold
            is used as pruning set.<br/>	(default 3)<br/>-N &lt;min. weights&gt; =
            	Set the minimal weights of instances<br/>	within a split.<br/>	(default
            2.0)<br/>-O &lt;number of runs&gt; = 	Set the number of runs
            of<br/>	optimizations. (Default: 2)<br/>-D = 	Set whether turn on the<br/>	debug mode
            (Default: false)<br/>-S &lt;seed&gt; = 	The seed of randomization<br/>	(Default:
            1)<br/>-E = 	Whether NOT check the error rate>=0.5<br/>	in stopping criteria
            	(default: check)<br/>-P = 	Whether NOT use pruning<br/>	(default: use
            pruning)
            </summary>
        </member>
        <member name="P:Ml2.Clss.ClassifiersRules.M5Rules">
            <summary>
            Generates a decision list for regression problems using
            separate-and-conquer. In each iteration it builds a model tree using M5 and makes the "best"
            leaf into a rule.<br/><br/>For more information see:<br/><br/>Geoffrey
            Holmes, Mark Hall, Eibe Frank: Generating Rule Sets from Model Trees. In:
            Twelfth Australian Joint Conference on Artificial Intelligence, 1-12,
            1999.<br/><br/>Ross J. Quinlan: Learning with Continuous Classes. In: 5th Australian
            Joint Conference on Artificial Intelligence, Singapore, 343-348,
            1992.<br/><br/>Y. Wang, I. H. Witten: Induction of model trees for predicting
            continuous classes. In: Poster papers of the 9th European Conference on Machine
            Learning, 1997.<br/><br/>Options:<br/><br/>-N = 	Use unpruned
            tree/rules<br/>-U = 	Use unsmoothed predictions<br/>-R = 	Build regression tree/rule
            rather than a model tree/rule<br/>-M &lt;minimum number of instances&gt; = 	Set
            minimum number of instances per leaf<br/>	(default 4)
            </summary>
        </member>
        <member name="P:Ml2.Clss.ClassifiersRules.OneR">
            <summary>
            Class for building and using a 1R classifier; in other words, uses the
            minimum-error attribute for prediction, discretizing numeric attributes. For
            more information, see:<br/><br/>R.C. Holte (1993). Very simple
            classification rules perform well on most commonly used datasets. Machine Learning.
            11:63-91.<br/><br/>Options:<br/><br/>-B &lt;minimum bucket size&gt; = 	The
            minimum number of objects in a bucket (default: 6).
            </summary>
        </member>
        <member name="P:Ml2.Clss.ClassifiersRules.PART">
            <summary>
            Class for generating a PART decision list. Uses separate-and-conquer.
            Builds a partial C4.5 decision tree in each iteration and makes the "best"
            leaf into a rule.<br/><br/>For more information, see:<br/><br/>Eibe Frank, Ian
            H. Witten: Generating Accurate Rule Sets Without Global Optimization. In:
            Fifteenth International Conference on Machine Learning, 144-151,
            1998.<br/><br/>Options:<br/><br/>-C &lt;pruning confidence&gt; = 	Set confidence
            threshold for pruning.<br/>	(default 0.25)<br/>-M &lt;minimum number of
            objects&gt; = 	Set minimum number of objects per leaf.<br/>	(default 2)<br/>-R =
            	Use reduced error pruning.<br/>-N &lt;number of folds&gt; = 	Set number of
            folds for reduced error<br/>	pruning. One fold is used as pruning
            set.<br/>	(default 3)<br/>-B = 	Use binary splits only.<br/>-U = 	Generate unpruned
            decision list.<br/>-J = 	Do not use MDL correction for info gain on numeric
            attributes.<br/>-Q &lt;seed&gt; = 	Seed for random data shuffling (default
            1).
            </summary>
        </member>
        <member name="P:Ml2.Clss.ClassifiersRules.ZeroR">
            <summary>
            Class for building and using a 0-R classifier. Predicts the mean (for a
            numeric class) or the mode (for a nominal
            class).<br/><br/>Options:<br/><br/>-D = 	If set, classifier is run in debug mode and<br/>	may output
            additional info to the console
            </summary>
        </member>
        <member name="P:Ml2.Clss.ClassifiersTrees.DecisionStump">
            <summary>
            Class for building and using a decision stump. Usually used in
            conjunction with a boosting algorithm. Does regression (based on mean-squared error)
            or classification (based on entropy). Missing is treated as a separate
            value.<br/><br/>Options:<br/><br/>-D = 	If set, classifier is run in debug mode
            and<br/>	may output additional info to the console
            </summary>
        </member>
        <member name="P:Ml2.Clss.ClassifiersTrees.J48">
            <summary>
            Class for generating a pruned or unpruned C4.5 decision tree. For more
            information, see<br/><br/>Ross Quinlan (1993). C4.5: Programs for Machine
            Learning. Morgan Kaufmann Publishers, San Mateo,
            CA.<br/><br/>Options:<br/><br/>-U = 	Use unpruned tree.<br/>-O = 	Do not collapse tree.<br/>-C
            &lt;pruning confidence&gt; = 	Set confidence threshold for pruning.<br/>	(default
            0.25)<br/>-M &lt;minimum number of instances&gt; = 	Set minimum number of
            instances per leaf.<br/>	(default 2)<br/>-R = 	Use reduced error
            pruning.<br/>-N &lt;number of folds&gt; = 	Set number of folds for reduced
            error<br/>	pruning. One fold is used as pruning set.<br/>	(default 3)<br/>-B = 	Use
            binary splits only.<br/>-S = 	Don't perform subtree raising.<br/>-L = 	Do not
            clean up after the tree has been built.<br/>-A = 	Laplace smoothing for
            predicted probabilities.<br/>-J = 	Do not use MDL correction for info gain on
            numeric attributes.<br/>-Q &lt;seed&gt; = 	Seed for random data shuffling
            (default 1).
            </summary>
        </member>
        <member name="P:Ml2.Clss.ClassifiersTrees.LMT">
            <summary>
            Classifier for building 'logistic model trees', which are classification
            trees with logistic regression functions at the leaves. The algorithm can
            deal with binary and multi-class target variables, numeric and nominal
            attributes and missing values.<br/><br/>For more information see:
            <br/><br/>Niels Landwehr, Mark Hall, Eibe Frank (2005). Logistic Model Trees. Machine
            Learning. 95(1-2):161-205.<br/><br/>Marc Sumner, Eibe Frank, Mark Hall:
            Speeding up Logistic Model Tree Induction. In: 9th European Conference on
            Principles and Practice of Knowledge Discovery in Databases, 675-683,
            2005.<br/><br/>Options:<br/><br/>-B = 	Binary splits (convert nominal attributes to
            binary ones)<br/>-R = 	Split on residuals instead of class values<br/>-C =
            	Use cross-validation for boosting at all nodes (i.e., disable
            heuristic)<br/>-P = 	Use error on probabilities instead of misclassification error for
            stopping criterion of LogitBoost.<br/>-I &lt;numIterations&gt; = 	Set fixed
            number of iterations for LogitBoost (instead of using
            cross-validation)<br/>-M &lt;numInstances&gt; = 	Set minimum number of instances at which a node
            can be split (default 15)<br/>-W &lt;beta&gt; = 	Set beta for weight
            trimming for LogitBoost. Set to 0 (default) for no weight trimming.<br/>-A = 	The
            AIC is used to choose the best iteration.
            </summary>
        </member>
        <member name="P:Ml2.Clss.ClassifiersTrees.M5P">
            <summary>
            M5Base. Implements base routines for generating M5 Model trees and
            rules<br/>The original algorithm M5 was invented by R. Quinlan and Yong Wang made
            improvements.<br/><br/>For more information see:<br/><br/>Ross J. Quinlan:
            Learning with Continuous Classes. In: 5th Australian Joint Conference on
            Artificial Intelligence, Singapore, 343-348, 1992.<br/><br/>Y. Wang, I. H.
            Witten: Induction of model trees for predicting continuous classes. In:
            Poster papers of the 9th European Conference on Machine Learning,
            1997.<br/><br/>Options:<br/><br/>-N = 	Use unpruned tree/rules<br/>-U = 	Use unsmoothed
            predictions<br/>-R = 	Build regression tree/rule rather than a model
            tree/rule<br/>-M &lt;minimum number of instances&gt; = 	Set minimum number of
            instances per leaf<br/>	(default 4)<br/>-L = 	Save instances at the nodes
            in<br/>	the tree (for visualization purposes)
            </summary>
        </member>
        <member name="P:Ml2.Clss.ClassifiersTrees.REPTree">
            <summary>
            Fast decision tree learner. Builds a decision/regression tree using
            information gain/variance and prunes it using reduced-error pruning (with
            backfitting). Only sorts values for numeric attributes once. Missing values are
            dealt with by splitting the corresponding instances into pieces (i.e. as in
            C4.5).<br/><br/>Options:<br/><br/>-M &lt;minimum number of instances&gt; =
            	Set minimum number of instances per leaf (default 2).<br/>-V &lt;minimum
            variance for split&gt; = 	Set minimum numeric class variance
            proportion<br/>	of train variance for split (default 1e-3).<br/>-N &lt;number of folds&gt;
            = 	Number of folds for reduced error pruning (default 3).<br/>-S
            &lt;seed&gt; = 	Seed for random data shuffling (default 1).<br/>-P = 	No
            pruning.<br/>-L = 	Maximum tree depth (default -1, no maximum)<br/>-I = 	Initial class
            value count (default 0)<br/>-R = 	Spread initial count over all class
            values (i.e. don't use 1 per value)
            </summary>
        </member>
        <!-- Badly formed XML comment ignored for member "P:Ml2.Clss.ClassifiersTrees.RandomForest" -->
        <!-- Badly formed XML comment ignored for member "P:Ml2.Clss.ClassifiersTrees.RandomTree" -->
        <member name="P:Ml2.Clss.ClassifiersTreesLmt.LogisticBase">
            <summary>
            No class description found.
            </summary>
        </member>
        <!-- Badly formed XML comment ignored for member "T:Ml2.Clss.LibLINEAR" -->
        <member name="M:Ml2.Clss.LibLINEAR.SVMType(Ml2.Clss.LibLINEAR.ESVMType)">
            <summary>
            
            </summary>    
        </member>
        <member name="M:Ml2.Clss.LibLINEAR.Cost(System.Double)">
            <summary>
            The cost parameter C.
            </summary>    
        </member>
        <member name="M:Ml2.Clss.LibLINEAR.Eps(System.Double)">
            <summary>
            The tolerance of the termination criterion.
            </summary>    
        </member>
        <member name="M:Ml2.Clss.LibLINEAR.Normalize(System.Boolean)">
            <summary>
            Whether to normalize the data.
            </summary>    
        </member>
        <member name="M:Ml2.Clss.LibLINEAR.ConvertNominalToBinary(System.Boolean)">
            <summary>
            Whether to turn on conversion of nominal attributes to binary.
            </summary>    
        </member>
        <member name="M:Ml2.Clss.LibLINEAR.DoNotReplaceMissingValues(System.Boolean)">
            <summary>
            Whether to turn off automatic replacement of missing values. WARNING: set
            to true only if the data does not contain missing values.
            </summary>    
        </member>
        <!-- Badly formed XML comment ignored for member "M:Ml2.Clss.LibLINEAR.Bias(System.Double)" -->
        <member name="M:Ml2.Clss.LibLINEAR.Weights(System.String)">
            <summary>
            The weights to use for the classes, if empty 1 is used by default.
            </summary>    
        </member>
        <member name="M:Ml2.Clss.LibLINEAR.ProbabilityEstimates(System.Boolean)">
            <summary>
            Whether to generate probability estimates instead of -1/+1 for
            classification problems (currently for L2-regularized logistic regression only!)
            </summary>    
        </member>
        <member name="M:Ml2.Clss.LibLINEAR.Debug(System.Boolean)">
            <summary>
            If set to true, classifier may output additional info to the console.
            </summary>    
        </member>
        <member name="T:Ml2.Clss.MetaCost">
            <summary>
            This metaclassifier makes its base classifier cost-sensitive using the
            method specified in<br/><br/>Pedro Domingos: MetaCost: A general method for
            making classifiers cost-sensitive. In: Fifth International Conference on
            Knowledge Discovery and Data Mining, 155-164, 1999.<br/><br/>This classifier
            should produce similar results to one created by passing the base learner to
            Bagging, which is in turn passed to a CostSensitiveClassifier operating on
            minimum expected cost. The difference is that MetaCost produces a single
            cost-sensitive classifier of the base learner, giving the benefits of fast
            classification and interpretable output (if the base learner itself is
            interpretable). This implementation uses all bagging iterations when
            reclassifying training data (the MetaCost paper reports a marginal improvement when
            only those iterations containing each training instance are used in
            reclassifying that instance).<br/><br/>Options:<br/><br/>-I &lt;num&gt; = 	Number of
            bagging iterations.<br/>	(default 10)<br/>-C &lt;cost file name&gt; =
            	File name of a cost matrix to use. If this is not supplied,<br/>	a cost matrix
            will be loaded on demand. The name of the<br/>	on-demand file is the
            relation name of the training data<br/>	plus ".cost", and the path to the
            on-demand file is<br/>	specified with the -N option.<br/>-N &lt;directory&gt; =
            	Name of a directory to search for cost files when loading<br/>	costs on
            demand (default current directory).<br/>-cost-matrix &lt;matrix&gt; = 	The
            cost matrix in Matlab single line format.<br/>-P = 	Size of each bag, as a
            percentage of the<br/>	training set size. (default 100)<br/>-S &lt;num&gt; =
            	Random number seed.<br/>	(default 1)<br/>-D = 	If set, classifier is run in
            debug mode and<br/>	may output additional info to the console<br/>-W =
            	Full name of base classifier.<br/>	(default:
            weka.classifiers.rules.ZeroR)<br/><br/>Options specific to classifier weka.classifiers.rules.ZeroR: =
            <br/>-D = 	If set, classifier is run in debug mode and<br/>	may output
            additional info to the console
            </summary>
        </member>
        <member name="M:Ml2.Clss.MetaCost.NumIterations(System.Int32)">
            <summary>
            The number of bagging iterations.
            </summary>    
        </member>
        <member name="M:Ml2.Clss.MetaCost.BagSizePercent(System.Int32)">
            <summary>
            The size of each bag, as a percentage of the training set size.
            </summary>    
        </member>
        <member name="M:Ml2.Clss.MetaCost.CostMatrix(System.Double[0:,0:])">
            <summary>
            A misclassification cost matrix.
            </summary>    
        </member>
        <member name="M:Ml2.Clss.MetaCost.CostMatrixSource(Ml2.Clss.MetaCost.ECostMatrixSource)">
            <summary>
            Gets the source location method of the cost matrix. Will be one of
            MATRIX_ON_DEMAND or MATRIX_SUPPLIED.
            </summary>    
        </member>
        <member name="M:Ml2.Clss.MetaCost.Classifier(Ml2.Clss.IBaseClassifier{weka.classifiers.Classifier})">
            <summary>
            The base classifier to be used.
            </summary>    
        </member>
        <member name="M:Ml2.Clss.MetaCost.Debug(System.Boolean)">
            <summary>
            If set to true, classifier may output additional info to the console.
            </summary>    
        </member>
        <member name="M:Ml2.Runtime.#ctor(weka.core.Instances,System.Type)">
            <summary>
            This is used to create a new Runtime with a specified set of Instances.
            </summary>
        </member>
        <member name="M:Ml2.Ml2Attribute.#ctor(weka.core.Attribute)">
            <summary>
            This is used to create a new Runtime with a specified set of Instances.
            </summary>
        </member>
        <member name="M:Ml2.Ml2Evaluation.#ctor(weka.classifiers.Evaluation)">
            <summary>
            This is used to create a new Runtime with a specified set of Instances.
            </summary>
        </member>
        <member name="M:Ml2.Ml2Instance.#ctor(weka.core.Instance)">
            <summary>
            This is used to create a new Runtime with a specified set of Instances.
            </summary>
        </member>
        <member name="T:Ml2.Fltr.SMOTE">
            <summary>
            Resamples a dataset by applying the Synthetic Minority Oversampling
            TEchnique (SMOTE). The original dataset must fit entirely in memory. The amount
            of SMOTE and number of nearest neighbors may be specified. For more
            information, see <br/><br/>Nitesh V. Chawla et. al. (2002). Synthetic Minority
            Over-sampling Technique. Journal of Artificial Intelligence Research.
            16:321-357.<br/><br/>Options:<br/><br/>-S &lt;num&gt; = 	Specifies the random
            number seed<br/>	(default 1)<br/>-P &lt;percentage&gt; = 	Specifies percentage
            of SMOTE instances to create.<br/>	(default 100.0)<br/><br/>-K
            &lt;nearest-neighbors&gt; = 	Specifies the number of nearest neighbors to
            use.<br/>	(default 5)<br/><br/>-C &lt;value-index&gt; = 	Specifies the index of the
            nominal class value to SMOTE<br/>	(default 0: auto-detect non-empty minority
            class))<br/>
            </summary>
        </member>
        <member name="M:Ml2.Fltr.SMOTE.Percentage(System.Double)">
            <summary>
            The percentage of SMOTE instances to create.
            </summary>    
        </member>
        <member name="M:Ml2.Fltr.SMOTE.NearestNeighbors(System.Int32)">
            <summary>
            The number of nearest neighbors to use.
            </summary>    
        </member>
        <member name="M:Ml2.Fltr.SMOTE.ClassValue(System.String)">
            <summary>
            The index of the class value to which SMOTE should be applied. Use a
            value of 0 to auto-detect the non-empty minority class.
            </summary>    
        </member>
        <member name="P:Ml2.Fltr.FiltersGeneral.AllFilter">
            <summary>
            An instance filter that passes all instances through unmodified.
            Primarily for testing purposes.
            </summary>
        </member>
        <member name="P:Ml2.Fltr.FiltersGeneral.MultiFilter">
            <summary>
            Applies several filters successively. In case all supplied filters are
            StreamableFilters, it will act as a streamable one,
            too.<br/><br/>Options:<br/><br/>-D = 	Turns on output of debugging information.<br/>-F &lt;classname
            [options]&gt; = 	A filter to apply (can be specified multiple times).
            </summary>
        </member>
        <member name="P:Ml2.Fltr.FiltersSupervisedAttribute.AddClassification">
            <summary>
            A filter for adding the classification, the class distribution and an
            error flag to a dataset with a classifier. The classifier is either trained on
            the data itself or provided as serialized
            model.<br/><br/>Options:<br/><br/>-D = 	Turns on output of debugging information.<br/>-W &lt;classifier
            specification&gt; = 	Full class name of classifier to use, followed<br/>	by
            scheme options. eg:<br/>		"weka.classifiers.bayes.NaiveBayes
            -D"<br/>	(default: weka.classifiers.rules.ZeroR)<br/>-serialized &lt;file&gt; = 	Instead of
            training a classifier on the data, one can also provide<br/>	a serialized
            model and use that for tagging the data.<br/>-classification = 	Adds an
            attribute with the actual classification.<br/>	(default:
            off)<br/>-remove-old-class = 	Removes the old class attribute.<br/>	(default:
            off)<br/>-distribution = 	Adds attributes with the distribution for all classes <br/>	(for
            numeric classes this will be identical to the attribute <br/>	output with
            '-classification').<br/>	(default: off)<br/>-error = 	Adds an attribute
            indicating whether the classifier output <br/>	a wrong classification (for
            numeric classes this is the numeric <br/>	difference).<br/>	(default: off)
            </summary>
        </member>
        <member name="P:Ml2.Fltr.FiltersSupervisedAttribute.AttributeSelection">
            <summary>
            A supervised attribute filter that can be used to select attributes. It
            is very flexible and allows various search and evaluation methods to be
            combined.<br/><br/>Options:<br/><br/>-S &lt;"Name of search class [search
            options]"&gt; = 	Sets search method for subset evaluators.<br/>	eg. -S
            "weka.attributeSelection.BestFirst -S 8"<br/>-E &lt;"Name of attribute/subset
            evaluation class [evaluator options]"&gt; = 	Sets attribute/subset
            evaluator.<br/>	eg. -E "weka.attributeSelection.CfsSubsetEval -L"<br/><br/>Options
            specific to evaluator weka.attributeSelection.CfsSubsetEval: = <br/>-M = 	Treat
            missing values as a separate value.<br/>-L = 	Don't include locally
            predictive attributes.<br/><br/>Options specific to search
            weka.attributeSelection.BestFirst: = <br/>-P &lt;start set&gt; = 	Specify a starting set of
            attributes.<br/>	Eg. 1,3,5-7.<br/>-D &lt;0 = backward | 1 = forward | 2 =
            bi-directional&gt; = 	Direction of search. (default = 1).<br/>-N &lt;num&gt; =
            	Number of non-improving nodes to<br/>	consider before terminating
            search.<br/>-S &lt;num&gt; = 	Size of lookup cache for evaluated
            subsets.<br/>	Expressed as a multiple of the number of<br/>	attributes in the data set. (default
            = 1)
            </summary>
        </member>
        <member name="P:Ml2.Fltr.FiltersSupervisedAttribute.ClassOrder">
            <summary>
            Changes the order of the classes so that the class values are no longer
            of in the order specified in the header. The values will be in the order
            specified by the user -- it could be either in ascending/descending order by
            the class frequency or in random order. Note that this filter currently does
            not change the header, only the class values of the instances, so there is
            not much point in using it in conjunction with the FilteredClassifier. The
            value can also be converted back using 'originalValue(double value)'
            procedure.<br/><br/>Options:<br/><br/>-R &lt;seed&gt; = 	Specify the seed of
            randomization<br/>	used to randomize the class<br/>	order (default: 1)<br/>-C
            &lt;order&gt; = 	Specify the class order to be<br/>	sorted, could be 0:
            ascending<br/>	1: descending and 2: random.(default: 0)
            </summary>
        </member>
        <!-- Badly formed XML comment ignored for member "P:Ml2.Fltr.FiltersSupervisedAttribute.Discretize" -->
        <member name="P:Ml2.Fltr.FiltersSupervisedAttribute.NominalToBinary">
            <summary>
            Converts all nominal attributes into binary numeric attributes. An
            attribute with k values is transformed into k binary attributes if the class is
            nominal (using the one-attribute-per-value approach). Binary attributes are
            left binary, if option '-A' is not given.If the class is numeric, k - 1 new
            binary attributes are generated in the manner described in "Classification
            and Regression Trees" by Breiman et al. (i.e. taking the average class
            value associated with each attribute value into account)<br/><br/>For more
            information, see:<br/><br/>L. Breiman, J.H. Friedman, R.A. Olshen, C.J. Stone
            (1984). Classification and Regression Trees. Wadsworth
            Inc.<br/><br/>Options:<br/><br/>-N = 	Sets if binary attributes are to be coded as nominal
            ones.<br/>-A = 	For each nominal value a new attribute is created, <br/>	not
            only if there are more than 2 values.
            </summary>
        </member>
        <member name="P:Ml2.Fltr.FiltersSupervisedInstance.Resample">
            <summary>
            Produces a random subsample of a dataset using either sampling with
            replacement or without replacement.<br/>The original dataset must fit entirely
            in memory. The number of instances in the generated dataset may be
            specified. The dataset must have a nominal class attribute. If not, use the
            unsupervised version. The filter can be made to maintain the class distribution in
            the subsample, or to bias the class distribution toward a uniform
            distribution. When used in batch mode (i.e. in the FilteredClassifier), subsequent
            batches are NOT resampled.<br/><br/>Options:<br/><br/>-S &lt;num&gt; =
            	Specify the random number seed (default 1)<br/>-Z &lt;num&gt; = 	The size of
            the output dataset, as a percentage of<br/>	the input dataset (default
            100)<br/>-B &lt;num&gt; = 	Bias factor towards uniform class distribution.<br/>	0
            = distribution in input data -- 1 = uniform distribution.<br/>	(default
            0)<br/>-no-replacement = 	Disables replacement of instances<br/>	(default:
            with replacement)<br/>-V = 	Inverts the selection - only available with
            '-no-replacement'.
            </summary>
        </member>
        <member name="P:Ml2.Fltr.FiltersSupervisedInstance.SpreadSubsample">
            <summary>
            Produces a random subsample of a dataset. The original dataset must fit
            entirely in memory. This filter allows you to specify the maximum "spread"
            between the rarest and most common class. For example, you may specify that
            there be at most a 2:1 difference in class frequencies. When used in batch
            mode, subsequent batches are NOT resampled.<br/><br/>Options:<br/><br/>-S
            &lt;num&gt; = 	Specify the random number seed (default 1)<br/>-M &lt;num&gt;
            = 	The maximum class distribution spread.<br/>	0 = no maximum spread, 1 =
            uniform distribution, 10 = allow at most<br/>	a 10:1 ratio between the
            classes (default 0)<br/>-W = 	Adjust weights so that total weight per class is
            maintained.<br/>	Individual instance weighting is not preserved. (default
            no<br/>	weights adjustment<br/>-X &lt;num&gt; = 	The maximum count for any
            class value (default 0 = unlimited).<br/>
            </summary>
        </member>
        <member name="P:Ml2.Fltr.FiltersSupervisedInstance.StratifiedRemoveFolds">
            <summary>
            This filter takes a dataset and outputs a specified fold for cross
            validation. If you do not want the folds to be stratified use the unsupervised
            version.<br/><br/>Options:<br/><br/>-V = 	Specifies if inverse of selection
            is to be output.<br/><br/>-N &lt;number of folds&gt; = 	Specifies number of
            folds dataset is split into. <br/>	(default 10)<br/><br/>-F &lt;fold&gt; =
            	Specifies which fold is selected. (default 1)<br/><br/>-S &lt;seed&gt; =
            	Specifies random number seed. (default 0, no randomizing)<br/>
            </summary>
        </member>
        <member name="P:Ml2.Fltr.FiltersSupervisedInstance.SMOTE">
            <summary>
            Resamples a dataset by applying the Synthetic Minority Oversampling
            TEchnique (SMOTE). The original dataset must fit entirely in memory. The amount
            of SMOTE and number of nearest neighbors may be specified. For more
            information, see <br/><br/>Nitesh V. Chawla et. al. (2002). Synthetic Minority
            Over-sampling Technique. Journal of Artificial Intelligence Research.
            16:321-357.<br/><br/>Options:<br/><br/>-S &lt;num&gt; = 	Specifies the random
            number seed<br/>	(default 1)<br/>-P &lt;percentage&gt; = 	Specifies percentage
            of SMOTE instances to create.<br/>	(default 100.0)<br/><br/>-K
            &lt;nearest-neighbors&gt; = 	Specifies the number of nearest neighbors to
            use.<br/>	(default 5)<br/><br/>-C &lt;value-index&gt; = 	Specifies the index of the
            nominal class value to SMOTE<br/>	(default 0: auto-detect non-empty minority
            class))<br/>
            </summary>
        </member>
        <member name="P:Ml2.Fltr.FiltersUnsupervisedAttribute.Add">
            <summary>
            An instance filter that adds a new attribute to the dataset. The new
            attribute will contain all missing values.<br/><br/>Options:<br/><br/>-T
            &lt;NUM|NOM|STR|DAT&gt; = 	The type of attribute to create:<br/>	NUM = Numeric
            attribute<br/>	NOM = Nominal attribute<br/>	STR = String attribute<br/>	DAT =
            Date attribute<br/>	(default: NUM)<br/>-C &lt;index&gt; = 	Specify where
            to insert the column. First and last<br/>	are valid indexes.(default:
            last)<br/>-N &lt;name&gt; = 	Name of the new attribute.<br/>	(default:
            'Unnamed')<br/>-L &lt;label1,label2,...&gt; = 	Create nominal attribute with given
            labels<br/>	(default: numeric attribute)<br/>-F &lt;format&gt; = 	The format
            of the date values (see ISO-8601)<br/>	(default: yyyy-MM-dd'T'HH:mm:ss)
            </summary>
        </member>
        <member name="P:Ml2.Fltr.FiltersUnsupervisedAttribute.AddCluster">
            <summary>
            A filter that adds a new nominal attribute representing the cluster
            assigned to each instance by the specified clustering algorithm.<br/>Either the
            clustering algorithm gets built with the first batch of data or one
            specifies are serialized clusterer model file to use
            instead.<br/><br/>Options:<br/><br/>-W &lt;clusterer specification&gt; = 	Full class name of clusterer
            to use, followed<br/>	by scheme options.
            eg:<br/>		"weka.clusterers.SimpleKMeans -N 3"<br/>	(default: weka.clusterers.SimpleKMeans)<br/>-serialized
            &lt;file&gt; = 	Instead of building a clusterer on the data, one can also
            provide<br/>	a serialized model and use that for adding the clusters.<br/>-I
            &lt;att1,att2-att4,...&gt; = 	The range of attributes the clusterer should
            ignore.<br/>
            </summary>
        </member>
        <member name="P:Ml2.Fltr.FiltersUnsupervisedAttribute.AddExpression">
            <summary>
            An instance filter that creates a new attribute by applying a
            mathematical expression to existing attributes. The expression can contain attribute
            references and numeric constants. Supported operators are :<br/>+, -, *, /,
            ^, log, abs, cos, exp, sqrt, floor, ceil, rint, tan, sin, (,
            )<br/>Attributes are specified by prefixing with 'a', eg. a7 is attribute number 7
            (starting from 1).<br/>Example expression :
            a1^2*a5/log(a7*4.0).<br/><br/>Options:<br/><br/>-E &lt;expression&gt; = 	Specify the expression to apply. Eg
            a1^2*a5/log(a7*4.0).<br/>	Supported opperators: ,+, -, *, /, ^, log, abs, cos,
            <br/>	exp, sqrt, floor, ceil, rint, tan, sin, (, )<br/>	(default:
            a1^2)<br/>-N &lt;name&gt; = 	Specify the name for the new attribute. (default is
            the expression provided with -E)<br/>-D = 	Debug. Names attribute with the
            postfix parse of the expression.
            </summary>
        </member>
        <member name="P:Ml2.Fltr.FiltersUnsupervisedAttribute.AddID">
            <summary>
            An instance filter that adds an ID attribute to the dataset. The new
            attribute contains a unique ID for each instance.<br/>Note: The ID is not reset
            for the second batch of files (using -b and -r and
            -s).<br/><br/>Options:<br/><br/>-C &lt;index&gt; = 	Specify where to insert the ID. First and
            last<br/>	are valid indexes.(default first)<br/>-N &lt;name&gt; = 	Name of the
            new attribute.<br/>	(default = 'ID')
            </summary>
        </member>
        <member name="P:Ml2.Fltr.FiltersUnsupervisedAttribute.AddNoise">
            <summary>
            An instance filter that changes a percentage of a given attributes
            values. The attribute must be nominal. Missing value can be treated as value
            itself.<br/><br/>Options:<br/><br/>-C &lt;col&gt; = 	Index of the attribute to
            be changed <br/>	(default last attribute)<br/>-M = 	Treat missing values as
            an extra value <br/><br/>-P &lt;num&gt; = 	Specify the percentage of noise
            introduced <br/>	to the data (default 10)<br/>-S &lt;num&gt; = 	Specify
            the random number seed (default 1)
            </summary>
        </member>
        <member name="P:Ml2.Fltr.FiltersUnsupervisedAttribute.AddUserFields">
            <summary>
            A filter that adds new attributes with user specified type and constant
            value. Numeric, nominal, string and date attributes can be created.
            Attribute name, and value can be set with environment variables. Date attributes
            can also specify a formatting string by which to parse the supplied date
            value. Alternatively, a current time stamp can be specified by supplying the
            special string "now" as the value for a date
            attribute.<br/><br/>Options:<br/><br/>-A &lt;name:type:value&gt; = 	New field specification
            (name@type@value).<br/>	 Environment variables may be used for any/all parts of
            the<br/>	specification. Type can be one of (numeric, nominal, string or
            date).<br/>	The value for date be a specific date string or the special
            string<br/>	"now" to indicate the current date-time. A specific date format<br/>	string for
            parsing specific date values can be specified by suffixing<br/>	the type
            specification - e.g. "myTime@date:MM-dd-yyyy@08-23-2009".This option may be
            specified multiple times
            </summary>
        </member>
        <member name="P:Ml2.Fltr.FiltersUnsupervisedAttribute.AddValues">
            <summary>
            Adds the labels from the given list to an attribute if they are missing.
            The labels can also be sorted in an ascending manner. If no labels are
            provided then only the (optional) sorting
            applies.<br/><br/>Options:<br/><br/>-C &lt;col&gt; = 	Sets the attribute index<br/>	(default last).<br/>-L
            &lt;label1,label2,...&gt; = 	Comma-separated list of labels to
            add.<br/>	(default: none)<br/>-S = 	Turns on the sorting of the labels.
            </summary>
        </member>
        <member name="P:Ml2.Fltr.FiltersUnsupervisedAttribute.Center">
            <summary>
            Centers all numeric attributes in the given dataset to have zero mean
            (apart from the class attribute, if
            set).<br/><br/>Options:<br/><br/>-unset-class-temporarily = 	Unsets the class index temporarily before the filter
            is<br/>	applied to the data.<br/>	(default: no)
            </summary>
        </member>
        <member name="P:Ml2.Fltr.FiltersUnsupervisedAttribute.ChangeDateFormat">
            <summary>
            Changes the date format used by a date attribute. This is most useful for
            converting to a format with less precision, for example, from an absolute
            date to day of year, etc. This changes the format string, and changes the
            date values to those that would be parsed by the new
            format.<br/><br/>Options:<br/><br/>-C &lt;col&gt; = 	Sets the attribute index (default
            last).<br/>-F &lt;value index&gt; = 	Sets the output date format string (default
            corresponds to ISO-8601).
            </summary>
        </member>
        <member name="P:Ml2.Fltr.FiltersUnsupervisedAttribute.ClassAssigner">
            <summary>
            Filter that can set and unset the class
            index.<br/><br/>Options:<br/><br/>-D = 	Turns on output of debugging information.<br/>-C
            &lt;num|first|last|0&gt; = 	The index of the class attribute. Index starts with 1,
            'first'<br/>	and 'last' are accepted, '0' unsets the class index.<br/>	(default: last)
            </summary>
        </member>
        <member name="P:Ml2.Fltr.FiltersUnsupervisedAttribute.ClusterMembership">
            <summary>
            A filter that uses a density-based clusterer to generate cluster
            membership values; filtered instances are composed of these values plus the class
            attribute (if set in the input data). If a (nominal) class attribute is set,
            the clusterer is run separately for each class. The class attribute (if
            set) and any user-specified attributes are ignored during the clustering
            operation<br/><br/>Options:<br/><br/>-W &lt;clusterer name&gt; = 	Full name of
            clusterer to use. eg:<br/>		weka.clusterers.EM<br/>	Additional options
            after the '--'.<br/>	(default: weka.clusterers.EM)<br/>-I
            &lt;att1,att2-att4,...&gt; = 	The range of attributes the clusterer should ignore.<br/>	(the
            class attribute is automatically ignored)
            </summary>
        </member>
        <member name="P:Ml2.Fltr.FiltersUnsupervisedAttribute.Copy">
            <summary>
            An instance filter that copies a range of attributes in the dataset. This
            is used in conjunction with other filters that overwrite attribute values
            during the course of their operation -- this filter allows the original
            attributes to be kept as well as the new
            attributes.<br/><br/>Options:<br/><br/>-R &lt;index1,index2-index4,...&gt; = 	Specify list of columns to copy.
            First and last are valid<br/>	indexes. (default none)<br/>-V = 	Invert
            matching sense (i.e. copy all non-specified columns)
            </summary>
        </member>
        <member name="P:Ml2.Fltr.FiltersUnsupervisedAttribute.Discretize">
            <summary>
            An instance filter that discretizes a range of numeric attributes in the
            dataset into nominal attributes. Discretization is by simple binning. Skips
            the class attribute if
            set.<br/><br/>Options:<br/><br/>-unset-class-temporarily = 	Unsets the class index temporarily before the filter
            is<br/>	applied to the data.<br/>	(default: no)<br/>-B &lt;num&gt; = 	Specifies the
            (maximum) number of bins to divide numeric attributes into.<br/>	(default =
            10)<br/>-M &lt;num&gt; = 	Specifies the desired weight of instances per bin
            for<br/>	equal-frequency binning. If this is set to a positive<br/>	number
            then the -B option will be ignored.<br/>	(default = -1)<br/>-F = 	Use
            equal-frequency instead of equal-width discretization.<br/>-O = 	Optimize number
            of bins using leave-one-out estimate<br/>	of estimated entropy (for
            equal-width discretization).<br/>	If this is set then the -B option will be
            ignored.<br/>-R &lt;col1,col2-col4,...&gt; = 	Specifies list of columns to
            Discretize. First and last are valid indexes.<br/>	(default: first-last)<br/>-V =
            	Invert matching sense of column indexes.<br/>-D = 	Output binary
            attributes for discretized attributes.<br/>-Y = 	Use bin numbers rather than ranges
            for discretized attributes.
            </summary>
        </member>
        <member name="P:Ml2.Fltr.FiltersUnsupervisedAttribute.FirstOrder">
            <summary>
            This instance filter takes a range of N numeric attributes and replaces
            them with N-1 numeric attributes, the values of which are the difference
            between consecutive attribute values from the original instance. eg:
            <br/><br/>Original attribute values<br/><br/> 0.1, 0.2, 0.3, 0.1, 0.3<br/><br/>New
            attribute values<br/><br/> 0.1, 0.1, -0.2, 0.2<br/><br/>The range of
            attributes used is taken in numeric order. That is, a range spec of 7-11,3-5 will
            use the attribute ordering 3,4,5,7,8,9,10,11 for the differences, NOT
            7,8,9,10,11,3,4,5.<br/><br/>Options:<br/><br/>-R
            &lt;index1,index2-index4,...&gt; = 	Specify list of columns to take the differences between.<br/>	First
            and last are valid indexes.<br/>	(default none)
            </summary>
        </member>
        <!-- Badly formed XML comment ignored for member "P:Ml2.Fltr.FiltersUnsupervisedAttribute.InterquartileRange" -->
        <member name="P:Ml2.Fltr.FiltersUnsupervisedAttribute.KernelFilter">
            <summary>
            Converts the given set of predictor variables into a kernel matrix. The
            class value remains unchangedm, as long as the preprocessing filter doesn't
            change it.<br/>By default, the data is preprocessed with the Center filter,
            but the user can choose any filter (NB: one must be careful that the
            filter does not alter the class attribute unintentionally). With
            weka.filters.AllFilter the preprocessing gets disabled.<br/><br/>For more information
            regarding preprocessing the data, see:<br/><br/>K.P. Bennett, M.J. Embrechts:
            An Optimization Perspective on Kernel Partial Least Squares Regression. In:
            Advances in Learning Theory: Methods, Models and Applications, 227-249,
            2003.<br/><br/>Options:<br/><br/>-D = 	Turns on output of debugging
            information.<br/>-no-checks = 	Turns off all checks - use with caution!<br/>	Turning
            them off assumes that data is purely numeric, doesn't<br/>	contain any
            missing values, and has a nominal class. Turning them<br/>	off also means that
            no header information will be stored if the<br/>	machine is linear.
            Finally, it also assumes that no instance has<br/>	a weight equal to
            0.<br/>	(default: checks on)<br/>-F &lt;filename&gt; = 	The file to initialize the
            filter with (optional).<br/>-C &lt;num&gt; = 	The class index for the file to
            initialize with,<br/>	First and last are valid (optional, default:
            last).<br/>-K &lt;classname and parameters&gt; = 	The Kernel to use.<br/>	(default:
            weka.classifiers.functions.supportVector.PolyKernel)<br/>-kernel-factor =
            	Defines a factor for the kernel.<br/>		- RBFKernel: a factor for
            gamma<br/>			Standardize: 1/(2*N)<br/>			Normalize..: 6/N<br/>	Available parameters
            are:<br/>		N for # of instances, A for # of attributes<br/>	(default:
            1)<br/>-P &lt;classname and parameters&gt; = 	The Filter used for preprocessing
            (use weka.filters.AllFilter<br/>	to disable preprocessing).<br/>	(default:
            weka.filters.unsupervised.attribute.Center)<br/><br/>Options specific to
            kernel weka.classifiers.functions.supportVector.PolyKernel: = <br/>-D =
            	Enables debugging output (if available) to be printed.<br/>	(default:
            off)<br/>-no-checks = 	Turns off all checks - use with caution!<br/>	(default: checks
            on)<br/>-C &lt;num&gt; = 	The size of the cache (a prime number), 0 for
            full cache and <br/>	-1 to turn it off.<br/>	(default: 250007)<br/>-E
            &lt;num&gt; = 	The Exponent to use.<br/>	(default: 1.0)<br/>-L = 	Use lower-order
            terms.<br/>	(default: no)<br/><br/>Options specific to preprocessing filter
            weka.filters.unsupervised.attribute.Center: = <br/>-unset-class-temporarily
            = 	Unsets the class index temporarily before the filter is<br/>	applied to
            the data.<br/>	(default: no)
            </summary>
        </member>
        <member name="P:Ml2.Fltr.FiltersUnsupervisedAttribute.MakeIndicator">
            <summary>
            A filter that creates a new dataset with a boolean attribute replacing a
            nominal attribute. In the new dataset, a value of 1 is assigned to an
            instance that exhibits a particular range of attribute values, a 0 to an
            instance that doesn't. The boolean attribute is coded as numeric by
            default.<br/><br/>Options:<br/><br/>-C &lt;col&gt; = 	Sets the attribute index.<br/>-V
            &lt;index1,index2-index4,...&gt; = 	Specify the list of values to indicate.
            First and last are<br/>	valid indexes (default last)<br/>-N &lt;index&gt; =
            	Set if new boolean attribute nominal.
            </summary>
        </member>
        <member name="P:Ml2.Fltr.FiltersUnsupervisedAttribute.MathExpression">
            <summary>
            Modify numeric attributes according to a given expression
            <br/><br/>Options:<br/><br/>-unset-class-temporarily = 	Unsets the class index temporarily
            before the filter is<br/>	applied to the data.<br/>	(default: no)<br/>-E
            &lt;expression&gt; = 	Specify the expression to apply. Eg.
            pow(A,6)/(MEAN+MAX)<br/>	Supported operators are +, -, *, /, pow, log,<br/>	abs, cos, exp,
            sqrt, tan, sin, ceil, floor, rint, (, ), <br/>	MEAN, MAX, MIN, SD, COUNT,
            SUM, SUMSQUARED, ifelse. The 'A'<br/>	letter refers to the value of the
            attribute being processed.<br/>	Other attribute values (numeric only) can be
            accessed through<br/>	the variables A1, A2, A3, ...<br/>-R
            &lt;index1,index2-index4,...&gt; = 	Specify list of columns to ignore. First and last are
            valid<br/>	indexes. (default none)<br/>-V = 	Invert matching sense (i.e. only
            modify specified columns)
            </summary>
        </member>
        <member name="P:Ml2.Fltr.FiltersUnsupervisedAttribute.MergeManyValues">
            <summary>
            Merges many values of a nominal attribute into one
            value.<br/><br/>Options:<br/><br/>-C &lt;col&gt; = 	Sets the attribute index<br/>	(default:
            last)<br/>-L &lt;label&gt; = 	Sets the label of the newly merged
            classes<br/>	(default: 'merged')<br/>-R &lt;range&gt; = 	Sets the merge range. 'first and
            'last' are accepted as well.'<br/>	E.g.: first-5,7,9,20-last<br/>	(default:
            1,2)
            </summary>
        </member>
        <member name="P:Ml2.Fltr.FiltersUnsupervisedAttribute.MergeTwoValues">
            <summary>
            Merges two values of a nominal attribute into one
            value.<br/><br/>Options:<br/><br/>-C &lt;col&gt; = 	Sets the attribute index (default
            last).<br/>-F &lt;value index&gt; = 	Sets the first value's index (default
            first).<br/>-S &lt;value index&gt; = 	Sets the second value's index (default last).
            </summary>
        </member>
        <member name="P:Ml2.Fltr.FiltersUnsupervisedAttribute.NominalToBinary">
            <summary>
            Converts all nominal attributes into binary numeric attributes. An
            attribute with k values is transformed into k binary attributes if the class is
            nominal (using the one-attribute-per-value approach). Binary attributes are
            left binary, if option '-A' is not given.If the class is numeric, you might
            want to use the supervised version of this
            filter.<br/><br/>Options:<br/><br/>-N = 	Sets if binary attributes are to be coded as nominal ones.<br/>-A
            = 	For each nominal value a new attribute is created, <br/>	not only if
            there are more than 2 values.<br/>-R &lt;col1,col2-col4,...&gt; = 	Specifies
            list of columns to act on. First and last are <br/>	valid
            indexes.<br/>	(default: first-last)<br/>-V = 	Invert matching sense of column indexes.
            </summary>
        </member>
        <member name="P:Ml2.Fltr.FiltersUnsupervisedAttribute.NominalToString">
            <summary>
            Converts a nominal attribute (that is, a set number of values) to string
            (that is, an unspecified number of values).<br/><br/>Options:<br/><br/>-C
            &lt;col&gt; = 	Sets the range of attributes to convert (default last).
            </summary>
        </member>
        <member name="P:Ml2.Fltr.FiltersUnsupervisedAttribute.Normalize">
            <summary>
            Normalizes all numeric values in the given dataset (apart from the class
            attribute, if set). The resulting values are by default in [0,1] for the
            data used to compute the normalization intervals. But with the scale and
            translation parameters one can change that, e.g., with scale = 2.0 and
            translation = -1.0 you get values in the range
            [-1,+1].<br/><br/>Options:<br/><br/>-unset-class-temporarily = 	Unsets the class index temporarily before the
            filter is<br/>	applied to the data.<br/>	(default: no)<br/>-S &lt;num&gt; =
            	The scaling factor for the output range.<br/>	(default: 1.0)<br/>-T
            &lt;num&gt; = 	The translation of the output range.<br/>	(default: 0.0)
            </summary>
        </member>
        <member name="P:Ml2.Fltr.FiltersUnsupervisedAttribute.NumericCleaner">
            <summary>
            A filter that 'cleanses' the numeric data from values that are too small,
            too big or very close to a certain value (e.g., 0) and sets these values
            to a pre-defined default.<br/><br/>Options:<br/><br/>-D = 	Turns on output
            of debugging information.<br/>-min &lt;double&gt; = 	The minimum threshold.
            (default -Double.MAX_VALUE)<br/>-min-default &lt;double&gt; = 	The
            replacement for values smaller than the minimum threshold.<br/>	(default
            -Double.MAX_VALUE)<br/>-max &lt;double&gt; = 	The maximum threshold. (default
            Double.MAX_VALUE)<br/>-max-default &lt;double&gt; = 	The replacement for values
            larger than the maximum threshold.<br/>	(default
            Double.MAX_VALUE)<br/>-closeto &lt;double&gt; = 	The number values are checked for closeness. (default
            0)<br/>-closeto-default &lt;double&gt; = 	The replacement for values that
            are close to '-closeto'.<br/>	(default 0)<br/>-closeto-tolerance
            &lt;double&gt; = 	The tolerance below which numbers are considered being close to
            <br/>	to each other. (default 1E-6)<br/>-decimals &lt;int&gt; = 	The number of
            decimals to round to, -1 means no rounding at all.<br/>	(default -1)<br/>-R
            &lt;col1,col2,...&gt; = 	The list of columns to cleanse, e.g., first-last
            or first-3,5-last.<br/>	(default first-last)<br/>-V = 	Inverts the matching
            sense.<br/>-include-class = 	Whether to include the class in the
            cleansing.<br/>	The class column will always be skipped, if this flag is
            not<br/>	present. (default no)
            </summary>
        </member>
        <member name="P:Ml2.Fltr.FiltersUnsupervisedAttribute.NumericToBinary">
            <summary>
            Converts all numeric attributes into binary attributes (apart from the
            class attribute, if set): if the value of the numeric attribute is exactly
            zero, the value of the new attribute will be zero. If the value of the
            numeric attribute is missing, the value of the new attribute will be missing.
            Otherwise, the value of the new attribute will be one. The new attributes will
            be nominal.<br/><br/>Options:<br/><br/>-unset-class-temporarily = 	Unsets
            the class index temporarily before the filter is<br/>	applied to the
            data.<br/>	(default: no)
            </summary>
        </member>
        <member name="P:Ml2.Fltr.FiltersUnsupervisedAttribute.NumericToNominal">
            <summary>
            A filter for turning numeric attributes into nominal ones. Unlike
            discretization, it just takes all numeric values and adds them to the list of
            nominal values of that attribute. Useful after CSV imports, to enforce certain
            attributes to become nominal, e.g., the class attribute, containing values
            from 1 to 5.<br/><br/>Options:<br/><br/>-R &lt;col1,col2-col4,...&gt; =
            	Specifies list of columns to Discretize. First and last are valid
            indexes.<br/>	(default: first-last)<br/>-V = 	Invert matching sense of column indexes.
            </summary>
        </member>
        <member name="P:Ml2.Fltr.FiltersUnsupervisedAttribute.NumericTransform">
            <summary>
            Transforms numeric attributes using a given transformation
            method.<br/><br/>Options:<br/><br/>-R &lt;index1,index2-index4,...&gt; = 	Specify list of
            columns to transform. First and last are<br/>	valid indexes (default
            none). Non-numeric columns are <br/>	skipped.<br/>-V = 	Invert matching
            sense.<br/>-C &lt;string&gt; = 	Sets the class containing transformation
            method.<br/>	(default java.lang.Math)<br/>-M &lt;string&gt; = 	Sets the method.
            (default abs)
            </summary>
        </member>
        <member name="P:Ml2.Fltr.FiltersUnsupervisedAttribute.Obfuscate">
            <summary>
            A simple instance filter that renames the relation, all attribute names
            and all nominal (and string) attribute values. For exchanging sensitive
            datasets. Currently doesn't like string or relational attributes.
            </summary>
        </member>
        <member name="P:Ml2.Fltr.FiltersUnsupervisedAttribute.PKIDiscretize">
            <summary>
            Discretizes numeric attributes using equal frequency binning, where the
            number of bins is equal to the square root of the number of non-missing
            values.<br/><br/>For more information, see:<br/><br/>Ying Yang, Geoffrey I.
            Webb: Proportional k-Interval Discretization for Naive-Bayes Classifiers. In:
            12th European Conference on Machine Learning, 564-575,
            2001.<br/><br/>Options:<br/><br/>-unset-class-temporarily = 	Unsets the class index temporarily
            before the filter is<br/>	applied to the data.<br/>	(default: no)<br/>-R
            &lt;col1,col2-col4,...&gt; = 	Specifies list of columns to Discretize. First
            and last are valid indexes.<br/>	(default: first-last)<br/>-V = 	Invert
            matching sense of column indexes.<br/>-D = 	Output binary attributes for
            discretized attributes.
            </summary>
        </member>
        <member name="P:Ml2.Fltr.FiltersUnsupervisedAttribute.PartitionMembership">
            <summary>
            A filter that uses a PartitionGenerator to generate partition membership
            values; filtered instances are composed of these values plus the class
            attribute (if set in the input data) and rendered as sparse
            instances.<br/><br/>Options:<br/><br/>-W &lt;name of partition generator&gt; = 	Full name of
            partition generator to use,
            e.g.:<br/>		weka.classifiers.trees.J48<br/>	Additional options after the '--'.<br/>	(default: weka.classifiers.trees.J48)
            </summary>
        </member>
        <member name="P:Ml2.Fltr.FiltersUnsupervisedAttribute.PartitionedMultiFilter">
            <summary>
            A filter that applies filters on subsets of attributes and assembles the
            output into a new dataset. Attributes that are not covered by any of the
            ranges can be either retained or removed from the
            output.<br/><br/>Options:<br/><br/>-D = 	Turns on output of debugging information.<br/>-F
            &lt;classname [options]&gt; = 	A filter to apply (can be specified multiple
            times).<br/>-R &lt;range&gt; = 	An attribute range (can be specified multiple
            times).<br/>	For each filter a range must be supplied. 'first' and 'last'<br/>	are
            valid indices. 'inv(...)' around the range denotes an<br/>	inverted
            range.<br/>-U = 	Flag for leaving unused attributes out of the output, by
            default<br/>	these are included in the filter output.
            </summary>
        </member>
        <member name="P:Ml2.Fltr.FiltersUnsupervisedAttribute.PrincipalComponents">
            <summary>
            Performs a principal components analysis and transformation of the
            data.<br/>Dimensionality reduction is accomplished by choosing enough
            eigenvectors to account for some percentage of the variance in the original data --
            default 0.95 (95%).<br/>Based on code of the attribute selection scheme
            'PrincipalComponents' by Mark Hall and Gabi
            Schmidberger.<br/><br/>Options:<br/><br/>-C = 	Center (rather than standardize) the<br/>	data and compute PCA
            using the covariance (rather<br/>	 than the correlation) matrix.<br/>-R
            &lt;num&gt; = 	Retain enough PC attributes to account<br/>	for this proportion
            of variance in the original data.<br/>	(default: 0.95)<br/>-A &lt;num&gt; =
            	Maximum number of attributes to include in <br/>	transformed attribute
            names.<br/>	(-1 = include all, default: 5)<br/>-M &lt;num&gt; = 	Maximum
            number of PC attributes to retain.<br/>	(-1 = include all, default: -1)
            </summary>
        </member>
        <member name="P:Ml2.Fltr.FiltersUnsupervisedAttribute.RandomProjection">
            <summary>
            Reduces the dimensionality of the data by projecting it onto a lower
            dimensional subspace using a random matrix with columns of unit length (i.e. It
            will reduce the number of attributes in the data while preserving much of
            its variation like PCA, but at a much less computational cost).<br/>It
            first applies the NominalToBinary filter to convert all attributes to numeric
            before reducing the dimension. It preserves the class
            attribute.<br/><br/>For more information, see:<br/><br/>Dmitriy Fradkin, David Madigan:
            Experiments with random projections for machine learning. In: KDD '03: Proceedings
            of the ninth ACM SIGKDD international conference on Knowledge discovery and
            data mining, New York, NY, USA, 517-522, 003.<br/><br/>Options:<br/><br/>-N
            &lt;number&gt; = 	The number of dimensions (attributes) the data should be
            reduced to<br/>	(default 10; exclusive of the class attribute, if it is
            set).<br/>-D [SPARSE1|SPARSE2|GAUSSIAN] = 	The distribution to use for
            calculating the random matrix.<br/>	Sparse1 is:<br/>	 sqrt(3)*{-1 with prob(1/6),
            0 with prob(2/3), +1 with prob(1/6)}<br/>	Sparse2 is:<br/>	 {-1 with
            prob(1/2), +1 with prob(1/2)}<br/><br/>-P &lt;percent&gt; = 	The percentage of
            dimensions (attributes) the data should<br/>	be reduced to (exclusive of the
            class attribute, if it is set). This -N<br/>	option is ignored if this
            option is present or is greater<br/>	than zero.<br/>-M = 	Replace missing
            values using the ReplaceMissingValues filter<br/>-R &lt;num&gt; = 	The random
            seed for the random number generator used for<br/>	calculating the random
            matrix (default 42).
            </summary>
        </member>
        <!-- Badly formed XML comment ignored for member "P:Ml2.Fltr.FiltersUnsupervisedAttribute.RandomSubset" -->
        <member name="P:Ml2.Fltr.FiltersUnsupervisedAttribute.Remove">
            <summary>
            A filter that removes a range of attributes from the dataset. Will
            re-order the remaining attributes if invert matching sense is turned on and the
            attribute column indices are not specified in ascending
            order.<br/><br/>Options:<br/><br/>-R &lt;index1,index2-index4,...&gt; = 	Specify list of
            columns to delete. First and last are valid<br/>	indexes. (default none)<br/>-V =
            	Invert matching sense (i.e. only keep specified columns)
            </summary>
        </member>
        <member name="P:Ml2.Fltr.FiltersUnsupervisedAttribute.RemoveByName">
            <summary>
            Removes attributes based on a regular expression matched against their
            names.<br/><br/>Options:<br/><br/>-D = 	Turns on output of debugging
            information.<br/>-E &lt;regular expression&gt; = 	The regular expression to match
            the attribute names against.<br/>	(default: ^.*id$)<br/>-V = 	Flag for
            inverting the matching sense. If set, attributes are kept<br/>	instead of
            deleted.<br/>	(default: off)
            </summary>
        </member>
        <member name="P:Ml2.Fltr.FiltersUnsupervisedAttribute.RemoveType">
            <summary>
            Removes attributes of a given type.<br/><br/>Options:<br/><br/>-T
            &lt;nominal|numeric|string|date|relational&gt; = 	Attribute type to delete. Valid
            options are "nominal", <br/>	"numeric", "string", "date" and
            "relational".<br/>	(default "string")<br/>-V = 	Invert matching sense (i.e. only keep
            specified columns)
            </summary>
        </member>
        <member name="P:Ml2.Fltr.FiltersUnsupervisedAttribute.RemoveUseless">
            <summary>
            This filter removes attributes that do not vary at all or that vary too
            much. All constant attributes are deleted automatically, along with any that
            exceed the maximum percentage of variance parameter. The maximum variance
            test is only applied to nominal attributes.<br/><br/>Options:<br/><br/>-M
            &lt;max variance %&gt; = 	Maximum variance percentage allowed (default 99)
            </summary>
        </member>
        <!-- Badly formed XML comment ignored for member "P:Ml2.Fltr.FiltersUnsupervisedAttribute.RenameAttribute" -->
        <member name="P:Ml2.Fltr.FiltersUnsupervisedAttribute.Reorder">
            <summary>
            A filter that generates output with a new order of the attributes. Useful
            if one wants to move an attribute to the end to use it as class attribute
            (e.g. with using "-R 2-last,1").<br/>But it's not only possible to change
            the order of all the attributes, but also to leave out attributes. E.g. if
            you have 10 attributes, you can generate the following output order:
            1,3,5,7,9,10 or 10,1-5.<br/>You can also duplicate attributes, e.g. for further
            processing later on: e.g. 1,1,1,4,4,4,2,2,2 where the second and the third
            column of each attribute are processed differently and the first one, i.e.
            the original one is kept.<br/>One can simply inverse the order of the
            attributes via 'last-first'.<br/>After appyling the filter, the index of the class
            attribute is the last attribute.<br/><br/>Options:<br/><br/>-R
            &lt;index1,index2-index4,...&gt; = 	Specify list of columns to copy. First and last
            are valid<br/>	indexes. (default first-last)
            </summary>
        </member>
        <member name="P:Ml2.Fltr.FiltersUnsupervisedAttribute.ReplaceMissingValues">
            <summary>
            Replaces all missing values for nominal and numeric attributes in a
            dataset with the modes and means from the training
            data.<br/><br/>Options:<br/><br/>-unset-class-temporarily = 	Unsets the class index temporarily before
            the filter is<br/>	applied to the data.<br/>	(default: no)
            </summary>
        </member>
        <member name="P:Ml2.Fltr.FiltersUnsupervisedAttribute.ReplaceMissingWithUserConstant">
            <summary>
            Replaces all missing values for nominal, string, numeric and date
            attributes in the dataset with user-supplied constant
            values.<br/><br/>Options:<br/><br/>-A &lt;index1,index2-index4,... | att-name1,att-name2,...&gt; =
            	Specify list of attributes to replace missing values for <br/>	(as weka range
            list of indices or a comma separated list of attribute
            names).<br/>	(default: consider all attributes)<br/>-N = 	Specify the replacement constant for
            nominal/string attributes<br/>-R = 	Specify the replacement constant for
            numeric attributes<br/>	(default: 0)<br/>-D = 	Specify the replacement
            constant for date attributes<br/>-F = 	Specify the date format for parsing the
            replacement date constant<br/>	(default:
            yyyy-MM-dd'T'HH:mm:ss)<br/>-unset-class-temporarily = 	Unsets the class index temporarily before the filter
            is<br/>	applied to the data.<br/>	(default: no)
            </summary>
        </member>
        <member name="P:Ml2.Fltr.FiltersUnsupervisedAttribute.SortLabels">
            <summary>
            A simple filter for sorting the labels of nominal
            attributes.<br/><br/>Options:<br/><br/>-D = 	Turns on output of debugging information.<br/>-R
            &lt;index1,index2-index4,...&gt; = 	Specify list of string attributes to
            convert to words.<br/>	(default: select all relational attributes)<br/>-V =
            	Inverts the matching sense of the selection.<br/>-S &lt;CASE|NON-CASE&gt; =
            	Determines the type of sorting:<br/>	CASE = Case-sensitive<br/>	NON-CASE =
            Case-insensitive<br/>	(default: CASE)
            </summary>
        </member>
        <member name="P:Ml2.Fltr.FiltersUnsupervisedAttribute.Standardize">
            <summary>
            Standardizes all numeric attributes in the given dataset to have zero
            mean and unit variance (apart from the class attribute, if
            set).<br/><br/>Options:<br/><br/>-unset-class-temporarily = 	Unsets the class index
            temporarily before the filter is<br/>	applied to the data.<br/>	(default: no)
            </summary>
        </member>
        <member name="P:Ml2.Fltr.FiltersUnsupervisedAttribute.StringToNominal">
            <summary>
            Converts a range of string attributes (unspecified number of values) to
            nominal (set number of values). You should ensure that all string values
            that will appear are represented in the first batch of the
            data.<br/><br/>Options:<br/><br/>-R &lt;col&gt; = 	Sets the range of attribute indices
            (default last).<br/>-V &lt;col&gt; = 	Invert the range specified by -R.
            </summary>
        </member>
        <member name="P:Ml2.Fltr.FiltersUnsupervisedAttribute.StringToWordVector">
            <summary>
            Converts String attributes into a set of attributes representing word
            occurrence (depending on the tokenizer) information from the text contained in
            the strings. The set of words (attributes) is determined by the first
            batch filtered (typically training data).<br/><br/>Options:<br/><br/>-C =
            	Output word counts rather than boolean word presence.<br/><br/>-R
            &lt;index1,index2-index4,...&gt; = 	Specify list of string attributes to convert to
            words (as weka Range).<br/>	(default: select all string attributes)<br/>-V =
            	Invert matching sense of column indexes.<br/>-P &lt;attribute name
            prefix&gt; = 	Specify a prefix for the created attribute names.<br/>	(default:
            "")<br/>-W &lt;number of words to keep&gt; = 	Specify approximate number of word
            fields to create.<br/>	Surplus words will be discarded..<br/>	(default:
            1000)<br/>-prune-rate &lt;rate as a percentage of dataset&gt; = 	Specify the
            rate (e.g., every 10% of the input dataset) at which to periodically prune
            the dictionary.<br/>	-W prunes after creating a full dictionary. You may
            not have enough memory for this approach.<br/>	(default: no periodic
            pruning)<br/>-T = 	Transform the word frequencies into log(1+fij)<br/>	where fij is
            the frequency of word i in jth document(instance).<br/><br/>-I =
            	Transform each word frequency into:<br/>	fij*log(num of Documents/num of documents
            containing word i)<br/>	 where fij if frequency of word i in jth
            document(instance)<br/>-N = 	Whether to 0=not normalize/1=normalize all
            data/2=normalize test data only<br/>	to average length of training documents (default
            0=don't normalize).<br/>-L = 	Convert all tokens to lowercase before adding
            to the dictionary.<br/>-S = 	Ignore words that are in the
            stoplist.<br/>-stemmer &lt;spec&gt; = 	The stemmering algorihtm (classname plus parameters)
            to use.<br/>-M &lt;int&gt; = 	The minimum term frequency (default =
            1).<br/>-O = 	If this is set, the maximum number of words and the <br/>	minimum
            term frequency is not enforced on a per-class <br/>	basis but based on the
            documents in all the classes <br/>	(even if a class attribute is
            set).<br/>-stopwords &lt;file&gt; = 	A file containing stopwords to override the default
            ones.<br/>	Using this option automatically sets the flag ('-S') to use
            the<br/>	stoplist if the file exists.<br/>	Format: one stopword per line,
            lines starting with '#'<br/>	are interpreted as comments and
            ignored.<br/>-tokenizer &lt;spec&gt; = 	The tokenizing algorihtm (classname plus parameters)
            to use.<br/>	(default: weka.core.tokenizers.WordTokenizer)
            </summary>
        </member>
        <member name="P:Ml2.Fltr.FiltersUnsupervisedAttribute.SwapValues">
            <summary>
            Swaps two values of a nominal attribute.<br/><br/>Options:<br/><br/>-C
            &lt;col&gt; = 	Sets the attribute index (default last).<br/>-F &lt;value
            index&gt; = 	Sets the first value's index (default first).<br/>-S &lt;value
            index&gt; = 	Sets the second value's index (default last).
            </summary>
        </member>
        <member name="P:Ml2.Fltr.FiltersUnsupervisedAttribute.TimeSeriesDelta">
            <summary>
            An instance filter that assumes instances form time-series data and
            replaces attribute values in the current instance with the difference between
            the current value and the equivalent attribute attribute value of some
            previous (or future) instance. For instances where the time-shifted value is
            unknown either the instance may be dropped, or missing values used. Skips the
            class attribute if it is set.<br/><br/>Options:<br/><br/>-R
            &lt;index1,index2-index4,...&gt; = 	Specify list of columns to translate in time. First
            and<br/>	last are valid indexes. (default none)<br/>-V = 	Invert matching
            sense (i.e. calculate for all non-specified columns)<br/>-I &lt;num&gt; = 	The
            number of instances forward to translate values<br/>	between. A negative
            number indicates taking values from<br/>	a past instance. (default -1)<br/>-M
            = 	For instances at the beginning or end of the dataset where<br/>	the
            translated values are not known, remove those instances<br/>	(default is to
            use missing values).
            </summary>
        </member>
        <member name="P:Ml2.Fltr.FiltersUnsupervisedAttribute.TimeSeriesTranslate">
            <summary>
            An instance filter that assumes instances form time-series data and
            replaces attribute values in the current instance with the equivalent attribute
            values of some previous (or future) instance. For instances where the
            desired value is unknown either the instance may be dropped, or missing values
            used. Skips the class attribute if it is set.<br/><br/>Options:<br/><br/>-R
            &lt;index1,index2-index4,...&gt; = 	Specify list of columns to translate in
            time. First and<br/>	last are valid indexes. (default none)<br/>-V =
            	Invert matching sense (i.e. calculate for all non-specified columns)<br/>-I
            &lt;num&gt; = 	The number of instances forward to translate
            values<br/>	between. A negative number indicates taking values from<br/>	a past instance.
            (default -1)<br/>-M = 	For instances at the beginning or end of the dataset
            where<br/>	the translated values are not known, remove those
            instances<br/>	(default is to use missing values).
            </summary>
        </member>
        <member name="P:Ml2.Fltr.FiltersUnsupervisedInstance.NonSparseToSparse">
            <summary>
            An instance filter that converts all incoming instances into sparse
            format.<br/><br/>Options:<br/><br/>-M = 	Treat missing values as zero.<br/>-F =
            	Add a dummy first value for nominal attributes.
            </summary>
        </member>
        <member name="P:Ml2.Fltr.FiltersUnsupervisedInstance.Randomize">
            <summary>
            Randomly shuffles the order of instances passed through it. The random
            number generator is reset with the seed value whenever a new set of instances
            is passed in.<br/><br/>Options:<br/><br/>-S &lt;num&gt; = 	Specify the
            random number seed (default 42)
            </summary>
        </member>
        <member name="P:Ml2.Fltr.FiltersUnsupervisedInstance.RemoveFolds">
            <summary>
            This filter takes a dataset and outputs a specified fold for cross
            validation. If you want the folds to be stratified use the supervised
            version.<br/><br/>Options:<br/><br/>-V = 	Specifies if inverse of selection is to be
            output.<br/><br/>-N &lt;number of folds&gt; = 	Specifies number of folds
            dataset is split into. <br/>	(default 10)<br/><br/>-F &lt;fold&gt; =
            	Specifies which fold is selected. (default 1)<br/><br/>-S &lt;seed&gt; = 	Specifies
            random number seed. (default 0, no randomizing)<br/>
            </summary>
        </member>
        <member name="P:Ml2.Fltr.FiltersUnsupervisedInstance.RemoveFrequentValues">
            <summary>
            Determines which values (frequent or infrequent ones) of an (nominal)
            attribute are retained and filters the instances accordingly. In case of
            values with the same frequency, they are kept in the way they appear in the
            original instances object. E.g. if you have the values "1,2,3,4" with the
            frequencies "10,5,5,3" and you chose to keep the 2 most common values, the
            values "1,2" would be returned, since the value "2" comes before "3", even
            though they have the same frequency.<br/><br/>Options:<br/><br/>-C &lt;num&gt; =
            	Choose attribute to be used for selection.<br/>-N &lt;num&gt; = 	Number
            of values to retain for the sepcified attribute, <br/>	i.e. the ones with
            the most instances (default 2).<br/>-L = 	Instead of values with the most
            instances the ones with the <br/>	least are retained.<br/><br/>-H = 	When
            selecting on nominal attributes, removes header<br/>	references to excluded
            values.<br/>-V = 	Invert matching sense.
            </summary>
        </member>
        <!-- Badly formed XML comment ignored for member "P:Ml2.Fltr.FiltersUnsupervisedInstance.RemoveMisclassified" -->
        <member name="P:Ml2.Fltr.FiltersUnsupervisedInstance.RemovePercentage">
            <summary>
            A filter that removes a given percentage of a
            dataset.<br/><br/>Options:<br/><br/>-P &lt;percentage&gt; = 	Specifies percentage of instances to
            select. (default 50)<br/><br/>-V = 	Specifies if inverse of selection is to be
            output.<br/>
            </summary>
        </member>
        <member name="P:Ml2.Fltr.FiltersUnsupervisedInstance.RemoveRange">
            <summary>
            A filter that removes a given range of instances of a
            dataset.<br/><br/>Options:<br/><br/>-R &lt;inst1,inst2-inst4,...&gt; = 	Specifies list of
            instances to select. First and last<br/>	are valid indexes.
            (required)<br/><br/>-V = 	Specifies if inverse of selection is to be output.<br/>
            </summary>
        </member>
        <member name="P:Ml2.Fltr.FiltersUnsupervisedInstance.RemoveWithValues">
            <summary>
            Filters instances according to the value of an
            attribute.<br/><br/>Options:<br/><br/>-C &lt;num&gt; = 	Choose attribute to be used for
            selection.<br/>-S &lt;num&gt; = 	Numeric value to be used for selection on
            numeric<br/>	attribute.<br/>	Instances with values smaller than given value will<br/>	be
            selected. (default 0)<br/>-L &lt;index1,index2-index4,...&gt; = 	Range of
            label indices to be used for selection on<br/>	nominal
            attribute.<br/>	First and last are valid indexes. (default all values)<br/>-M = 	Missing values
            count as a match. This setting is<br/>	independent of the -V
            option.<br/>	(default missing values don't match)<br/>-V = 	Invert matching
            sense.<br/>-H = 	When selecting on nominal attributes, removes header<br/>	references
            to excluded values.<br/>-F = 	Do not apply the filter to instances that
            arrive after the first<br/>	(training) batch. The default is to apply the
            filter (i.e.<br/>	the filter may not return an instance if it matches the remove
            criteria)
            </summary>
        </member>
        <member name="P:Ml2.Fltr.FiltersUnsupervisedInstance.Resample">
            <summary>
            Produces a random subsample of a dataset using either sampling with
            replacement or without replacement. The original dataset must fit entirely in
            memory. The number of instances in the generated dataset may be specified.
            When used in batch mode, subsequent batches are NOT
            resampled.<br/><br/>Options:<br/><br/>-S &lt;num&gt; = 	Specify the random number seed (default
            1)<br/>-Z &lt;num&gt; = 	The size of the output dataset, as a percentage
            of<br/>	the input dataset (default 100)<br/>-no-replacement = 	Disables
            replacement of instances<br/>	(default: with replacement)<br/>-V = 	Inverts the
            selection - only available with '-no-replacement'.
            </summary>
        </member>
        <member name="P:Ml2.Fltr.FiltersUnsupervisedInstance.ReservoirSample">
            <summary>
            Produces a random subsample of a dataset using the reservoir sampling
            Algorithm "R" by Vitter. The original data set does not have to fit into main
            memory, but the reservoir does. <br/><br/>Options:<br/><br/>-S &lt;num&gt;
            = 	Specify the random number seed (default 1)<br/>-Z &lt;num&gt; = 	The
            size of the output dataset - number of instances<br/>	(default 100)
            </summary>
        </member>
        <member name="P:Ml2.Fltr.FiltersUnsupervisedInstance.SparseToNonSparse">
            <summary>
            An instance filter that converts all incoming sparse instances into
            non-sparse format.
            </summary>
        </member>
        <!-- Badly formed XML comment ignored for member "P:Ml2.Fltr.FiltersUnsupervisedInstance.SubsetByExpression" -->
        <member name="P:Ml2.AttrSel.Algs.Algorithms.BestFirst">
            <summary>
            BestFirst:<br/><br/>Searches the space of attribute subsets by greedy
            hillclimbing augmented with a backtracking facility. Setting the number of
            consecutive non-improving nodes allowed controls the level of backtracking
            done. Best first may start with the empty set of attributes and search
            forward, or start with the full set of attributes and search backward, or start at
            any point and search in both directions (by considering all possible
            single attribute additions and deletions at a given
            point).<br/><br/><br/>Options:<br/><br/>-P &lt;start set&gt; = 	Specify a starting set of
            attributes.<br/>	Eg. 1,3,5-7.<br/>-D &lt;0 = backward | 1 = forward | 2 =
            bi-directional&gt; = 	Direction of search. (default = 1).<br/>-N &lt;num&gt; = 	Number of
            non-improving nodes to<br/>	consider before terminating search.<br/>-S
            &lt;num&gt; = 	Size of lookup cache for evaluated subsets.<br/>	Expressed as a
            multiple of the number of<br/>	attributes in the data set. (default = 1)
            </summary>
        </member>
        <member name="P:Ml2.AttrSel.Algs.Algorithms.GreedyStepwise">
            <summary>
            GreedyStepwise :<br/><br/>Performs a greedy forward or backward search
            through the space of attribute subsets. May start with no/all attributes or
            from an arbitrary point in the space. Stops when the addition/deletion of
            any remaining attributes results in a decrease in evaluation. Can also
            produce a ranked list of attributes by traversing the space from one side to the
            other and recording the order that attributes are
            selected.<br/><br/><br/>Options:<br/><br/>-C = 	Use conservative forward search<br/>-B = 	Use a
            backward search instead of a<br/>	forward one.<br/>-P &lt;start set&gt; =
            	Specify a starting set of attributes.<br/>	Eg. 1,3,5-7.<br/>-R = 	Produce a
            ranked list of attributes.<br/>-T &lt;threshold&gt; = 	Specify a theshold by
            which attributes<br/>	may be discarded from the ranking.<br/>	Use in
            conjuction with -R<br/>-N &lt;num to select&gt; = 	Specify number of attributes to
            select
            </summary>
        </member>
        <member name="P:Ml2.AttrSel.Algs.Algorithms.Ranker">
            <summary>
            Ranker : <br/><br/>Ranks attributes by their individual evaluations. Use
            in conjunction with attribute evaluators (ReliefF, GainRatio, Entropy
            etc).<br/><br/><br/>Options:<br/><br/>-P &lt;start set&gt; = 	Specify a starting
            set of attributes.<br/>	Eg. 1,3,5-7.<br/>	Any starting attributes
            specified are<br/>	ignored during the ranking.<br/>-T &lt;threshold&gt; = 	Specify
            a theshold by which attributes<br/>	may be discarded from the
            ranking.<br/>-N &lt;num to select&gt; = 	Specify number of attributes to select
            </summary>
        </member>
        <member name="T:Ml2.AttrSel.Algs.BestFirst">
            <summary>
            BestFirst:<br/><br/>Searches the space of attribute subsets by greedy
            hillclimbing augmented with a backtracking facility. Setting the number of
            consecutive non-improving nodes allowed controls the level of backtracking
            done. Best first may start with the empty set of attributes and search
            forward, or start with the full set of attributes and search backward, or start at
            any point and search in both directions (by considering all possible
            single attribute additions and deletions at a given
            point).<br/><br/><br/>Options:<br/><br/>-P &lt;start set&gt; = 	Specify a starting set of
            attributes.<br/>	Eg. 1,3,5-7.<br/>-D &lt;0 = backward | 1 = forward | 2 =
            bi-directional&gt; = 	Direction of search. (default = 1).<br/>-N &lt;num&gt; = 	Number of
            non-improving nodes to<br/>	consider before terminating search.<br/>-S
            &lt;num&gt; = 	Size of lookup cache for evaluated subsets.<br/>	Expressed as a
            multiple of the number of<br/>	attributes in the data set. (default = 1)
            </summary>
        </member>
        <member name="M:Ml2.AttrSel.Algs.BestFirst.StartSet(System.String)">
            <summary>
            Set the start point for the search. This is specified as a comma
            seperated list off attribute indexes starting at 1. It can include ranges. Eg.
            1,2,5-9,17.
            </summary>    
        </member>
        <member name="M:Ml2.AttrSel.Algs.BestFirst.Direction(Ml2.AttrSel.Algs.BestFirst.EDirection)">
            <summary>
            Set the direction of the search.
            </summary>    
        </member>
        <member name="M:Ml2.AttrSel.Algs.BestFirst.SearchTermination(System.Int32)">
            <summary>
            Set the amount of backtracking. Specify the number of
            </summary>    
        </member>
        <member name="M:Ml2.AttrSel.Algs.BestFirst.LookupCacheSize(System.Int32)">
            <summary>
            Set the maximum size of the lookup cache of evaluated subsets. This is
            expressed as a multiplier of the number of attributes in the data set.
            (default = 1).
            </summary>    
        </member>
        <member name="T:Ml2.AttrSel.Algs.GreedyStepwise">
            <summary>
            GreedyStepwise :<br/><br/>Performs a greedy forward or backward search
            through the space of attribute subsets. May start with no/all attributes or
            from an arbitrary point in the space. Stops when the addition/deletion of
            any remaining attributes results in a decrease in evaluation. Can also
            produce a ranked list of attributes by traversing the space from one side to the
            other and recording the order that attributes are
            selected.<br/><br/><br/>Options:<br/><br/>-C = 	Use conservative forward search<br/>-B = 	Use a
            backward search instead of a<br/>	forward one.<br/>-P &lt;start set&gt; =
            	Specify a starting set of attributes.<br/>	Eg. 1,3,5-7.<br/>-R = 	Produce a
            ranked list of attributes.<br/>-T &lt;threshold&gt; = 	Specify a theshold by
            which attributes<br/>	may be discarded from the ranking.<br/>	Use in
            conjuction with -R<br/>-N &lt;num to select&gt; = 	Specify number of attributes to
            select
            </summary>
        </member>
        <member name="M:Ml2.AttrSel.Algs.GreedyStepwise.GenerateRanking(System.Boolean)">
            <summary>
            Set to true if a ranked list is required.
            </summary>    
        </member>
        <member name="M:Ml2.AttrSel.Algs.GreedyStepwise.SearchBackwards(System.Boolean)">
            <summary>
            Search backwards rather than forwards.
            </summary>    
        </member>
        <member name="M:Ml2.AttrSel.Algs.GreedyStepwise.ConservativeForwardSelection(System.Boolean)">
            <summary>
            If true (and forward search is selected) then attributes will continue to
            be added to the best subset as long as merit does not degrade.
            </summary>    
        </member>
        <member name="M:Ml2.AttrSel.Algs.GreedyStepwise.StartSet(System.String)">
            <summary>
            Set the start point for the search. This is specified as a comma
            seperated list off attribute indexes starting at 1. It can include ranges. Eg.
            1,2,5-9,17.
            </summary>    
        </member>
        <member name="M:Ml2.AttrSel.Algs.GreedyStepwise.Threshold(System.Double)">
            <summary>
            Set threshold by which attributes can be discarded. Default value results
            in no attributes being discarded. Use in conjunction with generateRanking
            </summary>    
        </member>
        <member name="M:Ml2.AttrSel.Algs.GreedyStepwise.NumToSelect(System.Int32)">
            <summary>
            Specify the number of attributes to retain. The default value (-1)
            indicates that all attributes are to be retained. Use either this option or a
            threshold to reduce the attribute set.
            </summary>    
        </member>
        <member name="T:Ml2.AttrSel.Algs.Ranker">
            <summary>
            Ranker : <br/><br/>Ranks attributes by their individual evaluations. Use
            in conjunction with attribute evaluators (ReliefF, GainRatio, Entropy
            etc).<br/><br/><br/>Options:<br/><br/>-P &lt;start set&gt; = 	Specify a starting
            set of attributes.<br/>	Eg. 1,3,5-7.<br/>	Any starting attributes
            specified are<br/>	ignored during the ranking.<br/>-T &lt;threshold&gt; = 	Specify
            a theshold by which attributes<br/>	may be discarded from the
            ranking.<br/>-N &lt;num to select&gt; = 	Specify number of attributes to select
            </summary>
        </member>
        <member name="M:Ml2.AttrSel.Algs.Ranker.StartSet(System.String)">
            <summary>
            Specify a set of attributes to ignore. When generating the ranking,
            Ranker will not evaluate the attributes in this list. This is specified as a
            comma seperated list off attribute indexes starting at 1. It can include
            ranges. Eg. 1,2,5-9,17.
            </summary>    
        </member>
        <member name="M:Ml2.AttrSel.Algs.Ranker.Threshold(System.Double)">
            <summary>
            Set threshold by which attributes can be discarded. Default value results
            in no attributes being discarded. Use either this option or numToSelect to
            reduce the attribute set.
            </summary>    
        </member>
        <member name="M:Ml2.AttrSel.Algs.Ranker.NumToSelect(System.Int32)">
            <summary>
            Specify the number of attributes to retain. The default value (-1)
            indicates that all attributes are to be retained. Use either this option or a
            threshold to reduce the attribute set.
            </summary>    
        </member>
        <member name="M:Ml2.AttrSel.Algs.Ranker.GenerateRanking(System.Boolean)">
            <summary>
            A constant option. Ranker is only capable of generating attribute
            rankings.
            </summary>    
        </member>
        <member name="T:Ml2.AttrSel.Evals.CfsSubset">
            <summary>
            CfsSubsetEval :<br/><br/>Evaluates the worth of a subset of attributes by
            considering the individual predictive ability of each feature along with
            the degree of redundancy between them.<br/><br/>Subsets of features that are
            highly correlated with the class while having low intercorrelation are
            preferred.<br/><br/>For more information see:<br/><br/>M. A. Hall (1998).
            Correlation-based Feature Subset Selection for Machine Learning. Hamilton, New
            Zealand.<br/><br/>Options:<br/><br/>-M = 	Treat missing values as a
            separate value.<br/>-L = 	Don't include locally predictive attributes.
            </summary>
        </member>
        <member name="M:Ml2.AttrSel.Evals.CfsSubset.MissingSeparate(System.Boolean)">
            <summary>
            Treat missing as a separate value. Otherwise, counts for missing values
            are distributed across other values in proportion to their frequency.
            </summary>    
        </member>
        <member name="M:Ml2.AttrSel.Evals.CfsSubset.LocallyPredictive(System.Boolean)">
            <summary>
            Identify locally predictive attributes. Iteratively adds attributes with
            the highest correlation with the class as long as there is not already an
            attribute in the subset that has a higher correlation with the attribute in
            question
            </summary>    
        </member>
        <member name="P:Ml2.AttrSel.Evals.Evaluators.CfsSubset">
            <summary>
            CfsSubsetEval :<br/><br/>Evaluates the worth of a subset of attributes by
            considering the individual predictive ability of each feature along with
            the degree of redundancy between them.<br/><br/>Subsets of features that are
            highly correlated with the class while having low intercorrelation are
            preferred.<br/><br/>For more information see:<br/><br/>M. A. Hall (1998).
            Correlation-based Feature Subset Selection for Machine Learning. Hamilton, New
            Zealand.<br/><br/>Options:<br/><br/>-M = 	Treat missing values as a
            separate value.<br/>-L = 	Don't include locally predictive attributes.
            </summary>
        </member>
        <member name="P:Ml2.AttrSel.Evals.Evaluators.CorrelationAttribute">
            <summary>
            CorrelationAttributeEval :<br/><br/>Evaluates the worth of an attribute
            by measuring the correlation (Pearson's) between it and the
            class.<br/><br/>Nominal attributes are considered on a value by value basis by treating
            each value as an indicator. An overall correlation for a nominal attribute is
            arrived at via a weighted average.<br/><br/><br/>Options:<br/><br/>-D =
            	Output detailed info for nominal attributes
            </summary>
        </member>
        <member name="P:Ml2.AttrSel.Evals.Evaluators.GainRatioAttribute">
            <summary>
            GainRatioAttributeEval :<br/><br/>Evaluates the worth of an attribute by
            measuring the gain ratio with respect to the class.<br/><br/>GainR(Class,
            Attribute) = (H(Class) - H(Class | Attribute)) /
            H(Attribute).<br/><br/><br/>Options:<br/><br/>-M = 	treat missing values as a seperate value.
            </summary>
        </member>
        <member name="P:Ml2.AttrSel.Evals.Evaluators.InfoGainAttribute">
            <summary>
            InfoGainAttributeEval :<br/><br/>Evaluates the worth of an attribute by
            measuring the information gain with respect to the
            class.<br/><br/>InfoGain(Class,Attribute) = H(Class) - H(Class |
            Attribute).<br/><br/><br/>Options:<br/><br/>-M = 	treat missing values as a seperate value.<br/>-B = 	just
            binarize numeric attributes instead <br/>	of properly discretizing them.
            </summary>
        </member>
        <member name="P:Ml2.AttrSel.Evals.Evaluators.OneRAttribute">
            <summary>
            OneRAttributeEval :<br/><br/>Evaluates the worth of an attribute by using
            the OneR classifier.<br/><br/><br/>Options:<br/><br/>-S &lt;seed&gt; =
            	Random number seed for cross validation<br/>	(default = 1)<br/>-F
            &lt;folds&gt; = 	Number of folds for cross validation<br/>	(default = 10)<br/>-D =
            	Use training data for evaluation rather than cross validaton<br/>-B
            &lt;minimum bucket size&gt; = 	Minimum number of objects in a bucket<br/>	(passed on
            to OneR, default = 6)
            </summary>
        </member>
        <member name="P:Ml2.AttrSel.Evals.Evaluators.PrincipalComponents">
            <summary>
            Performs a principal components analysis and transformation of the data.
            Use in conjunction with a Ranker search. Dimensionality reduction is
            accomplished by choosing enough eigenvectors to account for some percentage of
            the variance in the original data---default 0.95 (95%). Attribute noise can
            be filtered by transforming to the PC space, eliminating some of the worst
            eigenvectors, and then transforming back to the original
            space.<br/><br/>Options:<br/><br/>-C = 	Center (rather than standardize) the<br/>	data and
            compute PCA using the covariance (rather<br/>	 than the correlation)
            matrix.<br/>-R = 	Retain enough PC attributes to account <br/>	for this proportion
            of variance in the original data.<br/>	(default = 0.95)<br/>-O = 	Transform
            through the PC space and <br/>	back to the original space.<br/>-A =
            	Maximum number of attributes to include in <br/>	transformed attribute names. (-1
            = include all)
            </summary>
        </member>
        <member name="P:Ml2.AttrSel.Evals.Evaluators.ReliefFAttribute">
            <summary>
            ReliefFAttributeEval :<br/><br/>Evaluates the worth of an attribute by
            repeatedly sampling an instance and considering the value of the given
            attribute for the nearest instance of the same and different class. Can operate
            on both discrete and continuous class data.<br/><br/>For more information
            see:<br/><br/>Kenji Kira, Larry A. Rendell: A Practical Approach to Feature
            Selection. In: Ninth International Workshop on Machine Learning, 249-256,
            1992.<br/><br/>Igor Kononenko: Estimating Attributes: Analysis and Extensions
            of RELIEF. In: European Conference on Machine Learning, 171-182,
            1994.<br/><br/>Marko Robnik-Sikonja, Igor Kononenko: An adaptation of Relief for
            attribute estimation in regression. In: Fourteenth International Conference on
            Machine Learning, 296-304, 1997.<br/><br/>Options:<br/><br/>-M &lt;num
            instances&gt; = 	Specify the number of instances to<br/>	sample when
            estimating attributes.<br/>	If not specified, then all instances<br/>	will be
            used.<br/>-D &lt;seed&gt; = 	Seed for randomly sampling instances.<br/>	(Default
            = 1)<br/>-K &lt;number of neighbours&gt; = 	Number of nearest neighbours
            (k) used<br/>	to estimate attribute relevances<br/>	(Default = 10).<br/>-W =
            	Weight nearest neighbours by distance<br/>-A &lt;num&gt; = 	Specify sigma
            value (used in an exp<br/>	function to control how quickly<br/>	weights for
            more distant instances<br/>	decrease. Use in conjunction with
            -W.<br/>	Sensible value=1/5 to 1/10 of the<br/>	number of nearest
            neighbours.<br/>	(Default = 2)
            </summary>
        </member>
        <member name="P:Ml2.AttrSel.Evals.Evaluators.SymmetricalUncertAttribute">
            <summary>
            SymmetricalUncertAttributeEval :<br/><br/>Evaluates the worth of an
            attribute by measuring the symmetrical uncertainty with respect to the class.
            <br/><br/> SymmU(Class, Attribute) = 2 * (H(Class) - H(Class | Attribute)) /
            H(Class) + H(Attribute).<br/><br/><br/>Options:<br/><br/>-M = 	treat
            missing values as a seperate value.
            </summary>
        </member>
        <member name="P:Ml2.AttrSel.Evals.Evaluators.WrapperSubset">
            <summary>
            WrapperSubsetEval:<br/><br/>Evaluates attribute sets by using a learning
            scheme. Cross validation is used to estimate the accuracy of the learning
            scheme for a set of attributes.<br/><br/>For more information
            see:<br/><br/>Ron Kohavi, George H. John (1997). Wrappers for feature subset selection.
            Artificial Intelligence. 97(1-2):273-324.<br/><br/>Options:<br/><br/>-B
            &lt;base learner&gt; = 	class name of base learner to use for 	accuracy
            estimation.<br/>	Place any classifier options LAST on the command
            line<br/>	following a "--". eg.:<br/>		-B weka.classifiers.bayes.NaiveBayes ... --
            -K<br/>	(default: weka.classifiers.rules.ZeroR)<br/>-F &lt;num&gt; = 	number of
            cross validation folds to use for estimating accuracy.<br/>	(default=5)<br/>-R
            &lt;seed&gt; = 	Seed for cross validation accuracy
            testimation.<br/>	(default = 1)<br/>-T &lt;num&gt; = 	threshold by which to execute another cross
            validation<br/>	(standard deviation---expressed as a percentage of the
            mean).<br/>	(default: 0.01 (1%))<br/>-E &lt;acc | rmse | mae | f-meas | auc |
            auprc&gt; = 	Performance evaluation measure to use for selecting
            attributes.<br/>	(Default = accuracy for discrete class and rmse for numeric
            class)<br/><br/>Options specific to scheme weka.classifiers.rules.ZeroR: = <br/>-D =
            	If set, classifier is run in debug mode and<br/>	may output additional
            info to the console
            </summary>
        </member>
        <member name="P:Ml2.AttrSel.Evals.Evaluators.LatentSemanticAnalysis">
            <summary>
            Performs latent semantic analysis and transformation of the data. Use in
            conjunction with a Ranker search. A low-rank approximation of the full data
            is found by either specifying the number of singular values to use or
            specifying a proportion of the singular values to
            cover.<br/><br/>Options:<br/><br/>-N = 	Normalize input data.<br/>-R = 	Rank approximation used in LSA.
            <br/>	May be actual number of LSA attributes <br/>	to include (if greater
            than 1) or a <br/>	proportion of total singular values to <br/>	account for
            (if between 0 and 1). <br/>	A value less than or equal to zero means
            <br/>	use all latent variables.(default = 0.95)<br/>-A = 	Maximum number of
            attributes to include<br/>	in transformed attribute names.<br/>	(-1 = include
            all)
            </summary>
        </member>
        <member name="T:Ml2.AttrSel.Evals.InfoGainAttribute">
            <summary>
            InfoGainAttributeEval :<br/><br/>Evaluates the worth of an attribute by
            measuring the information gain with respect to the
            class.<br/><br/>InfoGain(Class,Attribute) = H(Class) - H(Class |
            Attribute).<br/><br/><br/>Options:<br/><br/>-M = 	treat missing values as a seperate value.<br/>-B = 	just
            binarize numeric attributes instead <br/>	of properly discretizing them.
            </summary>
        </member>
        <member name="M:Ml2.AttrSel.Evals.InfoGainAttribute.MissingMerge(System.Boolean)">
            <summary>
            Distribute counts for missing values. Counts are distributed across other
            values in proportion to their frequency. Otherwise, missing is treated as
            a separate value.
            </summary>    
        </member>
        <member name="M:Ml2.AttrSel.Evals.InfoGainAttribute.BinarizeNumericAttributes(System.Boolean)">
            <summary>
            Just binarize numeric attributes instead of properly discretizing them.
            </summary>    
        </member>
        <member name="T:Ml2.AttrSel.Evals.GainRatioAttribute">
            <summary>
            GainRatioAttributeEval :<br/><br/>Evaluates the worth of an attribute by
            measuring the gain ratio with respect to the class.<br/><br/>GainR(Class,
            Attribute) = (H(Class) - H(Class | Attribute)) /
            H(Attribute).<br/><br/><br/>Options:<br/><br/>-M = 	treat missing values as a seperate value.
            </summary>
        </member>
        <member name="M:Ml2.AttrSel.Evals.GainRatioAttribute.MissingMerge(System.Boolean)">
            <summary>
            Distribute counts for missing values. Counts are distributed across other
            values in proportion to their frequency. Otherwise, missing is treated as
            a separate value.
            </summary>    
        </member>
        <member name="T:Ml2.AttrSel.Evals.CorrelationAttribute">
            <summary>
            CorrelationAttributeEval :<br/><br/>Evaluates the worth of an attribute
            by measuring the correlation (Pearson's) between it and the
            class.<br/><br/>Nominal attributes are considered on a value by value basis by treating
            each value as an indicator. An overall correlation for a nominal attribute is
            arrived at via a weighted average.<br/><br/><br/>Options:<br/><br/>-D =
            	Output detailed info for nominal attributes
            </summary>
        </member>
        <member name="M:Ml2.AttrSel.Evals.CorrelationAttribute.OutputDetailedInfo(System.Boolean)">
            <summary>
            Output per value correlation for nominal attributes
            </summary>    
        </member>
        <member name="T:Ml2.AttrSel.Evals.OneRAttribute">
            <summary>
            OneRAttributeEval :<br/><br/>Evaluates the worth of an attribute by using
            the OneR classifier.<br/><br/><br/>Options:<br/><br/>-S &lt;seed&gt; =
            	Random number seed for cross validation<br/>	(default = 1)<br/>-F
            &lt;folds&gt; = 	Number of folds for cross validation<br/>	(default = 10)<br/>-D =
            	Use training data for evaluation rather than cross validaton<br/>-B
            &lt;minimum bucket size&gt; = 	Minimum number of objects in a bucket<br/>	(passed on
            to OneR, default = 6)
            </summary>
        </member>
        <member name="M:Ml2.AttrSel.Evals.OneRAttribute.Folds(System.Int32)">
            <summary>
            Set the number of folds for cross validation.
            </summary>    
        </member>
        <member name="M:Ml2.AttrSel.Evals.OneRAttribute.MinimumBucketSize(System.Int32)">
            <summary>
            The minimum number of objects in a bucket (passed to OneR).
            </summary>    
        </member>
        <member name="M:Ml2.AttrSel.Evals.OneRAttribute.EvalUsingTrainingData(System.Boolean)">
            <summary>
            Use the training data to evaluate attributes rather than cross
            validation.
            </summary>    
        </member>
        <member name="T:Ml2.AttrSel.Evals.PrincipalComponents">
            <summary>
            Performs a principal components analysis and transformation of the data.
            Use in conjunction with a Ranker search. Dimensionality reduction is
            accomplished by choosing enough eigenvectors to account for some percentage of
            the variance in the original data---default 0.95 (95%). Attribute noise can
            be filtered by transforming to the PC space, eliminating some of the worst
            eigenvectors, and then transforming back to the original
            space.<br/><br/>Options:<br/><br/>-C = 	Center (rather than standardize) the<br/>	data and
            compute PCA using the covariance (rather<br/>	 than the correlation)
            matrix.<br/>-R = 	Retain enough PC attributes to account <br/>	for this proportion
            of variance in the original data.<br/>	(default = 0.95)<br/>-O = 	Transform
            through the PC space and <br/>	back to the original space.<br/>-A =
            	Maximum number of attributes to include in <br/>	transformed attribute names. (-1
            = include all)
            </summary>
        </member>
        <member name="M:Ml2.AttrSel.Evals.PrincipalComponents.VarianceCovered(System.Double)">
            <summary>
            Retain enough PC attributes to account for this proportion of variance.
            </summary>    
        </member>
        <member name="M:Ml2.AttrSel.Evals.PrincipalComponents.MaximumAttributeNames(System.Int32)">
            <summary>
            The maximum number of attributes to include in transformed attribute
            names.
            </summary>    
        </member>
        <!-- Badly formed XML comment ignored for member "M:Ml2.AttrSel.Evals.PrincipalComponents.TransformBackToOriginal(System.Boolean)" -->
        <member name="M:Ml2.AttrSel.Evals.PrincipalComponents.CenterData(System.Boolean)">
            <summary>
            Center (rather than standardize) the data. PCA will be computed from the
            covariance (rather than correlation) matrix
            </summary>    
        </member>
        <member name="T:Ml2.AttrSel.Evals.ReliefFAttribute">
            <summary>
            ReliefFAttributeEval :<br/><br/>Evaluates the worth of an attribute by
            repeatedly sampling an instance and considering the value of the given
            attribute for the nearest instance of the same and different class. Can operate
            on both discrete and continuous class data.<br/><br/>For more information
            see:<br/><br/>Kenji Kira, Larry A. Rendell: A Practical Approach to Feature
            Selection. In: Ninth International Workshop on Machine Learning, 249-256,
            1992.<br/><br/>Igor Kononenko: Estimating Attributes: Analysis and Extensions
            of RELIEF. In: European Conference on Machine Learning, 171-182,
            1994.<br/><br/>Marko Robnik-Sikonja, Igor Kononenko: An adaptation of Relief for
            attribute estimation in regression. In: Fourteenth International Conference on
            Machine Learning, 296-304, 1997.<br/><br/>Options:<br/><br/>-M &lt;num
            instances&gt; = 	Specify the number of instances to<br/>	sample when
            estimating attributes.<br/>	If not specified, then all instances<br/>	will be
            used.<br/>-D &lt;seed&gt; = 	Seed for randomly sampling instances.<br/>	(Default
            = 1)<br/>-K &lt;number of neighbours&gt; = 	Number of nearest neighbours
            (k) used<br/>	to estimate attribute relevances<br/>	(Default = 10).<br/>-W =
            	Weight nearest neighbours by distance<br/>-A &lt;num&gt; = 	Specify sigma
            value (used in an exp<br/>	function to control how quickly<br/>	weights for
            more distant instances<br/>	decrease. Use in conjunction with
            -W.<br/>	Sensible value=1/5 to 1/10 of the<br/>	number of nearest
            neighbours.<br/>	(Default = 2)
            </summary>
        </member>
        <member name="M:Ml2.AttrSel.Evals.ReliefFAttribute.WeightByDistance(System.Boolean)">
            <summary>
            Weight nearest neighbours by their distance.
            </summary>    
        </member>
        <member name="M:Ml2.AttrSel.Evals.ReliefFAttribute.SampleSize(System.Int32)">
            <summary>
            Number of instances to sample. Default (-1) indicates that all instances
            will be used for attribute estimation.
            </summary>    
        </member>
        <member name="M:Ml2.AttrSel.Evals.ReliefFAttribute.NumNeighbours(System.Int32)">
            <summary>
            Number of nearest neighbours for attribute estimation.
            </summary>    
        </member>
        <member name="M:Ml2.AttrSel.Evals.ReliefFAttribute.Sigma(System.Int32)">
            <summary>
            Set influence of nearest neighbours. Used in an exp function to control
            how quickly weights decrease for more distant instances. Use in conjunction
            with weightByDistance. Sensible values = 1/5 to 1/10 the number of nearest
            neighbours.
            </summary>    
        </member>
        <member name="T:Ml2.AttrSel.Evals.SymmetricalUncertAttribute">
            <summary>
            SymmetricalUncertAttributeEval :<br/><br/>Evaluates the worth of an
            attribute by measuring the symmetrical uncertainty with respect to the class.
            <br/><br/> SymmU(Class, Attribute) = 2 * (H(Class) - H(Class | Attribute)) /
            H(Class) + H(Attribute).<br/><br/><br/>Options:<br/><br/>-M = 	treat
            missing values as a seperate value.
            </summary>
        </member>
        <member name="M:Ml2.AttrSel.Evals.SymmetricalUncertAttribute.MissingMerge(System.Boolean)">
            <summary>
            Distribute counts for missing values. Counts are distributed across other
            values in proportion to their frequency. Otherwise, missing is treated as
            a separate value.
            </summary>    
        </member>
        <member name="T:Ml2.AttrSel.Evals.WrapperSubset">
            <summary>
            WrapperSubsetEval:<br/><br/>Evaluates attribute sets by using a learning
            scheme. Cross validation is used to estimate the accuracy of the learning
            scheme for a set of attributes.<br/><br/>For more information
            see:<br/><br/>Ron Kohavi, George H. John (1997). Wrappers for feature subset selection.
            Artificial Intelligence. 97(1-2):273-324.<br/><br/>Options:<br/><br/>-B
            &lt;base learner&gt; = 	class name of base learner to use for 	accuracy
            estimation.<br/>	Place any classifier options LAST on the command
            line<br/>	following a "--". eg.:<br/>		-B weka.classifiers.bayes.NaiveBayes ... --
            -K<br/>	(default: weka.classifiers.rules.ZeroR)<br/>-F &lt;num&gt; = 	number of
            cross validation folds to use for estimating accuracy.<br/>	(default=5)<br/>-R
            &lt;seed&gt; = 	Seed for cross validation accuracy
            testimation.<br/>	(default = 1)<br/>-T &lt;num&gt; = 	threshold by which to execute another cross
            validation<br/>	(standard deviation---expressed as a percentage of the
            mean).<br/>	(default: 0.01 (1%))<br/>-E &lt;acc | rmse | mae | f-meas | auc |
            auprc&gt; = 	Performance evaluation measure to use for selecting
            attributes.<br/>	(Default = accuracy for discrete class and rmse for numeric
            class)<br/><br/>Options specific to scheme weka.classifiers.rules.ZeroR: = <br/>-D =
            	If set, classifier is run in debug mode and<br/>	may output additional
            info to the console
            </summary>
        </member>
        <member name="M:Ml2.AttrSel.Evals.WrapperSubset.Classifier(Ml2.Clss.IBaseClassifier{weka.classifiers.Classifier})">
            <summary>
            Classifier to use for estimating the accuracy of subsets
            </summary>    
        </member>
        <member name="M:Ml2.AttrSel.Evals.WrapperSubset.Folds(System.Int32)">
            <summary>
            Number of xval folds to use when estimating subset accuracy.
            </summary>    
        </member>
        <member name="M:Ml2.AttrSel.Evals.WrapperSubset.Threshold(System.Double)">
            <summary>
            Repeat xval if stdev of mean exceeds this value.
            </summary>    
        </member>
        <member name="M:Ml2.AttrSel.Evals.WrapperSubset.EvaluationMeasure(Ml2.AttrSel.Evals.WrapperSubset.EEvaluationMeasure)">
            <summary>
            The measure used to evaluate the performance of attribute combinations.
            </summary>    
        </member>
        <member name="T:Ml2.Clss.AdaBoostM1">
            <summary>
            Class for boosting a nominal class classifier using the Adaboost M1
            method. Only nominal class problems can be tackled. Often dramatically improves
            performance, but sometimes overfits.<br/><br/>For more information,
            see<br/><br/>Yoav Freund, Robert E. Schapire: Experiments with a new boosting
            algorithm. In: Thirteenth International Conference on Machine Learning, San
            Francisco, 148-156, 1996.<br/><br/>Options:<br/><br/>-P &lt;num&gt; =
            	Percentage of weight mass to base training on.<br/>	(default 100, reduce to around
            90 speed up)<br/>-Q = 	Use resampling for boosting.<br/>-S &lt;num&gt; =
            	Random number seed.<br/>	(default 1)<br/>-I &lt;num&gt; = 	Number of
            iterations.<br/>	(default 10)<br/>-D = 	If set, classifier is run in debug mode
            and<br/>	may output additional info to the console<br/>-W = 	Full name of
            base classifier.<br/>	(default:
            weka.classifiers.trees.DecisionStump)<br/><br/>Options specific to classifier weka.classifiers.trees.DecisionStump: =
            <br/>-D = 	If set, classifier is run in debug mode and<br/>	may output
            additional info to the console
            </summary>
        </member>
        <member name="M:Ml2.Clss.AdaBoostM1.WeightThreshold(System.Int32)">
            <summary>
            Weight threshold for weight pruning.
            </summary>    
        </member>
        <member name="M:Ml2.Clss.AdaBoostM1.UseResampling(System.Boolean)">
            <summary>
            Whether resampling is used instead of reweighting.
            </summary>    
        </member>
        <member name="M:Ml2.Clss.AdaBoostM1.NumIterations(System.Int32)">
            <summary>
            The number of iterations to be performed.
            </summary>    
        </member>
        <member name="M:Ml2.Clss.AdaBoostM1.Classifier(Ml2.Clss.IBaseClassifier{weka.classifiers.Classifier})">
            <summary>
            The base classifier to be used.
            </summary>    
        </member>
        <member name="M:Ml2.Clss.AdaBoostM1.Debug(System.Boolean)">
            <summary>
            If set to true, classifier may output additional info to the console.
            </summary>    
        </member>
        <member name="T:Ml2.Clss.AdditiveRegression">
            <summary>
            Meta classifier that enhances the performance of a regression base
            classifier. Each iteration fits a model to the residuals left by the classifier
            on the previous iteration. Prediction is accomplished by adding the
            predictions of each classifier. Reducing the shrinkage (learning rate) parameter
            helps prevent overfitting and has a smoothing effect but increases the
            learning time.<br/><br/>For more information see:<br/><br/>J.H. Friedman (1999).
            Stochastic Gradient Boosting.<br/><br/>Options:<br/><br/>-S = 	Specify
            shrinkage rate. (default = 1.0, ie. no shrinkage)<br/><br/>-I &lt;num&gt; =
            	Number of iterations.<br/>	(default 10)<br/>-D = 	If set, classifier is run
            in debug mode and<br/>	may output additional info to the console<br/>-W =
            	Full name of base classifier.<br/>	(default:
            weka.classifiers.trees.DecisionStump)<br/><br/>Options specific to classifier
            weka.classifiers.trees.DecisionStump: = <br/>-D = 	If set, classifier is run in debug mode and<br/>	may
            output additional info to the console
            </summary>
        </member>
        <member name="M:Ml2.Clss.AdditiveRegression.Shrinkage(System.Double)">
            <summary>
            Shrinkage rate. Smaller values help prevent overfitting and have a
            smoothing effect (but increase learning time). Default = 1.0, ie. no shrinkage.
            </summary>    
        </member>
        <member name="M:Ml2.Clss.AdditiveRegression.NumIterations(System.Int32)">
            <summary>
            The number of iterations to be performed.
            </summary>    
        </member>
        <member name="M:Ml2.Clss.AdditiveRegression.Classifier(Ml2.Clss.IBaseClassifier{weka.classifiers.Classifier})">
            <summary>
            The base classifier to be used.
            </summary>    
        </member>
        <member name="M:Ml2.Clss.AdditiveRegression.Debug(System.Boolean)">
            <summary>
            If set to true, classifier may output additional info to the console.
            </summary>    
        </member>
        <member name="T:Ml2.Clss.AttributeSelectedClassifier">
            <summary>
            Dimensionality of training and test data is reduced by attribute
            selection before being passed on to a classifier.<br/><br/>Options:<br/><br/>-E
            &lt;attribute evaluator specification&gt; = 	Full class name of attribute
            evaluator, followed<br/>	by its options.<br/>	eg:
            "weka.attributeSelection.CfsSubsetEval -L"<br/>	(default weka.attributeSelection.CfsSubsetEval)<br/>-S
            &lt;search method specification&gt; = 	Full class name of search method,
            followed<br/>	by its options.<br/>	eg: "weka.attributeSelection.BestFirst -D
            1"<br/>	(default weka.attributeSelection.BestFirst)<br/>-D = 	If set,
            classifier is run in debug mode and<br/>	may output additional info to the
            console<br/>-W = 	Full name of base classifier.<br/>	(default:
            weka.classifiers.trees.J48)<br/><br/>Options specific to classifier
            weka.classifiers.trees.J48: = <br/>-U = 	Use unpruned tree.<br/>-O = 	Do not collapse tree.<br/>-C
            &lt;pruning confidence&gt; = 	Set confidence threshold for
            pruning.<br/>	(default 0.25)<br/>-M &lt;minimum number of instances&gt; = 	Set minimum
            number of instances per leaf.<br/>	(default 2)<br/>-R = 	Use reduced error
            pruning.<br/>-N &lt;number of folds&gt; = 	Set number of folds for reduced
            error<br/>	pruning. One fold is used as pruning set.<br/>	(default 3)<br/>-B =
            	Use binary splits only.<br/>-S = 	Don't perform subtree raising.<br/>-L =
            	Do not clean up after the tree has been built.<br/>-A = 	Laplace smoothing
            for predicted probabilities.<br/>-J = 	Do not use MDL correction for info
            gain on numeric attributes.<br/>-Q &lt;seed&gt; = 	Seed for random data
            shuffling (default 1).
            </summary>
        </member>
        <member name="M:Ml2.Clss.AttributeSelectedClassifier.Evaluator(Ml2.AttrSel.Evals.BaseAttributeSelectionEvaluator{weka.attributeSelection.ASEvaluation})">
            <summary>
            Set the attribute evaluator to use. This evaluator is used during the
            attribute selection phase before the classifier is invoked.
            </summary>    
        </member>
        <member name="M:Ml2.Clss.AttributeSelectedClassifier.Search(Ml2.AttrSel.Algs.BaseAttributeSelectionAlgorithm{weka.attributeSelection.ASSearch})">
            <summary>
            Set the search method. This search method is used during the attribute
            selection phase before the classifier is invoked.
            </summary>    
        </member>
        <member name="M:Ml2.Clss.AttributeSelectedClassifier.Classifier(Ml2.Clss.IBaseClassifier{weka.classifiers.Classifier})">
            <summary>
            The base classifier to be used.
            </summary>    
        </member>
        <member name="M:Ml2.Clss.AttributeSelectedClassifier.Debug(System.Boolean)">
            <summary>
            If set to true, classifier may output additional info to the console.
            </summary>    
        </member>
        <member name="T:Ml2.Clss.Bagging">
            <summary>
            Class for bagging a classifier to reduce variance. Can do classification
            and regression depending on the base learner. <br/><br/>For more
            information, see<br/><br/>Leo Breiman (1996). Bagging predictors. Machine Learning.
            24(2):123-140.<br/><br/>Options:<br/><br/>-P = 	Size of each bag, as a
            percentage of the<br/>	training set size. (default 100)<br/>-O = 	Calculate the
            out of bag error.<br/>-S &lt;num&gt; = 	Random number seed.<br/>	(default
            1)<br/>-num-slots &lt;num&gt; = 	Number of execution slots.<br/>	(default 1
            - i.e. no parallelism)<br/>-I &lt;num&gt; = 	Number of
            iterations.<br/>	(default 10)<br/>-D = 	If set, classifier is run in debug mode and<br/>	may
            output additional info to the console<br/>-W = 	Full name of base
            classifier.<br/>	(default: weka.classifiers.trees.REPTree)<br/><br/>Options specific
            to classifier weka.classifiers.trees.REPTree: = <br/>-M &lt;minimum number
            of instances&gt; = 	Set minimum number of instances per leaf (default
            2).<br/>-V &lt;minimum variance for split&gt; = 	Set minimum numeric class
            variance proportion<br/>	of train variance for split (default 1e-3).<br/>-N
            &lt;number of folds&gt; = 	Number of folds for reduced error pruning (default
            3).<br/>-S &lt;seed&gt; = 	Seed for random data shuffling (default 1).<br/>-P
            = 	No pruning.<br/>-L = 	Maximum tree depth (default -1, no
            maximum)<br/>-I = 	Initial class value count (default 0)<br/>-R = 	Spread initial count
            over all class values (i.e. don't use 1 per value)
            </summary>
        </member>
        <member name="M:Ml2.Clss.Bagging.BagSizePercent(System.Int32)">
            <summary>
            Size of each bag, as a percentage of the training set size.
            </summary>    
        </member>
        <member name="M:Ml2.Clss.Bagging.CalcOutOfBag(System.Boolean)">
            <summary>
            Whether the out-of-bag error is calculated.
            </summary>    
        </member>
        <member name="M:Ml2.Clss.Bagging.NumExecutionSlots(System.Int32)">
            <summary>
            The number of execution slots (threads) to use for constructing the
            ensemble.
            </summary>    
        </member>
        <member name="M:Ml2.Clss.Bagging.NumIterations(System.Int32)">
            <summary>
            The number of iterations to be performed.
            </summary>    
        </member>
        <member name="M:Ml2.Clss.Bagging.Classifier(Ml2.Clss.IBaseClassifier{weka.classifiers.Classifier})">
            <summary>
            The base classifier to be used.
            </summary>    
        </member>
        <member name="M:Ml2.Clss.Bagging.Debug(System.Boolean)">
            <summary>
            If set to true, classifier may output additional info to the console.
            </summary>    
        </member>
        <member name="T:Ml2.Clss.BayesNet">
            <summary>
            Bayes Network learning using various search algorithms and quality
            measures.<br/>Base class for a Bayes Network classifier. Provides datastructures
            (network structure, conditional probability distributions, etc.) and
            facilities common to Bayes Network learning algorithms like K2 and
            B.<br/><br/>For more information
            see:<br/><br/>http://www.cs.waikato.ac.nz/~remco/weka.pdf<br/><br/>Options:<br/><br/>-D = 	Do not use ADTree data
            structure<br/><br/>-B &lt;BIF file&gt; = 	BIF file to compare with<br/><br/>-Q
            weka.classifiers.bayes.net.search.SearchAlgorithm = 	Search algorithm<br/><br/>-E
            weka.classifiers.bayes.net.estimate.SimpleEstimator = 	Estimator algorithm<br/>
            </summary>
        </member>
        <member name="M:Ml2.Clss.BayesNet.BIFFile(System.String)">
            <summary>
            
            </summary>    
        </member>
        <member name="M:Ml2.Clss.BayesNet.SearchAlgorithm(weka.classifiers.bayes.net.search.SearchAlgorithm)">
            <summary>
            Select method used for searching network structures.
            </summary>    
        </member>
        <member name="M:Ml2.Clss.BayesNet.Estimator(weka.classifiers.bayes.net.estimate.BayesNetEstimator)">
            <summary>
            Select Estimator algorithm for finding the conditional probability tables
            of the Bayes Network.
            </summary>    
        </member>
        <member name="M:Ml2.Clss.BayesNet.UseADTree(System.Boolean)">
            <summary>
            When ADTree (the data structure for increasing speed on counts, not to be
            confused with the classifier under the same name) is used learning time
            goes down typically. However, because ADTrees are memory intensive, memory
            problems may occur. Switching this option off makes the structure learning
            algorithms slower, and run with less memory. By default, ADTrees are used.
            </summary>    
        </member>
        <member name="M:Ml2.Clss.BayesNet.Debug(System.Boolean)">
            <summary>
            If set to true, classifier may output additional info to the console.
            </summary>    
        </member>
        <member name="T:Ml2.Clss.BayesNetGenerator">
            <summary>
            Bayes Network learning using various search algorithms and quality
            measures.<br/>Base class for a Bayes Network classifier. Provides datastructures
            (network structure, conditional probability distributions, etc.) and
            facilities common to Bayes Network learning algorithms like K2 and
            B.<br/><br/>For more information
            see:<br/><br/>http://www.cs.waikato.ac.nz/~remco/weka.pdf<br/><br/>Options:<br/><br/>-B = 	Generate network (instead of
            instances)<br/><br/>-N &lt;integer&gt; = 	Nr of nodes<br/><br/>-A &lt;integer&gt; =
            	Nr of arcs<br/><br/>-M &lt;integer&gt; = 	Nr of instances<br/><br/>-C
            &lt;integer&gt; = 	Cardinality of the variables<br/><br/>-S &lt;integer&gt; =
            	Seed for random number generator<br/><br/>-F &lt;file&gt; = 	The BIF file to
            obtain the structure from.<br/>
            </summary>
        </member>
        <member name="M:Ml2.Clss.BayesNetGenerator.Distribution(System.Int32,System.Double[][])">
            <summary>
            
            </summary>    
        </member>
        <member name="M:Ml2.Clss.BayesNetGenerator.Evidence(System.Int32,System.Int32)">
            <summary>
            
            </summary>    
        </member>
        <member name="M:Ml2.Clss.BayesNetGenerator.Data(Ml2.Runtime)">
            <summary>
            
            </summary>    
        </member>
        <member name="M:Ml2.Clss.BayesNetGenerator.Distribution(System.String,System.Double[][])">
            <summary>
            
            </summary>    
        </member>
        <member name="M:Ml2.Clss.BayesNetGenerator.NodeName(System.Int32,System.String)">
            <summary>
            
            </summary>    
        </member>
        <member name="M:Ml2.Clss.BayesNetGenerator.Position(System.Int32,System.Int32,System.Int32)">
            <summary>
            
            </summary>    
        </member>
        <member name="M:Ml2.Clss.BayesNetGenerator.Margin(System.Int32,System.Double[])">
            <summary>
            
            </summary>    
        </member>
        <member name="M:Ml2.Clss.BayesNetGenerator.BIFFile(System.String)">
            <summary>
            
            </summary>    
        </member>
        <member name="M:Ml2.Clss.BayesNetGenerator.SearchAlgorithm(weka.classifiers.bayes.net.search.SearchAlgorithm)">
            <summary>
            Select method used for searching network structures.
            </summary>    
        </member>
        <member name="M:Ml2.Clss.BayesNetGenerator.Estimator(weka.classifiers.bayes.net.estimate.BayesNetEstimator)">
            <summary>
            Select Estimator algorithm for finding the conditional probability tables
            of the Bayes Network.
            </summary>    
        </member>
        <member name="M:Ml2.Clss.BayesNetGenerator.UseADTree(System.Boolean)">
            <summary>
            When ADTree (the data structure for increasing speed on counts, not to be
            confused with the classifier under the same name) is used learning time
            goes down typically. However, because ADTrees are memory intensive, memory
            problems may occur. Switching this option off makes the structure learning
            algorithms slower, and run with less memory. By default, ADTrees are used.
            </summary>    
        </member>
        <member name="M:Ml2.Clss.BayesNetGenerator.Debug(System.Boolean)">
            <summary>
            If set to true, classifier may output additional info to the console.
            </summary>    
        </member>
        <member name="T:Ml2.Clss.BIFReader">
            <summary>
            Builds a description of a Bayes Net classifier stored in XML BIF 0.3
            format.<br/><br/>For more details on XML BIF see:<br/><br/>Fabio Cozman, Marek
            Druzdzel, Daniel Garcia (1998). XML BIF version 0.3. URL
            http://www-2.cs.cmu.edu/~fgcozman/Research/InterchangeFormat/.<br/><br/>Options:<br/><br/>-D
            = 	Do not use ADTree data structure<br/><br/>-B &lt;BIF file&gt; = 	BIF
            file to compare with<br/><br/>-Q
            weka.classifiers.bayes.net.search.SearchAlgorithm = 	Search algorithm<br/><br/>-E
            weka.classifiers.bayes.net.estimate.SimpleEstimator = 	Estimator algorithm<br/>
            </summary>
        </member>
        <member name="M:Ml2.Clss.BIFReader.BIFFile(System.String)">
            <summary>
            
            </summary>    
        </member>
        <member name="M:Ml2.Clss.BIFReader.SearchAlgorithm(weka.classifiers.bayes.net.search.SearchAlgorithm)">
            <summary>
            Select method used for searching network structures.
            </summary>    
        </member>
        <member name="M:Ml2.Clss.BIFReader.Estimator(weka.classifiers.bayes.net.estimate.BayesNetEstimator)">
            <summary>
            Select Estimator algorithm for finding the conditional probability tables
            of the Bayes Network.
            </summary>    
        </member>
        <member name="M:Ml2.Clss.BIFReader.UseADTree(System.Boolean)">
            <summary>
            When ADTree (the data structure for increasing speed on counts, not to be
            confused with the classifier under the same name) is used learning time
            goes down typically. However, because ADTrees are memory intensive, memory
            problems may occur. Switching this option off makes the structure learning
            algorithms slower, and run with less memory. By default, ADTrees are used.
            </summary>    
        </member>
        <member name="M:Ml2.Clss.BIFReader.Debug(System.Boolean)">
            <summary>
            If set to true, classifier may output additional info to the console.
            </summary>    
        </member>
        <member name="T:Ml2.Clss.ClassificationViaRegression">
            <summary>
            Class for doing classification using regression methods. Class is
            binarized and one regression model is built for each class value. For more
            information, see, for example<br/><br/>E. Frank, Y. Wang, S. Inglis, G. Holmes,
            I.H. Witten (1998). Using model trees for classification. Machine Learning.
            32(1):63-76.<br/><br/>Options:<br/><br/>-D = 	If set, classifier is run in
            debug mode and<br/>	may output additional info to the console<br/>-W = 	Full
            name of base classifier.<br/>	(default:
            weka.classifiers.trees.M5P)<br/><br/>Options specific to classifier weka.classifiers.trees.M5P: = <br/>-N =
            	Use unpruned tree/rules<br/>-U = 	Use unsmoothed predictions<br/>-R =
            	Build regression tree/rule rather than a model tree/rule<br/>-M &lt;minimum
            number of instances&gt; = 	Set minimum number of instances per
            leaf<br/>	(default 4)<br/>-L = 	Save instances at the nodes in<br/>	the tree (for
            visualization purposes)
            </summary>
        </member>
        <member name="M:Ml2.Clss.ClassificationViaRegression.Classifier(Ml2.Clss.IBaseClassifier{weka.classifiers.Classifier})">
            <summary>
            The base classifier to be used.
            </summary>    
        </member>
        <member name="M:Ml2.Clss.ClassificationViaRegression.Debug(System.Boolean)">
            <summary>
            If set to true, classifier may output additional info to the console.
            </summary>    
        </member>
        <member name="T:Ml2.Clss.CostSensitiveClassifier">
            <summary>
            A metaclassifier that makes its base classifier cost-sensitive. Two
            methods can be used to introduce cost-sensitivity: reweighting training
            instances according to the total cost assigned to each class; or predicting the
            class with minimum expected misclassification cost (rather than the most
            likely class). Performance can often be improved by using a Bagged classifier to
            improve the probability estimates of the base
            classifier.<br/><br/>Options:<br/><br/>-M = 	Minimize expected misclassification cost. Default is
            to<br/>	reweight training instances according to costs per class<br/>-C &lt;cost
            file name&gt; = 	File name of a cost matrix to use. If this is not
            supplied,<br/>	a cost matrix will be loaded on demand. The name of
            the<br/>	on-demand file is the relation name of the training data<br/>	plus ".cost", and
            the path to the on-demand file is<br/>	specified with the -N option.<br/>-N
            &lt;directory&gt; = 	Name of a directory to search for cost files when
            loading<br/>	costs on demand (default current directory).<br/>-cost-matrix
            &lt;matrix&gt; = 	The cost matrix in Matlab single line format.<br/>-S
            &lt;num&gt; = 	Random number seed.<br/>	(default 1)<br/>-D = 	If set, classifier is
            run in debug mode and<br/>	may output additional info to the
            console<br/>-W = 	Full name of base classifier.<br/>	(default:
            weka.classifiers.rules.ZeroR)<br/><br/>Options specific to classifier weka.classifiers.rules.ZeroR:
            = <br/>-D = 	If set, classifier is run in debug mode and<br/>	may output
            additional info to the console
            </summary>
        </member>
        <member name="M:Ml2.Clss.CostSensitiveClassifier.MinimizeExpectedCost(System.Boolean)">
            <summary>
            Sets whether the minimum expected cost criteria will be used. If this is
            false, the training data will be reweighted according to the costs assigned
            to each class. If true, the minimum expected cost criteria will be used.
            </summary>    
        </member>
        <member name="M:Ml2.Clss.CostSensitiveClassifier.CostMatrix(System.Double[0:,0:])">
            <summary>
            Sets the cost matrix explicitly. This matrix is used if the
            costMatrixSource property is set to "Supplied".
            </summary>    
        </member>
        <member name="M:Ml2.Clss.CostSensitiveClassifier.CostMatrixSource(Ml2.Clss.CostSensitiveClassifier.ECostMatrixSource)">
            <summary>
            Sets where to get the cost matrix. The two options areto use the supplied
            explicit cost matrix (the setting of the costMatrix property), or to load
            a cost matrix from a file when required (this file will be loaded from the
            directory set by the onDemandDirectory property and will be named
            relation_name.cost).
            </summary>    
        </member>
        <member name="M:Ml2.Clss.CostSensitiveClassifier.Classifier(Ml2.Clss.IBaseClassifier{weka.classifiers.Classifier})">
            <summary>
            The base classifier to be used.
            </summary>    
        </member>
        <member name="M:Ml2.Clss.CostSensitiveClassifier.Debug(System.Boolean)">
            <summary>
            If set to true, classifier may output additional info to the console.
            </summary>    
        </member>
        <member name="T:Ml2.Clss.CVParameterSelection">
            <summary>
            Class for performing parameter selection by cross-validation for any
            classifier.<br/><br/>For more information, see:<br/><br/>R. Kohavi (1995).
            Wrappers for Performance Enhancement and Oblivious Decision Graphs. Department
            of Computer Science, Stanford University.<br/><br/>Options:<br/><br/>-X
            &lt;number of folds&gt; = 	Number of folds used for cross validation (default
            10).<br/>-P &lt;classifier parameter&gt; = 	Classifier parameter
            options.<br/>	eg: "N 1 5 10" Sets an optimisation parameter for the<br/>	classifier
            with name -N, with lower bound 1, upper bound<br/>	5, and 10 optimisation
            steps. The upper bound may be the<br/>	character 'A' or 'I' to substitute the
            number of<br/>	attributes or instances in the training
            data,<br/>	respectively. This parameter may be supplied more than<br/>	once to optimise over
            several classifier options<br/>	simultaneously.<br/>-S &lt;num&gt; = 	Random
            number seed.<br/>	(default 1)<br/>-D = 	If set, classifier is run in debug
            mode and<br/>	may output additional info to the console<br/>-W = 	Full
            name of base classifier.<br/>	(default:
            weka.classifiers.rules.ZeroR)<br/><br/>Options specific to classifier weka.classifiers.rules.ZeroR: = <br/>-D =
            	If set, classifier is run in debug mode and<br/>	may output additional info
            to the console
            </summary>
        </member>
        <member name="M:Ml2.Clss.CVParameterSelection.NumFolds(System.Int32)">
            <summary>
            Get the number of folds used for cross-validation.
            </summary>    
        </member>
        <member name="M:Ml2.Clss.CVParameterSelection.CVParameters(System.Object[])">
            <summary>
            
            </summary>    
        </member>
        <member name="M:Ml2.Clss.CVParameterSelection.Classifier(Ml2.Clss.IBaseClassifier{weka.classifiers.Classifier})">
            <summary>
            The base classifier to be used.
            </summary>    
        </member>
        <member name="M:Ml2.Clss.CVParameterSelection.Debug(System.Boolean)">
            <summary>
            If set to true, classifier may output additional info to the console.
            </summary>    
        </member>
        <member name="T:Ml2.Clss.DecisionStump">
            <summary>
            Class for building and using a decision stump. Usually used in
            conjunction with a boosting algorithm. Does regression (based on mean-squared error)
            or classification (based on entropy). Missing is treated as a separate
            value.<br/><br/>Options:<br/><br/>-D = 	If set, classifier is run in debug mode
            and<br/>	may output additional info to the console
            </summary>
        </member>
        <member name="M:Ml2.Clss.DecisionStump.Debug(System.Boolean)">
            <summary>
            If set to true, classifier may output additional info to the console.
            </summary>    
        </member>
        <member name="T:Ml2.Clss.DecisionTable">
            <summary>
            Class for building and using a simple decision table majority
            classifier.<br/><br/>For more information see: <br/><br/>Ron Kohavi: The Power of
            Decision Tables. In: 8th European Conference on Machine Learning, 174-189,
            1995.<br/><br/>Options:<br/><br/>-S &lt;search method specification&gt; = 	Full
            class name of search method, followed<br/>	by its options.<br/>	eg:
            "weka.attributeSelection.BestFirst -D 1"<br/>	(default
            weka.attributeSelection.BestFirst)<br/>-X &lt;number of folds&gt; = 	Use cross validation to evaluate
            features.<br/>	Use number of folds = 1 for leave one out CV.<br/>	(Default
            = leave one out CV)<br/>-E &lt;acc | rmse | mae | auc&gt; = 	Performance
            evaluation measure to use for selecting attributes.<br/>	(Default = accuracy
            for discrete class and rmse for numeric class)<br/>-I = 	Use nearest
            neighbour instead of global table majority.<br/>-R = 	Display decision table
            rules.<br/><br/><br/>Options specific to search method
            weka.attributeSelection.BestFirst: = <br/>-P &lt;start set&gt; = 	Specify a starting set of
            attributes.<br/>	Eg. 1,3,5-7.<br/>-D &lt;0 = backward | 1 = forward | 2 =
            bi-directional&gt; = 	Direction of search. (default = 1).<br/>-N &lt;num&gt; =
            	Number of non-improving nodes to<br/>	consider before terminating
            search.<br/>-S &lt;num&gt; = 	Size of lookup cache for evaluated
            subsets.<br/>	Expressed as a multiple of the number of<br/>	attributes in the data set. (default
            = 1)
            </summary>
        </member>
        <member name="M:Ml2.Clss.DecisionTable.EvaluationMeasure(Ml2.Clss.DecisionTable.EEvaluationMeasure)">
            <summary>
            The measure used to evaluate the performance of attribute combinations
            used in the decision table.
            </summary>    
        </member>
        <member name="M:Ml2.Clss.DecisionTable.Search(Ml2.AttrSel.Algs.BaseAttributeSelectionAlgorithm{weka.attributeSelection.ASSearch})">
            <summary>
            The search method used to find good attribute combinations for the
            decision table.
            </summary>    
        </member>
        <member name="M:Ml2.Clss.DecisionTable.CrossVal(System.Int32)">
            <summary>
            Sets the number of folds for cross validation (1 = leave one out).
            </summary>    
        </member>
        <member name="M:Ml2.Clss.DecisionTable.UseIBk(System.Boolean)">
            <summary>
            Sets whether IBk should be used instead of the majority class.
            </summary>    
        </member>
        <member name="M:Ml2.Clss.DecisionTable.DisplayRules(System.Boolean)">
            <summary>
            Sets whether rules are to be printed.
            </summary>    
        </member>
        <member name="M:Ml2.Clss.DecisionTable.Debug(System.Boolean)">
            <summary>
            If set to true, classifier may output additional info to the console.
            </summary>    
        </member>
        <member name="T:Ml2.Clss.EditableBayesNet">
            <summary>
            Bayes Network learning using various search algorithms and quality
            measures.<br/>Base class for a Bayes Network classifier. Provides datastructures
            (network structure, conditional probability distributions, etc.) and
            facilities common to Bayes Network learning algorithms like K2 and
            B.<br/><br/>For more information
            see:<br/><br/>http://www.cs.waikato.ac.nz/~remco/weka.pdf<br/><br/>Options:<br/><br/>-D = 	Do not use ADTree data
            structure<br/><br/>-B &lt;BIF file&gt; = 	BIF file to compare with<br/><br/>-Q
            weka.classifiers.bayes.net.search.SearchAlgorithm = 	Search algorithm<br/><br/>-E
            weka.classifiers.bayes.net.estimate.SimpleEstimator = 	Estimator algorithm<br/>
            </summary>
        </member>
        <member name="M:Ml2.Clss.EditableBayesNet.Distribution(System.Int32,System.Double[][])">
            <summary>
            
            </summary>    
        </member>
        <member name="M:Ml2.Clss.EditableBayesNet.Evidence(System.Int32,System.Int32)">
            <summary>
            
            </summary>    
        </member>
        <member name="M:Ml2.Clss.EditableBayesNet.Data(Ml2.Runtime)">
            <summary>
            
            </summary>    
        </member>
        <member name="M:Ml2.Clss.EditableBayesNet.Distribution(System.String,System.Double[][])">
            <summary>
            
            </summary>    
        </member>
        <member name="M:Ml2.Clss.EditableBayesNet.NodeName(System.Int32,System.String)">
            <summary>
            
            </summary>    
        </member>
        <member name="M:Ml2.Clss.EditableBayesNet.Position(System.Int32,System.Int32,System.Int32)">
            <summary>
            
            </summary>    
        </member>
        <member name="M:Ml2.Clss.EditableBayesNet.Margin(System.Int32,System.Double[])">
            <summary>
            
            </summary>    
        </member>
        <member name="M:Ml2.Clss.EditableBayesNet.BIFFile(System.String)">
            <summary>
            
            </summary>    
        </member>
        <member name="M:Ml2.Clss.EditableBayesNet.SearchAlgorithm(weka.classifiers.bayes.net.search.SearchAlgorithm)">
            <summary>
            Select method used for searching network structures.
            </summary>    
        </member>
        <member name="M:Ml2.Clss.EditableBayesNet.Estimator(weka.classifiers.bayes.net.estimate.BayesNetEstimator)">
            <summary>
            Select Estimator algorithm for finding the conditional probability tables
            of the Bayes Network.
            </summary>    
        </member>
        <member name="M:Ml2.Clss.EditableBayesNet.UseADTree(System.Boolean)">
            <summary>
            When ADTree (the data structure for increasing speed on counts, not to be
            confused with the classifier under the same name) is used learning time
            goes down typically. However, because ADTrees are memory intensive, memory
            problems may occur. Switching this option off makes the structure learning
            algorithms slower, and run with less memory. By default, ADTrees are used.
            </summary>    
        </member>
        <member name="M:Ml2.Clss.EditableBayesNet.Debug(System.Boolean)">
            <summary>
            If set to true, classifier may output additional info to the console.
            </summary>    
        </member>
        <member name="T:Ml2.Clss.FilteredClassifier">
            <summary>
            Class for running an arbitrary classifier on data that has been passed
            through an arbitrary filter. Like the classifier, the structure of the filter
            is based exclusively on the training data and test instances will be
            processed by the filter without changing their
            structure.<br/><br/>Options:<br/><br/>-F &lt;filter specification&gt; = 	Full class name of filter to use,
            followed<br/>	by filter options.<br/>	eg:
            "weka.filters.unsupervised.attribute.Remove -V -R 1,2"<br/>-D = 	If set, classifier is run in debug mode
            and<br/>	may output additional info to the console<br/>-W = 	Full name of base
            classifier.<br/>	(default: weka.classifiers.trees.J48)<br/><br/>Options
            specific to classifier weka.classifiers.trees.J48: = <br/>-U = 	Use unpruned
            tree.<br/>-O = 	Do not collapse tree.<br/>-C &lt;pruning confidence&gt; =
            	Set confidence threshold for pruning.<br/>	(default 0.25)<br/>-M &lt;minimum
            number of instances&gt; = 	Set minimum number of instances per
            leaf.<br/>	(default 2)<br/>-R = 	Use reduced error pruning.<br/>-N &lt;number of
            folds&gt; = 	Set number of folds for reduced error<br/>	pruning. One fold is
            used as pruning set.<br/>	(default 3)<br/>-B = 	Use binary splits only.<br/>-S
            = 	Don't perform subtree raising.<br/>-L = 	Do not clean up after the tree
            has been built.<br/>-A = 	Laplace smoothing for predicted
            probabilities.<br/>-J = 	Do not use MDL correction for info gain on numeric
            attributes.<br/>-Q &lt;seed&gt; = 	Seed for random data shuffling (default 1).
            </summary>
        </member>
        <member name="M:Ml2.Clss.FilteredClassifier.Filter(Ml2.Fltr.IBaseFilter{weka.filters.Filter})">
            <summary>
            The filter to be used.
            </summary>    
        </member>
        <member name="M:Ml2.Clss.FilteredClassifier.Classifier(Ml2.Clss.IBaseClassifier{weka.classifiers.Classifier})">
            <summary>
            The base classifier to be used.
            </summary>    
        </member>
        <member name="M:Ml2.Clss.FilteredClassifier.Debug(System.Boolean)">
            <summary>
            If set to true, classifier may output additional info to the console.
            </summary>    
        </member>
        <member name="T:Ml2.Clss.GaussianProcesses">
            <summary>
            Implements Gaussian processes for regression without
            hyperparameter-tuning. To make choosing an appropriate noise level easier, this implementation
            applies normalization/standardization to the target attribute as well as
            the other attributes (if normalization/standardizaton is turned on). Missing
            values are replaced by the global mean/mode. Nominal attributes are
            converted to binary ones. Note that kernel caching is turned off if the kernel
            used implements CachedKernel.<br/><br/>Options:<br/><br/>-D = 	If set,
            classifier is run in debug mode and<br/>	may output additional info to the
            console<br/>-L &lt;double&gt; = 	Level of Gaussian Noise wrt transformed target.
            (default 1)<br/>-N = 	Whether to 0=normalize/1=standardize/2=neither.
            (default 0=normalize)<br/>-K &lt;classname and parameters&gt; = 	The Kernel to
            use.<br/>	(default:
            weka.classifiers.functions.supportVector.PolyKernel)<br/><br/>Options specific to kernel
            weka.classifiers.functions.supportVector.PolyKernel: = <br/>-D = 	Enables debugging output (if available) to be
            printed.<br/>	(default: off)<br/>-no-checks = 	Turns off all checks - use with
            caution!<br/>	(default: checks on)<br/>-C &lt;num&gt; = 	The size of the
            cache (a prime number), 0 for full cache and <br/>	-1 to turn it
            off.<br/>	(default: 250007)<br/>-E &lt;num&gt; = 	The Exponent to use.<br/>	(default:
            1.0)<br/>-L = 	Use lower-order terms.<br/>	(default: no)
            </summary>
        </member>
        <member name="M:Ml2.Clss.GaussianProcesses.Noise(System.Double)">
            <summary>
            The level of Gaussian Noise (added to the diagonal of the Covariance
            Matrix), after the target has been normalized/standardized/left unchanged).
            </summary>    
        </member>
        <member name="M:Ml2.Clss.GaussianProcesses.FilterType(Ml2.Clss.GaussianProcesses.EFilterType)">
            <summary>
            Determines how/if the data will be transformed.
            </summary>    
        </member>
        <member name="M:Ml2.Clss.GaussianProcesses.Kernel(weka.classifiers.functions.supportVector.Kernel)">
            <summary>
            The kernel to use.
            </summary>    
        </member>
        <member name="M:Ml2.Clss.GaussianProcesses.Debug(System.Boolean)">
            <summary>
            If set to true, classifier may output additional info to the console.
            </summary>    
        </member>
        <member name="T:Ml2.Clss.IBk">
            <summary>
            K-nearest neighbours classifier. Can select appropriate value of K based
            on cross-validation. Can also do distance weighting.<br/><br/>For more
            information, see<br/><br/>D. Aha, D. Kibler (1991). Instance-based learning
            algorithms. Machine Learning. 6:37-66.<br/><br/>Options:<br/><br/>-I = 	Weight
            neighbours by the inverse of their distance<br/>	(use when k > 1)<br/>-F =
            	Weight neighbours by 1 - their distance<br/>	(use when k > 1)<br/>-K
            &lt;number of neighbors&gt; = 	Number of nearest neighbours (k) used in
            classification.<br/>	(Default = 1)<br/>-E = 	Minimise mean squared error rather
            than mean absolute<br/>	error when using -X option with numeric
            prediction.<br/>-W &lt;window size&gt; = 	Maximum number of training instances
            maintained.<br/>	Training instances are dropped FIFO. (Default = no window)<br/>-X =
            	Select the number of nearest neighbours between 1<br/>	and the k value
            specified using hold-one-out evaluation<br/>	on the training data (use when k
            > 1)<br/>-A = 	The nearest neighbour search algorithm to use (default:
            weka.core.neighboursearch.LinearNNSearch).<br/>
            </summary>
        </member>
        <member name="M:Ml2.Clss.IBk.KNN(System.Int32)">
            <summary>
            
            </summary>    
        </member>
        <member name="M:Ml2.Clss.IBk.WindowSize(System.Int32)">
            <summary>
            Gets the maximum number of instances allowed in the training pool. The
            addition of new instances above this value will result in old instances being
            removed. A value of 0 signifies no limit to the number of training
            instances.
            </summary>    
        </member>
        <member name="M:Ml2.Clss.IBk.DistanceWeighting(Ml2.Clss.IBk.EDistanceWeighting)">
            <summary>
            Gets the distance weighting method used.
            </summary>    
        </member>
        <member name="M:Ml2.Clss.IBk.CrossValidate(System.Boolean)">
            <summary>
            Whether hold-one-out cross-validation will be used to select the best k
            value.
            </summary>    
        </member>
        <member name="M:Ml2.Clss.IBk.MeanSquared(System.Boolean)">
            <summary>
            Whether the mean squared error is used rather than mean absolute error
            when doing cross-validation for regression problems.
            </summary>    
        </member>
        <member name="M:Ml2.Clss.IBk.NearestNeighbourSearchAlgorithm(weka.core.neighboursearch.NearestNeighbourSearch)">
            <summary>
            The nearest neighbour search algorithm to use (Default:
            weka.core.neighboursearch.LinearNNSearch).
            </summary>    
        </member>
        <member name="M:Ml2.Clss.IBk.Debug(System.Boolean)">
            <summary>
            If set to true, classifier may output additional info to the console.
            </summary>    
        </member>
        <member name="T:Ml2.Clss.InputMappedClassifier">
            <summary>
            Wrapper classifier that addresses incompatible training and test data by
            building a mapping between the training data that a classifier has been
            built with and the incoming test instances' structure. Model attributes that
            are not found in the incoming instances receive missing values, so do
            incoming nominal attribute values that the classifier has not seen before. A new
            classifier can be trained or an existing one loaded from a
            file.<br/><br/>Options:<br/><br/>-I = 	Ignore case when matching attribute names and
            nominal values.<br/>-M = 	Suppress the output of the mapping report.<br/>-trim =
            	Trim white space from either end of names before matching.<br/>-L &lt;path
            to model to load&gt; = 	Path to a model to load. If set, this
            model<br/>	will be used for prediction and any base classifier<br/>	specification will
            be ignored. Environment variables<br/>	may be used in the path (e.g.
            ${HOME}/myModel.model)<br/>-D = 	If set, classifier is run in debug mode
            and<br/>	may output additional info to the console<br/>-W = 	Full name of base
            classifier.<br/>	(default: weka.classifiers.rules.ZeroR)<br/><br/>Options
            specific to classifier weka.classifiers.rules.ZeroR: = <br/>-D = 	If set,
            classifier is run in debug mode and<br/>	may output additional info to the
            console
            </summary>
        </member>
        <member name="M:Ml2.Clss.InputMappedClassifier.IgnoreCaseForNames(System.Boolean)">
            <summary>
            Ignore case when matching attribute names and nomina values.
            </summary>    
        </member>
        <member name="M:Ml2.Clss.InputMappedClassifier.SuppressMappingReport(System.Boolean)">
            <summary>
            Don't output a report of model-to-input mappings.
            </summary>    
        </member>
        <member name="M:Ml2.Clss.InputMappedClassifier.Trim(System.Boolean)">
            <summary>
            Trim white space from each end of attribute names and nominal values
            before matching.
            </summary>    
        </member>
        <member name="M:Ml2.Clss.InputMappedClassifier.ModelPath(System.String)">
            <summary>
            Set the path from which to load a model. Loading occurs when the first
            test instance is received. Environment variables can be used in the supplied
            path.
            </summary>    
        </member>
        <member name="M:Ml2.Clss.InputMappedClassifier.Environment(weka.core.Environment)">
            <summary>
            
            </summary>    
        </member>
        <member name="M:Ml2.Clss.InputMappedClassifier.TestStructure(Ml2.Runtime)">
            <summary>
            
            </summary>    
        </member>
        <member name="M:Ml2.Clss.InputMappedClassifier.ModelHeader(Ml2.Runtime)">
            <summary>
            
            </summary>    
        </member>
        <member name="M:Ml2.Clss.InputMappedClassifier.Classifier(Ml2.Clss.IBaseClassifier{weka.classifiers.Classifier})">
            <summary>
            The base classifier to be used.
            </summary>    
        </member>
        <member name="M:Ml2.Clss.InputMappedClassifier.Debug(System.Boolean)">
            <summary>
            If set to true, classifier may output additional info to the console.
            </summary>    
        </member>
        <member name="T:Ml2.Clss.J48">
            <summary>
            Class for generating a pruned or unpruned C4.5 decision tree. For more
            information, see<br/><br/>Ross Quinlan (1993). C4.5: Programs for Machine
            Learning. Morgan Kaufmann Publishers, San Mateo,
            CA.<br/><br/>Options:<br/><br/>-U = 	Use unpruned tree.<br/>-O = 	Do not collapse tree.<br/>-C
            &lt;pruning confidence&gt; = 	Set confidence threshold for pruning.<br/>	(default
            0.25)<br/>-M &lt;minimum number of instances&gt; = 	Set minimum number of
            instances per leaf.<br/>	(default 2)<br/>-R = 	Use reduced error
            pruning.<br/>-N &lt;number of folds&gt; = 	Set number of folds for reduced
            error<br/>	pruning. One fold is used as pruning set.<br/>	(default 3)<br/>-B = 	Use
            binary splits only.<br/>-S = 	Don't perform subtree raising.<br/>-L = 	Do not
            clean up after the tree has been built.<br/>-A = 	Laplace smoothing for
            predicted probabilities.<br/>-J = 	Do not use MDL correction for info gain on
            numeric attributes.<br/>-Q &lt;seed&gt; = 	Seed for random data shuffling
            (default 1).
            </summary>
        </member>
        <member name="M:Ml2.Clss.J48.UseLaplace(System.Boolean)">
            <summary>
            Whether counts at leaves are smoothed based on Laplace.
            </summary>    
        </member>
        <member name="M:Ml2.Clss.J48.UseMDLcorrection(System.Boolean)">
            <summary>
            Whether MDL correction is used when finding splits on numeric attributes.
            </summary>    
        </member>
        <member name="M:Ml2.Clss.J48.Unpruned(System.Boolean)">
            <summary>
            Whether pruning is performed.
            </summary>    
        </member>
        <member name="M:Ml2.Clss.J48.CollapseTree(System.Boolean)">
            <summary>
            Whether parts are removed that do not reduce training error.
            </summary>    
        </member>
        <member name="M:Ml2.Clss.J48.ConfidenceFactor(System.Single)">
            <summary>
            The confidence factor used for pruning (smaller values incur more
            pruning).
            </summary>    
        </member>
        <member name="M:Ml2.Clss.J48.MinNumObj(System.Int32)">
            <summary>
            The minimum number of instances per leaf.
            </summary>    
        </member>
        <member name="M:Ml2.Clss.J48.ReducedErrorPruning(System.Boolean)">
            <summary>
            Whether reduced-error pruning is used instead of C.4.5 pruning.
            </summary>    
        </member>
        <member name="M:Ml2.Clss.J48.NumFolds(System.Int32)">
            <summary>
            Determines the amount of data used for reduced-error pruning. One fold is
            used for pruning, the rest for growing the tree.
            </summary>    
        </member>
        <member name="M:Ml2.Clss.J48.BinarySplits(System.Boolean)">
            <summary>
            Whether to use binary splits on nominal attributes when building the
            trees.
            </summary>    
        </member>
        <member name="M:Ml2.Clss.J48.SubtreeRaising(System.Boolean)">
            <summary>
            Whether to consider the subtree raising operation when pruning.
            </summary>    
        </member>
        <member name="M:Ml2.Clss.J48.SaveInstanceData(System.Boolean)">
            <summary>
            Whether to save the training data for visualization.
            </summary>    
        </member>
        <member name="M:Ml2.Clss.J48.Debug(System.Boolean)">
            <summary>
            If set to true, classifier may output additional info to the console.
            </summary>    
        </member>
        <member name="T:Ml2.Clss.JRip">
            <summary>
            This class implements a propositional rule learner, Repeated Incremental
            Pruning to Produce Error Reduction (RIPPER), which was proposed by William
            W. Cohen as an optimized version of IREP. <br/><br/>The algorithm is
            briefly described as follows: <br/><br/>Initialize RS = {}, and for each class
            from the less prevalent one to the more frequent one, DO: <br/><br/>1.
            Building stage:<br/>Repeat 1.1 and 1.2 until the descrition length (DL) of the
            ruleset and examples is 64 bits greater than the smallest DL met so far, or
            there are no positive examples, or the error rate >= 50%. <br/><br/>1.1.
            Grow phase:<br/>Grow one rule by greedily adding antecedents (or conditions)
            to the rule until the rule is perfect (i.e. 100% accurate). The procedure
            tries every possible value of each attribute and selects the condition with
            highest information gain: p(log(p/t)-log(P/T)).<br/><br/>1.2. Prune
            phase:<br/>Incrementally prune each rule and allow the pruning of any final
            sequences of the antecedents;The pruning metric is (p-n)/(p+n) -- but it's
            actually 2p/(p+n) -1, so in this implementation we simply use p/(p+n) (actually
            (p+1)/(p+n+2), thus if p+n is 0, it's 0.5).<br/><br/>2. Optimization
            stage:<br/> after generating the initial ruleset {Ri}, generate and prune two
            variants of each rule Ri from randomized data using procedure 1.1 and 1.2. But
            one variant is generated from an empty rule while the other is generated by
            greedily adding antecedents to the original rule. Moreover, the pruning
            metric used here is (TP+TN)/(P+N).Then the smallest possible DL for each
            variant and the original rule is computed. The variant with the minimal DL is
            selected as the final representative of Ri in the ruleset.After all the rules
            in {Ri} have been examined and if there are still residual positives, more
            rules are generated based on the residual positives using Building Stage
            again. <br/>3. Delete the rules from the ruleset that would increase the DL
            of the whole ruleset if it were in it. and add resultant ruleset to RS.
            <br/>ENDDO<br/><br/>Note that there seem to be 2 bugs in the original ripper
            program that would affect the ruleset size and accuracy slightly. This
            implementation avoids these bugs and thus is a little bit different from Cohen's
            original implementation. Even after fixing the bugs, since the order of
            classes with the same frequency is not defined in ripper, there still seems
            to be some trivial difference between this implementation and the original
            ripper, especially for audiology data in UCI repository, where there are
            lots of classes of few instances.<br/><br/>Details please
            see:<br/><br/>William W. Cohen: Fast Effective Rule Induction. In: Twelfth International
            Conference on Machine Learning, 115-123, 1995.<br/><br/>PS. We have compared this
            implementation with the original ripper implementation in aspects of
            accuracy, ruleset size and running time on both artificial data "ab+bcd+defg"
            and UCI datasets. In all these aspects it seems to be quite comparable to the
            original ripper implementation. However, we didn't consider memory
            consumption optimization in this
            implementation.<br/><br/><br/><br/>Options:<br/><br/>-F &lt;number of folds&gt; = 	Set number of folds for REP<br/>	One fold
            is used as pruning set.<br/>	(default 3)<br/>-N &lt;min. weights&gt; =
            	Set the minimal weights of instances<br/>	within a split.<br/>	(default
            2.0)<br/>-O &lt;number of runs&gt; = 	Set the number of runs
            of<br/>	optimizations. (Default: 2)<br/>-D = 	Set whether turn on the<br/>	debug mode
            (Default: false)<br/>-S &lt;seed&gt; = 	The seed of randomization<br/>	(Default:
            1)<br/>-E = 	Whether NOT check the error rate>=0.5<br/>	in stopping criteria
            	(default: check)<br/>-P = 	Whether NOT use pruning<br/>	(default: use
            pruning)
            </summary>
        </member>
        <member name="M:Ml2.Clss.JRip.Folds(System.Int32)">
            <summary>
            Determines the amount of data used for pruning. One fold is used for
            pruning, the rest for growing the rules.
            </summary>    
        </member>
        <member name="M:Ml2.Clss.JRip.MinNo(System.Double)">
            <summary>
            The minimum total weight of the instances in a rule.
            </summary>    
        </member>
        <member name="M:Ml2.Clss.JRip.Optimizations(System.Int32)">
            <summary>
            The number of optimization runs.
            </summary>    
        </member>
        <member name="M:Ml2.Clss.JRip.Debug(System.Boolean)">
            <summary>
            Whether debug information is output to the console.
            </summary>    
        </member>
        <member name="M:Ml2.Clss.JRip.CheckErrorRate(System.Boolean)">
            <summary>
            Whether check for error rate >= 1/2 is included in stopping criterion.
            </summary>    
        </member>
        <member name="M:Ml2.Clss.JRip.UsePruning(System.Boolean)">
            <summary>
            Whether pruning is performed.
            </summary>    
        </member>
        <member name="T:Ml2.Clss.KStar">
            <summary>
            K* is an instance-based classifier, that is the class of a test instance
            is based upon the class of those training instances similar to it, as
            determined by some similarity function. It differs from other instance-based
            learners in that it uses an entropy-based distance function.<br/><br/>For more
            information on K*, see<br/><br/>John G. Cleary, Leonard E. Trigg: K*: An
            Instance-based Learner Using an Entropic Distance Measure. In: 12th
            International Conference on Machine Learning, 108-114,
            1995.<br/><br/>Options:<br/><br/>-B &lt;num&gt; = 	Manual blend setting (default 20%)<br/><br/>-E =
            	Enable entropic auto-blend setting (symbolic class only)<br/><br/>-M
            &lt;char&gt; = 	Specify the missing value treatment mode (default a)<br/>	Valid
            options are: a(verage), d(elete), m(axdiff), n(ormal)<br/>
            </summary>
        </member>
        <member name="M:Ml2.Clss.KStar.GlobalBlend(System.Int32)">
            <summary>
            The parameter for global blending. Values are restricted to [0,100].
            </summary>    
        </member>
        <member name="M:Ml2.Clss.KStar.EntropicAutoBlend(System.Boolean)">
            <summary>
            Whether entropy-based blending is to be used.
            </summary>    
        </member>
        <member name="M:Ml2.Clss.KStar.MissingMode(Ml2.Clss.KStar.EMissingMode)">
            <summary>
            Determines how missing attribute values are treated.
            </summary>    
        </member>
        <member name="M:Ml2.Clss.KStar.Debug(System.Boolean)">
            <summary>
            If set to true, classifier may output additional info to the console.
            </summary>    
        </member>
        <member name="T:Ml2.Clss.LinearRegression">
            <summary>
            Class for using linear regression for prediction. Uses the Akaike
            criterion for model selection, and is able to deal with weighted
            instances.<br/><br/>Options:<br/><br/>-D = 	Produce debugging output.<br/>	(default no
            debugging output)<br/>-S &lt;number of selection method&gt; = 	Set the attribute
            selection method to use. 1 = None, 2 = Greedy.<br/>	(default 0 = M5'
            method)<br/>-C = 	Do not try to eliminate colinear attributes.<br/><br/>-R
            &lt;double&gt; = 	Set ridge parameter (default 1.0e-8).<br/><br/>-minimal =
            	Conserve memory, don't keep dataset header and means/stdevs.<br/>	Model cannot
            be printed out if this option is enabled.	(default: keep data)
            </summary>
        </member>
        <member name="M:Ml2.Clss.LinearRegression.AttributeSelectionMethod(Ml2.Clss.LinearRegression.EAttributeSelectionMethod)">
            <summary>
            Set the method used to select attributes for use in the linear
            regression. Available methods are: no attribute selection, attribute selection using
            M5's method (step through the attributes removing the one with the smallest
            standardised coefficient until no improvement is observed in the estimate
            of the error given by the Akaike information criterion), and a greedy
            selection using the Akaike information metric.
            </summary>    
        </member>
        <member name="M:Ml2.Clss.LinearRegression.Ridge(System.Double)">
            <summary>
            The value of the Ridge parameter.
            </summary>    
        </member>
        <member name="M:Ml2.Clss.LinearRegression.EliminateColinearAttributes(System.Boolean)">
            <summary>
            Eliminate colinear attributes.
            </summary>    
        </member>
        <member name="M:Ml2.Clss.LinearRegression.Minimal(System.Boolean)">
            <summary>
            If enabled, dataset header, means and stdevs get discarded to conserve
            memory; also, the model cannot be printed out.
            </summary>    
        </member>
        <member name="M:Ml2.Clss.LinearRegression.Debug(System.Boolean)">
            <summary>
            If set to true, classifier may output additional info to the console.
            </summary>    
        </member>
        <member name="T:Ml2.Clss.LMT">
            <summary>
            Classifier for building 'logistic model trees', which are classification
            trees with logistic regression functions at the leaves. The algorithm can
            deal with binary and multi-class target variables, numeric and nominal
            attributes and missing values.<br/><br/>For more information see:
            <br/><br/>Niels Landwehr, Mark Hall, Eibe Frank (2005). Logistic Model Trees. Machine
            Learning. 95(1-2):161-205.<br/><br/>Marc Sumner, Eibe Frank, Mark Hall:
            Speeding up Logistic Model Tree Induction. In: 9th European Conference on
            Principles and Practice of Knowledge Discovery in Databases, 675-683,
            2005.<br/><br/>Options:<br/><br/>-B = 	Binary splits (convert nominal attributes to
            binary ones)<br/>-R = 	Split on residuals instead of class values<br/>-C =
            	Use cross-validation for boosting at all nodes (i.e., disable
            heuristic)<br/>-P = 	Use error on probabilities instead of misclassification error for
            stopping criterion of LogitBoost.<br/>-I &lt;numIterations&gt; = 	Set fixed
            number of iterations for LogitBoost (instead of using
            cross-validation)<br/>-M &lt;numInstances&gt; = 	Set minimum number of instances at which a node
            can be split (default 15)<br/>-W &lt;beta&gt; = 	Set beta for weight
            trimming for LogitBoost. Set to 0 (default) for no weight trimming.<br/>-A = 	The
            AIC is used to choose the best iteration.
            </summary>
        </member>
        <member name="M:Ml2.Clss.LMT.ConvertNominal(System.Boolean)">
            <summary>
            Convert all nominal attributes to binary ones before building the tree.
            This means that all splits in the final tree will be binary.
            </summary>    
        </member>
        <member name="M:Ml2.Clss.LMT.SplitOnResiduals(System.Boolean)">
            <summary>
            Set splitting criterion based on the residuals of LogitBoost. There are
            two possible splitting criteria for LMT: the default is to use the C4.5
            splitting criterion that uses information gain on the class variable. The other
            splitting criterion tries to improve the purity in the residuals produces
            when fitting the logistic regression functions. The choice of the splitting
            criterion does not usually affect classification accuracy much, but can
            produce different trees.
            </summary>    
        </member>
        <member name="M:Ml2.Clss.LMT.FastRegression(System.Boolean)">
            <summary>
            Use heuristic that avoids cross-validating the number of Logit-Boost
            iterations at every node. When fitting the logistic regression functions at a
            node, LMT has to determine the number of LogitBoost iterations to run.
            Originally, this number was cross-validated at every node in the tree. To save
            time, this heuristic cross-validates the number only once and then uses that
            number at every node in the tree. Usually this does not decrease accuracy
            but improves runtime considerably.
            </summary>    
        </member>
        <member name="M:Ml2.Clss.LMT.ErrorOnProbabilities(System.Boolean)">
            <summary>
            Minimize error on probabilities instead of misclassification error when
            cross-validating the number of LogitBoost iterations. When set, the number
            of LogitBoost iterations is chosen that minimizes the root mean squared
            error instead of the misclassification error.
            </summary>    
        </member>
        <!-- Badly formed XML comment ignored for member "M:Ml2.Clss.LMT.NumBoostingIterations(System.Int32)" -->
        <member name="M:Ml2.Clss.LMT.MinNumInstances(System.Int32)">
            <summary>
            Set the minimum number of instances at which a node is considered for
            splitting. The default value is 15.
            </summary>    
        </member>
        <member name="M:Ml2.Clss.LMT.WeightTrimBeta(System.Double)">
            <summary>
            Set the beta value used for weight trimming in LogitBoost. Only instances
            carrying (1 - beta)% of the weight from previous iteration are used in the
            next iteration. Set to 0 for no weight trimming. The default value is 0.
            </summary>    
        </member>
        <member name="M:Ml2.Clss.LMT.UseAIC(System.Boolean)">
            <summary>
            The AIC is used to determine when to stop LogitBoost iterations. The
            default is not to use AIC.
            </summary>    
        </member>
        <member name="M:Ml2.Clss.LMT.Debug(System.Boolean)">
            <summary>
            If set to true, classifier may output additional info to the console.
            </summary>    
        </member>
        <member name="T:Ml2.Clss.Logistic">
            <summary>
            Class for building and using a multinomial logistic regression model with
            a ridge estimator.<br/><br/>There are some modifications, however,
            compared to the paper of leCessie and van Houwelingen(1992): <br/><br/>If there
            are k classes for n instances with m attributes, the parameter matrix B to be
            calculated will be an m*(k-1) matrix.<br/><br/>The probability for class j
            with the exception of the last class is<br/><br/>Pj(Xi) =
            exp(XiBj)/((sum[j=1..(k-1)]exp(Xi*Bj))+1) <br/><br/>The last class has
            probability<br/><br/>1-(sum[j=1..(k-1)]Pj(Xi)) <br/>	=
            1/((sum[j=1..(k-1)]exp(Xi*Bj))+1)<br/><br/>The (negative) multinomial log-likelihood is thus: <br/><br/>L =
            -sum[i=1..n]{<br/>	sum[j=1..(k-1)](Yij * ln(Pj(Xi)))<br/>	+(1 -
            (sum[j=1..(k-1)]Yij)) <br/>	* ln(1 - sum[j=1..(k-1)]Pj(Xi))<br/>	} + ridge *
            (B^2)<br/><br/>In order to find the matrix B for which L is minimised, a Quasi-Newton
            Method is used to search for the optimized values of the m*(k-1) variables. Note
            that before we use the optimization procedure, we 'squeeze' the matrix B
            into a m*(k-1) vector. For details of the optimization procedure, please
            check weka.core.Optimization class.<br/><br/>Although original Logistic
            Regression does not deal with instance weights, we modify the algorithm a little
            bit to handle the instance weights.<br/><br/>For more information
            see:<br/><br/>le Cessie, S., van Houwelingen, J.C. (1992). Ridge Estimators in
            Logistic Regression. Applied Statistics. 41(1):191-201.<br/><br/>Note: Missing
            values are replaced using a ReplaceMissingValuesFilter, and nominal
            attributes are transformed into numeric attributes using a
            NominalToBinaryFilter.<br/><br/>Options:<br/><br/>-D = 	Turn on debugging output.<br/>-C = 	Use
            conjugate gradient descent rather than BFGS updates.<br/>-R &lt;ridge&gt; =
            	Set the ridge in the log-likelihood.<br/>-M &lt;number&gt; = 	Set the
            maximum number of iterations (default -1, until convergence).
            </summary>
        </member>
        <member name="M:Ml2.Clss.Logistic.Debug(System.Boolean)">
            <summary>
            Output debug information to the console.
            </summary>    
        </member>
        <member name="M:Ml2.Clss.Logistic.UseConjugateGradientDescent(System.Boolean)">
            <summary>
            Use conjugate gradient descent rather than BFGS updates; faster for
            problems with many parameters.
            </summary>    
        </member>
        <member name="M:Ml2.Clss.Logistic.Ridge(System.Double)">
            <summary>
            Set the Ridge value in the log-likelihood.
            </summary>    
        </member>
        <member name="M:Ml2.Clss.Logistic.MaxIts(System.Int32)">
            <summary>
            Maximum number of iterations to perform.
            </summary>    
        </member>
        <member name="T:Ml2.Clss.LogisticBase">
            <summary>
            No class description found.
            </summary>
        </member>
        <member name="M:Ml2.Clss.LogisticBase.MaxIterations(System.Int32)">
            <summary>
            
            </summary>    
        </member>
        <member name="M:Ml2.Clss.LogisticBase.HeuristicStop(System.Int32)">
            <summary>
            
            </summary>    
        </member>
        <member name="M:Ml2.Clss.LogisticBase.WeightTrimBeta(System.Double)">
            <summary>
            
            </summary>    
        </member>
        <member name="M:Ml2.Clss.LogisticBase.UseAIC(System.Boolean)">
            <summary>
            
            </summary>    
        </member>
        <member name="M:Ml2.Clss.LogisticBase.Debug(System.Boolean)">
            <summary>
            If set to true, classifier may output additional info to the console.
            </summary>    
        </member>
        <member name="T:Ml2.Clss.LogitBoost">
            <summary>
            Class for performing additive logistic regression. <br/>This class
            performs classification using a regression scheme as the base learner, and can
            handle multi-class problems. For more information, see<br/><br/>J. Friedman,
            T. Hastie, R. Tibshirani (1998). Additive Logistic Regression: a
            Statistical View of Boosting. Stanford University.<br/><br/>Can do efficient internal
            cross-validation to determine appropriate number of
            iterations.<br/><br/>Options:<br/><br/>-Q = 	Use resampling instead of reweighting for
            boosting.<br/>-P &lt;percent&gt; = 	Percentage of weight mass to base training
            on.<br/>	(default 100, reduce to around 90 speed up)<br/>-F &lt;num&gt; = 	Number
            of folds for internal cross-validation.<br/>	(default 0 -- no
            cross-validation)<br/>-R &lt;num&gt; = 	Number of runs for internal
            cross-validation.<br/>	(default 1)<br/>-L &lt;num&gt; = 	Threshold on the improvement of the
            likelihood.<br/>	(default -Double.MAX_VALUE)<br/>-H &lt;num&gt; = 	Shrinkage
            parameter.<br/>	(default 1)<br/>-S &lt;num&gt; = 	Random number
            seed.<br/>	(default 1)<br/>-I &lt;num&gt; = 	Number of iterations.<br/>	(default
            10)<br/>-D = 	If set, classifier is run in debug mode and<br/>	may output
            additional info to the console<br/>-W = 	Full name of base
            classifier.<br/>	(default: weka.classifiers.trees.DecisionStump)<br/><br/>Options specific to
            classifier weka.classifiers.trees.DecisionStump: = <br/>-D = 	If set,
            classifier is run in debug mode and<br/>	may output additional info to the console
            </summary>
        </member>
        <member name="M:Ml2.Clss.LogitBoost.NumFolds(System.Int32)">
            <summary>
            Number of folds for internal cross-validation (default 0 means no
            cross-validation is performed).
            </summary>    
        </member>
        <member name="M:Ml2.Clss.LogitBoost.NumRuns(System.Int32)">
            <summary>
            Number of runs for internal cross-validation.
            </summary>    
        </member>
        <member name="M:Ml2.Clss.LogitBoost.WeightThreshold(System.Int32)">
            <summary>
            Weight threshold for weight pruning (reduce to 90 for speeding up
            learning process).
            </summary>    
        </member>
        <member name="M:Ml2.Clss.LogitBoost.LikelihoodThreshold(System.Double)">
            <summary>
            Threshold on improvement in likelihood.
            </summary>    
        </member>
        <member name="M:Ml2.Clss.LogitBoost.Shrinkage(System.Double)">
            <summary>
            Shrinkage parameter (use small value like 0.1 to reduce overfitting).
            </summary>    
        </member>
        <member name="M:Ml2.Clss.LogitBoost.UseResampling(System.Boolean)">
            <summary>
            Whether resampling is used instead of reweighting.
            </summary>    
        </member>
        <member name="M:Ml2.Clss.LogitBoost.NumIterations(System.Int32)">
            <summary>
            The number of iterations to be performed.
            </summary>    
        </member>
        <member name="M:Ml2.Clss.LogitBoost.Classifier(Ml2.Clss.IBaseClassifier{weka.classifiers.Classifier})">
            <summary>
            The base classifier to be used.
            </summary>    
        </member>
        <member name="M:Ml2.Clss.LogitBoost.Debug(System.Boolean)">
            <summary>
            If set to true, classifier may output additional info to the console.
            </summary>    
        </member>
        <member name="T:Ml2.Clss.LWL">
            <summary>
            Locally weighted learning. Uses an instance-based algorithm to assign
            instance weights which are then used by a specified
            WeightedInstancesHandler.<br/>Can do classification (e.g. using naive Bayes) or regression (e.g.
            using linear regression).<br/><br/>For more info, see<br/><br/>Eibe Frank, Mark
            Hall, Bernhard Pfahringer: Locally Weighted Naive Bayes. In: 19th
            Conference in Uncertainty in Artificial Intelligence, 249-256, 2003.<br/><br/>C.
            Atkeson, A. Moore, S. Schaal (1996). Locally weighted learning. AI
            Review..<br/><br/>Options:<br/><br/>-A = 	The nearest neighbour search algorithm to
            use (default: weka.core.neighboursearch.LinearNNSearch).<br/><br/>-K
            &lt;number of neighbours&gt; = 	Set the number of neighbours used to set the
            kernel bandwidth.<br/>	(default all)<br/>-U &lt;number of weighting method&gt; =
            	Set the weighting kernel shape to use. 0=Linear,
            1=Epanechnikov,<br/>	2=Tricube, 3=Inverse, 4=Gaussian.<br/>	(default 0 = Linear)<br/>-D = 	If set,
            classifier is run in debug mode and<br/>	may output additional info to the
            console<br/>-W = 	Full name of base classifier.<br/>	(default:
            weka.classifiers.trees.DecisionStump)<br/><br/>Options specific to classifier
            weka.classifiers.trees.DecisionStump: = <br/>-D = 	If set, classifier is run in
            debug mode and<br/>	may output additional info to the console
            </summary>
        </member>
        <member name="M:Ml2.Clss.LWL.KNN(System.Int32)">
            <summary>
            
            </summary>    
        </member>
        <member name="M:Ml2.Clss.LWL.WeightingKernel(System.Int32)">
            <summary>
            Determines weighting function. [0 = Linear, 1 = Epnechnikov,2 = Tricube,
            3 = Inverse, 4 = Gaussian and 5 = Constant. (default 0 = Linear)].
            </summary>    
        </member>
        <member name="M:Ml2.Clss.LWL.NearestNeighbourSearchAlgorithm(weka.core.neighboursearch.NearestNeighbourSearch)">
            <summary>
            The nearest neighbour search algorithm to use (Default: LinearNN).
            </summary>    
        </member>
        <member name="M:Ml2.Clss.LWL.Classifier(Ml2.Clss.IBaseClassifier{weka.classifiers.Classifier})">
            <summary>
            The base classifier to be used.
            </summary>    
        </member>
        <member name="M:Ml2.Clss.LWL.Debug(System.Boolean)">
            <summary>
            If set to true, classifier may output additional info to the console.
            </summary>    
        </member>
        <member name="T:Ml2.Clss.M5P">
            <summary>
            M5Base. Implements base routines for generating M5 Model trees and
            rules<br/>The original algorithm M5 was invented by R. Quinlan and Yong Wang made
            improvements.<br/><br/>For more information see:<br/><br/>Ross J. Quinlan:
            Learning with Continuous Classes. In: 5th Australian Joint Conference on
            Artificial Intelligence, Singapore, 343-348, 1992.<br/><br/>Y. Wang, I. H.
            Witten: Induction of model trees for predicting continuous classes. In:
            Poster papers of the 9th European Conference on Machine Learning,
            1997.<br/><br/>Options:<br/><br/>-N = 	Use unpruned tree/rules<br/>-U = 	Use unsmoothed
            predictions<br/>-R = 	Build regression tree/rule rather than a model
            tree/rule<br/>-M &lt;minimum number of instances&gt; = 	Set minimum number of
            instances per leaf<br/>	(default 4)<br/>-L = 	Save instances at the nodes
            in<br/>	the tree (for visualization purposes)
            </summary>
        </member>
        <member name="M:Ml2.Clss.M5P.SaveInstances(System.Boolean)">
            <summary>
            Whether to save instance data at each node in the tree for visualization
            purposes.
            </summary>    
        </member>
        <member name="M:Ml2.Clss.M5P.Unpruned(System.Boolean)">
            <summary>
            Whether unpruned tree/rules are to be generated.
            </summary>    
        </member>
        <member name="M:Ml2.Clss.M5P.UseUnsmoothed(System.Boolean)">
            <summary>
            Whether to use unsmoothed predictions.
            </summary>    
        </member>
        <member name="M:Ml2.Clss.M5P.BuildRegressionTree(System.Boolean)">
            <summary>
            Whether to generate a regression tree/rule instead of a model tree/rule.
            </summary>    
        </member>
        <member name="M:Ml2.Clss.M5P.MinNumInstances(System.Double)">
            <summary>
            The minimum number of instances to allow at a leaf node.
            </summary>    
        </member>
        <member name="M:Ml2.Clss.M5P.Debug(System.Boolean)">
            <summary>
            If set to true, classifier may output additional info to the console.
            </summary>    
        </member>
        <member name="T:Ml2.Clss.M5Rules">
            <summary>
            Generates a decision list for regression problems using
            separate-and-conquer. In each iteration it builds a model tree using M5 and makes the "best"
            leaf into a rule.<br/><br/>For more information see:<br/><br/>Geoffrey
            Holmes, Mark Hall, Eibe Frank: Generating Rule Sets from Model Trees. In:
            Twelfth Australian Joint Conference on Artificial Intelligence, 1-12,
            1999.<br/><br/>Ross J. Quinlan: Learning with Continuous Classes. In: 5th Australian
            Joint Conference on Artificial Intelligence, Singapore, 343-348,
            1992.<br/><br/>Y. Wang, I. H. Witten: Induction of model trees for predicting
            continuous classes. In: Poster papers of the 9th European Conference on Machine
            Learning, 1997.<br/><br/>Options:<br/><br/>-N = 	Use unpruned
            tree/rules<br/>-U = 	Use unsmoothed predictions<br/>-R = 	Build regression tree/rule
            rather than a model tree/rule<br/>-M &lt;minimum number of instances&gt; = 	Set
            minimum number of instances per leaf<br/>	(default 4)
            </summary>
        </member>
        <member name="M:Ml2.Clss.M5Rules.Unpruned(System.Boolean)">
            <summary>
            Whether unpruned tree/rules are to be generated.
            </summary>    
        </member>
        <member name="M:Ml2.Clss.M5Rules.UseUnsmoothed(System.Boolean)">
            <summary>
            Whether to use unsmoothed predictions.
            </summary>    
        </member>
        <member name="M:Ml2.Clss.M5Rules.BuildRegressionTree(System.Boolean)">
            <summary>
            Whether to generate a regression tree/rule instead of a model tree/rule.
            </summary>    
        </member>
        <member name="M:Ml2.Clss.M5Rules.MinNumInstances(System.Double)">
            <summary>
            The minimum number of instances to allow at a leaf node.
            </summary>    
        </member>
        <member name="M:Ml2.Clss.M5Rules.Debug(System.Boolean)">
            <summary>
            If set to true, classifier may output additional info to the console.
            </summary>    
        </member>
        <member name="T:Ml2.Clss.MultiClassClassifier">
            <summary>
            A metaclassifier for handling multi-class datasets with 2-class
            classifiers. This classifier is also capable of applying error correcting output
            codes for increased accuracy.<br/><br/>Options:<br/><br/>-M &lt;num&gt; =
            	Sets the method to use. Valid values are 0 (1-against-all),<br/>	1 (random
            codes), 2 (exhaustive code), and 3 (1-against-1). (default 0)<br/><br/>-R
            &lt;num&gt; = 	Sets the multiplier when using random codes. (default
            2.0)<br/>-P = 	Use pairwise coupling (only has an effect for 1-against1)<br/>-S
            &lt;num&gt; = 	Random number seed.<br/>	(default 1)<br/>-D = 	If set, classifier
            is run in debug mode and<br/>	may output additional info to the
            console<br/>-W = 	Full name of base classifier.<br/>	(default:
            weka.classifiers.functions.Logistic)<br/><br/>Options specific to classifier
            weka.classifiers.functions.Logistic: = <br/>-D = 	Turn on debugging output.<br/>-C = 	Use
            conjugate gradient descent rather than BFGS updates.<br/>-R &lt;ridge&gt; = 	Set
            the ridge in the log-likelihood.<br/>-M &lt;number&gt; = 	Set the maximum
            number of iterations (default -1, until convergence).
            </summary>
        </member>
        <member name="M:Ml2.Clss.MultiClassClassifier.Method(Ml2.Clss.MultiClassClassifier.EMethod)">
            <summary>
            Sets the method to use for transforming the multi-class problem into
            several 2-class ones.
            </summary>    
        </member>
        <member name="M:Ml2.Clss.MultiClassClassifier.RandomWidthFactor(System.Double)">
            <summary>
            Sets the width multiplier when using random codes. The number of codes
            generated will be thus number multiplied by the number of classes.
            </summary>    
        </member>
        <member name="M:Ml2.Clss.MultiClassClassifier.UsePairwiseCoupling(System.Boolean)">
            <summary>
            Use pairwise coupling (only has an effect for 1-against-1).
            </summary>    
        </member>
        <member name="M:Ml2.Clss.MultiClassClassifier.Classifier(Ml2.Clss.IBaseClassifier{weka.classifiers.Classifier})">
            <summary>
            The base classifier to be used.
            </summary>    
        </member>
        <member name="M:Ml2.Clss.MultiClassClassifier.Debug(System.Boolean)">
            <summary>
            If set to true, classifier may output additional info to the console.
            </summary>    
        </member>
        <member name="T:Ml2.Clss.MultiClassClassifierUpdateable">
            <summary>
            A metaclassifier for handling multi-class datasets with 2-class
            classifiers. This classifier is also capable of applying error correcting output
            codes for increased accuracy. The base classifier must be an updateable
            classifier<br/><br/>Options:<br/><br/>-M &lt;num&gt; = 	Sets the method to use.
            Valid values are 0 (1-against-all),<br/>	1 (random codes), 2 (exhaustive
            code), and 3 (1-against-1). (default 0)<br/><br/>-R &lt;num&gt; = 	Sets the
            multiplier when using random codes. (default 2.0)<br/>-P = 	Use pairwise
            coupling (only has an effect for 1-against1)<br/>-S &lt;num&gt; = 	Random
            number seed.<br/>	(default 1)<br/>-D = 	If set, classifier is run in debug mode
            and<br/>	may output additional info to the console<br/>-W = 	Full name of
            base classifier.<br/>	(default:
            weka.classifiers.functions.Logistic)<br/><br/>Options specific to classifier weka.classifiers.functions.SGD: = <br/>-F
            = 	Set the loss function to minimize. 0 = hinge loss (SVM), 1 = log loss
            (logistic regression),<br/>	2 = squared loss (regression).<br/>	(default =
            0)<br/>-L = 	The learning rate. If normalization is<br/>	turned off (as it is
            automatically for streaming data), then the<br/>	default learning rate
            will need to be reduced (try 0.0001).<br/>	(default = 0.01).<br/>-R
            &lt;double&gt; = 	The lambda regularization constant (default = 0.0001)<br/>-E
            &lt;integer&gt; = 	The number of epochs to perform (batch learning only, default
            = 500)<br/>-N = 	Don't normalize the data<br/>-M = 	Don't replace missing
            values
            </summary>
        </member>
        <member name="M:Ml2.Clss.MultiClassClassifierUpdateable.Method(Ml2.Clss.MultiClassClassifierUpdateable.EMethod)">
            <summary>
            Sets the method to use for transforming the multi-class problem into
            several 2-class ones.
            </summary>    
        </member>
        <member name="M:Ml2.Clss.MultiClassClassifierUpdateable.RandomWidthFactor(System.Double)">
            <summary>
            Sets the width multiplier when using random codes. The number of codes
            generated will be thus number multiplied by the number of classes.
            </summary>    
        </member>
        <member name="M:Ml2.Clss.MultiClassClassifierUpdateable.UsePairwiseCoupling(System.Boolean)">
            <summary>
            Use pairwise coupling (only has an effect for 1-against-1).
            </summary>    
        </member>
        <member name="M:Ml2.Clss.MultiClassClassifierUpdateable.Classifier(Ml2.Clss.IBaseClassifier{weka.classifiers.Classifier})">
            <summary>
            The base classifier to be used.
            </summary>    
        </member>
        <member name="M:Ml2.Clss.MultiClassClassifierUpdateable.Debug(System.Boolean)">
            <summary>
            If set to true, classifier may output additional info to the console.
            </summary>    
        </member>
        <member name="T:Ml2.Clss.MultilayerPerceptron">
            <summary>
            A Classifier that uses backpropagation to classify instances.<br/>This
            network can be built by hand, created by an algorithm or both. The network
            can also be monitored and modified during training time. The nodes in this
            network are all sigmoid (except for when the class is numeric in which case
            the the output nodes become unthresholded linear
            units).<br/><br/>Options:<br/><br/>-L &lt;learning rate&gt; = 	Learning Rate for the backpropagation
            algorithm.<br/>	(Value should be between 0 - 1, Default = 0.3).<br/>-M
            &lt;momentum&gt; = 	Momentum Rate for the backpropagation algorithm.<br/>	(Value
            should be between 0 - 1, Default = 0.2).<br/>-N &lt;number of epochs&gt; =
            	Number of epochs to train through.<br/>	(Default = 500).<br/>-V
            &lt;percentage size of validation set&gt; = 	Percentage size of validation set to
            use to terminate<br/>	training (if this is non zero it can pre-empt num of
            epochs.<br/>	(Value should be between 0 - 100, Default = 0).<br/>-S
            &lt;seed&gt; = 	The value used to seed the random number generator<br/>	(Value
            should be >= 0 and and a long, Default = 0).<br/>-E &lt;threshold for number of
            consequetive errors&gt; = 	The consequetive number of errors allowed for
            validation<br/>	testing before the netwrok terminates.<br/>	(Value should be
            > 0, Default = 20).<br/>-G = 	GUI will be opened.<br/>	(Use this to bring
            up a GUI).<br/>-A = 	Autocreation of the network connections will NOT be
            done.<br/>	(This will be ignored if -G is NOT set)<br/>-B = 	A NominalToBinary
            filter will NOT automatically be used.<br/>	(Set this to not use a
            NominalToBinary filter).<br/>-H &lt;comma seperated numbers for nodes on each
            layer&gt; = 	The hidden layers to be created for the network.<br/>	(Value
            should be a list of comma separated Natural <br/>	numbers or the letters 'a' =
            (attribs + classes) / 2, <br/>	'i' = attribs, 'o' = classes, 't' = attribs
            .+ classes)<br/>	for wildcard values, Default = a).<br/>-C = 	Normalizing a
            numeric class will NOT be done.<br/>	(Set this to not normalize the class
            if it's numeric).<br/>-I = 	Normalizing the attributes will NOT be
            done.<br/>	(Set this to not normalize the attributes).<br/>-R = 	Reseting the
            network will NOT be allowed.<br/>	(Set this to not allow the network to
            reset).<br/>-D = 	Learning rate decay will occur.<br/>	(Set this to cause the
            learning rate to decay).
            </summary>
        </member>
        <member name="M:Ml2.Clss.MultilayerPerceptron.LearningRate(System.Double)">
            <summary>
            The amount the weights are updated.
            </summary>    
        </member>
        <member name="M:Ml2.Clss.MultilayerPerceptron.Momentum(System.Double)">
            <summary>
            Momentum applied to the weights during updating.
            </summary>    
        </member>
        <member name="M:Ml2.Clss.MultilayerPerceptron.AutoBuild(System.Boolean)">
            <summary>
            Adds and connects up hidden layers in the network.
            </summary>    
        </member>
        <member name="M:Ml2.Clss.MultilayerPerceptron.Reset(System.Boolean)">
            <summary>
            This will allow the network to reset with a lower learning rate. If the
            network diverges from the answer this will automatically reset the network
            with a lower learning rate and begin training again. This option is only
            available if the gui is not set. Note that if the network diverges but isn't
            allowed to reset it will fail the training process and return an error
            message.
            </summary>    
        </member>
        <member name="M:Ml2.Clss.MultilayerPerceptron.TrainingTime(System.Int32)">
            <summary>
            The number of epochs to train through. If the validation set is non-zero
            then it can terminate the network early
            </summary>    
        </member>
        <member name="M:Ml2.Clss.MultilayerPerceptron.ValidationSetSize(System.Int32)">
            <summary>
            The percentage size of the validation set.(The training will continue
            until it is observed that the error on the validation set has been
            consistently getting worse, or if the training time is reached). If This is set to
            zero no validation set will be used and instead the network will train for the
            specified number of epochs.
            </summary>    
        </member>
        <member name="M:Ml2.Clss.MultilayerPerceptron.ValidationThreshold(System.Int32)">
            <summary>
            Used to terminate validation testing.The value here dictates how many
            times in a row the validation set error can get worse before training is
            terminated.
            </summary>    
        </member>
        <member name="M:Ml2.Clss.MultilayerPerceptron.HiddenLayers(System.String)">
            <summary>
            This defines the hidden layers of the neural network. This is a list of
            positive whole numbers. 1 for each hidden layer. Comma seperated. To have no
            hidden layers put a single 0 here. This will only be used if autobuild is
            set. There are also wildcard values 'a' = (attribs + classes) / 2, 'i' =
            attribs, 'o' = classes , 't' = attribs + classes.
            </summary>    
        </member>
        <member name="M:Ml2.Clss.MultilayerPerceptron.GUI(System.Boolean)">
            <summary>
            
            </summary>    
        </member>
        <member name="M:Ml2.Clss.MultilayerPerceptron.NominalToBinaryFilter(System.Boolean)">
            <summary>
            This will preprocess the instances with the filter. This could help
            improve performance if there are nominal attributes in the data.
            </summary>    
        </member>
        <member name="M:Ml2.Clss.MultilayerPerceptron.NormalizeNumericClass(System.Boolean)">
            <summary>
            This will normalize the class if it's numeric. This could help improve
            performance of the network, It normalizes the class to be between -1 and 1.
            Note that this is only internally, the output will be scaled back to the
            original range.
            </summary>    
        </member>
        <member name="M:Ml2.Clss.MultilayerPerceptron.NormalizeAttributes(System.Boolean)">
            <summary>
            This will normalize the attributes. This could help improve performance
            of the network. This is not reliant on the class being numeric. This will
            also normalize nominal attributes as well (after they have been run through
            the nominal to binary filter if that is in use) so that the nominal values
            are between -1 and 1
            </summary>    
        </member>
        <member name="M:Ml2.Clss.MultilayerPerceptron.Decay(System.Boolean)">
            <summary>
            This will cause the learning rate to decrease. This will divide the
            starting learning rate by the epoch number, to determine what the current
            learning rate should be. This may help to stop the network from diverging from
            the target output, as well as improve general performance. Note that the
            decaying learning rate will not be shown in the gui, only the original learning
            rate. If the learning rate is changed in the gui, this is treated as the
            starting learning rate.
            </summary>    
        </member>
        <member name="M:Ml2.Clss.MultilayerPerceptron.Debug(System.Boolean)">
            <summary>
            If set to true, classifier may output additional info to the console.
            </summary>    
        </member>
        <member name="T:Ml2.Clss.MultiScheme">
            <summary>
            Class for selecting a classifier from among several using cross
            validation on the training data or the performance on the training data. Performance
            is measured based on percent correct (classification) or mean-squared
            error (regression).<br/><br/>Options:<br/><br/>-X &lt;number of folds&gt; =
            	Use cross validation for model selection using the<br/>	given number of
            folds. (default 0, is to<br/>	use training error)<br/>-S &lt;num&gt; = 	Random
            number seed.<br/>	(default 1)<br/>-B &lt;classifier specification&gt; =
            	Full class name of classifier to include, followed<br/>	by scheme options. May
            be specified multiple times.<br/>	(default:
            "weka.classifiers.rules.ZeroR")<br/>-D = 	If set, classifier is run in debug mode and<br/>	may output
            additional info to the console
            </summary>
        </member>
        <member name="M:Ml2.Clss.MultiScheme.NumFolds(System.Int32)">
            <summary>
            The number of folds used for cross-validation (if 0, performance on
            training data will be used).
            </summary>    
        </member>
        <member name="M:Ml2.Clss.MultiScheme.Classifiers(System.Collections.Generic.IEnumerable{Ml2.Clss.IBaseClassifier{weka.classifiers.Classifier}})">
            <summary>
            The classifiers to be chosen from.
            </summary>    
        </member>
        <member name="M:Ml2.Clss.MultiScheme.Debug(System.Boolean)">
            <summary>
            Whether debug information is output to console.
            </summary>    
        </member>
        <member name="T:Ml2.Clss.NaiveBayes">
            <summary>
            Class for a Naive Bayes classifier using estimator classes. Numeric
            estimator precision values are chosen based on analysis of the training data.
            For this reason, the classifier is not an UpdateableClassifier (which in
            typical usage are initialized with zero training instances) -- if you need the
            UpdateableClassifier functionality, use the NaiveBayesUpdateable
            classifier. The NaiveBayesUpdateable classifier will use a default precision of 0.1
            for numeric attributes when buildClassifier is called with zero training
            instances.<br/><br/>For more information on Naive Bayes classifiers,
            see<br/><br/>George H. John, Pat Langley: Estimating Continuous Distributions in
            Bayesian Classifiers. In: Eleventh Conference on Uncertainty in Artificial
            Intelligence, San Mateo, 338-345, 1995.<br/><br/>Options:<br/><br/>-K = 	Use
            kernel density estimator rather than normal<br/>	distribution for numeric
            attributes<br/>-D = 	Use supervised discretization to process numeric
            attributes<br/><br/>-O = 	Display model in old format (good when there are many
            classes)<br/>
            </summary>
        </member>
        <member name="M:Ml2.Clss.NaiveBayes.UseSupervisedDiscretization(System.Boolean)">
            <summary>
            Use supervised discretization to convert numeric attributes to nominal
            ones.
            </summary>    
        </member>
        <member name="M:Ml2.Clss.NaiveBayes.UseKernelEstimator(System.Boolean)">
            <summary>
            Use a kernel estimator for numeric attributes rather than a normal
            distribution.
            </summary>    
        </member>
        <member name="M:Ml2.Clss.NaiveBayes.DisplayModelInOldFormat(System.Boolean)">
            <summary>
            Use old format for model output. The old format is better when there are
            many class values. The new format is better when there are fewer classes
            and many attributes.
            </summary>    
        </member>
        <member name="M:Ml2.Clss.NaiveBayes.Debug(System.Boolean)">
            <summary>
            If set to true, classifier may output additional info to the console.
            </summary>    
        </member>
        <member name="T:Ml2.Clss.NaiveBayesMultinomial">
            <summary>
            Class for building and using a multinomial Naive Bayes classifier. For
            more information see,<br/><br/>Andrew Mccallum, Kamal Nigam: A Comparison of
            Event Models for Naive Bayes Text Classification. In: AAAI-98 Workshop on
            'Learning for Text Categorization', 1998.<br/><br/>The core equation for
            this classifier:<br/><br/>P[Ci|D] = (P[D|Ci] x P[Ci]) / P[D] (Bayes
            rule)<br/><br/>where Ci is class i and D is a document.<br/><br/>Options:<br/><br/>-D
            = 	If set, classifier is run in debug mode and<br/>	may output additional
            info to the console
            </summary>
        </member>
        <member name="M:Ml2.Clss.NaiveBayesMultinomial.Debug(System.Boolean)">
            <summary>
            If set to true, classifier may output additional info to the console.
            </summary>    
        </member>
        <member name="T:Ml2.Clss.NaiveBayesMultinomialText">
            <summary>
            Multinomial naive bayes for text data. Operates directly (and only) on
            String attributes. Other types of input attributes are accepted but ignored
            during training and classification<br/><br/>Options:<br/><br/>-W = 	Use word
            frequencies instead of binary bag of words.<br/>-P &lt;# instances&gt; =
            	How often to prune the dictionary of low frequency words (default = 0, i.e.
            don't prune)<br/>-M &lt;double&gt; = 	Minimum word frequency. Words with
            less than this frequence are ignored.<br/>	If periodic pruning is turned on
            then this is also used to determine which<br/>	words to remove from the
            dictionary (default = 3).<br/>-normalize = 	Normalize document length (use in
            conjunction with -norm and -lnorm)<br/>-norm &lt;num&gt; = 	Specify the
            norm that each instance must have (default 1.0)<br/>-lnorm &lt;num&gt; =
            	Specify L-norm to use (default 2.0)<br/>-lowercase = 	Convert all tokens to
            lowercase before adding to the dictionary.<br/>-stoplist = 	Ignore words that
            are in the stoplist.<br/>-stopwords &lt;file&gt; = 	A file containing
            stopwords to override the default ones.<br/>	Using this option automatically
            sets the flag ('-stoplist') to use the<br/>	stoplist if the file
            exists.<br/>	Format: one stopword per line, lines starting with '#'<br/>	are interpreted
            as comments and ignored.<br/>-tokenizer &lt;spec&gt; = 	The tokenizing
            algorihtm (classname plus parameters) to use.<br/>	(default:
            weka.core.tokenizers.WordTokenizer)<br/>-stemmer &lt;spec&gt; = 	The stemmering algorihtm
            (classname plus parameters) to use.
            </summary>
        </member>
        <member name="M:Ml2.Clss.NaiveBayesMultinomialText.UseStopList(System.Boolean)">
            <summary>
            If true, ignores all words that are on the stoplist.
            </summary>    
        </member>
        <member name="M:Ml2.Clss.NaiveBayesMultinomialText.UseWordFrequencies(System.Boolean)">
            <summary>
            Use word frequencies rather than binary bag of words representation
            </summary>    
        </member>
        <member name="M:Ml2.Clss.NaiveBayesMultinomialText.PeriodicPruning(System.Int32)">
            <summary>
            How often (number of instances) to prune the dictionary of low frequency
            terms. 0 means don't prune. Setting a positive integer n means prune after
            every n instances
            </summary>    
        </member>
        <member name="M:Ml2.Clss.NaiveBayesMultinomialText.MinWordFrequency(System.Double)">
            <summary>
            Ignore any words that don't occur at least min frequency times in the
            training data. If periodic pruning is turned on, then the dictionary is pruned
            according to this value
            </summary>    
        </member>
        <member name="M:Ml2.Clss.NaiveBayesMultinomialText.NormalizeDocLength(System.Boolean)">
            <summary>
            If true then document length is normalized according to the settings for
            norm and lnorm
            </summary>    
        </member>
        <member name="M:Ml2.Clss.NaiveBayesMultinomialText.Norm(System.Double)">
            <summary>
            The norm of the instances after normalization.
            </summary>    
        </member>
        <member name="M:Ml2.Clss.NaiveBayesMultinomialText.LNorm(System.Double)">
            <summary>
            
            </summary>    
        </member>
        <member name="M:Ml2.Clss.NaiveBayesMultinomialText.LowercaseTokens(System.Boolean)">
            <summary>
            Whether to convert all tokens to lowercase
            </summary>    
        </member>
        <member name="M:Ml2.Clss.NaiveBayesMultinomialText.Tokenizer(weka.core.tokenizers.Tokenizer)">
            <summary>
            The tokenizing algorithm to use on the strings.
            </summary>    
        </member>
        <member name="M:Ml2.Clss.NaiveBayesMultinomialText.Stemmer(weka.core.stemmers.Stemmer)">
            <summary>
            The stemming algorithm to use on the words.
            </summary>    
        </member>
        <member name="M:Ml2.Clss.NaiveBayesMultinomialText.Debug(System.Boolean)">
            <summary>
            If set to true, classifier may output additional info to the console.
            </summary>    
        </member>
        <member name="T:Ml2.Clss.NaiveBayesMultinomialUpdateable">
            <summary>
            Class for building and using a multinomial Naive Bayes classifier. For
            more information see,<br/><br/>Andrew Mccallum, Kamal Nigam: A Comparison of
            Event Models for Naive Bayes Text Classification. In: AAAI-98 Workshop on
            'Learning for Text Categorization', 1998.<br/><br/>The core equation for
            this classifier:<br/><br/>P[Ci|D] = (P[D|Ci] x P[Ci]) / P[D] (Bayes
            rule)<br/><br/>where Ci is class i and D is a document.<br/><br/>Incremental version
            of the algorithm.<br/><br/>Options:<br/><br/>-D = 	If set, classifier is
            run in debug mode and<br/>	may output additional info to the console
            </summary>
        </member>
        <member name="M:Ml2.Clss.NaiveBayesMultinomialUpdateable.Debug(System.Boolean)">
            <summary>
            If set to true, classifier may output additional info to the console.
            </summary>    
        </member>
        <member name="T:Ml2.Clss.NaiveBayesUpdateable">
            <summary>
            Class for a Naive Bayes classifier using estimator classes. This is the
            updateable version of NaiveBayes.<br/>This classifier will use a default
            precision of 0.1 for numeric attributes when buildClassifier is called with
            zero training instances.<br/><br/>For more information on Naive Bayes
            classifiers, see<br/><br/>George H. John, Pat Langley: Estimating Continuous
            Distributions in Bayesian Classifiers. In: Eleventh Conference on Uncertainty in
            Artificial Intelligence, San Mateo, 338-345,
            1995.<br/><br/>Options:<br/><br/>-K = 	Use kernel density estimator rather than normal<br/>	distribution
            for numeric attributes<br/>-D = 	Use supervised discretization to process
            numeric attributes<br/><br/>-O = 	Display model in old format (good when
            there are many classes)<br/>
            </summary>
        </member>
        <member name="M:Ml2.Clss.NaiveBayesUpdateable.UseSupervisedDiscretization(System.Boolean)">
            <summary>
            Use supervised discretization to convert numeric attributes to nominal
            ones.
            </summary>    
        </member>
        <member name="M:Ml2.Clss.NaiveBayesUpdateable.UseKernelEstimator(System.Boolean)">
            <summary>
            Use a kernel estimator for numeric attributes rather than a normal
            distribution.
            </summary>    
        </member>
        <member name="M:Ml2.Clss.NaiveBayesUpdateable.DisplayModelInOldFormat(System.Boolean)">
            <summary>
            Use old format for model output. The old format is better when there are
            many class values. The new format is better when there are fewer classes
            and many attributes.
            </summary>    
        </member>
        <member name="M:Ml2.Clss.NaiveBayesUpdateable.Debug(System.Boolean)">
            <summary>
            If set to true, classifier may output additional info to the console.
            </summary>    
        </member>
        <member name="T:Ml2.Clss.OneR">
            <summary>
            Class for building and using a 1R classifier; in other words, uses the
            minimum-error attribute for prediction, discretizing numeric attributes. For
            more information, see:<br/><br/>R.C. Holte (1993). Very simple
            classification rules perform well on most commonly used datasets. Machine Learning.
            11:63-91.<br/><br/>Options:<br/><br/>-B &lt;minimum bucket size&gt; = 	The
            minimum number of objects in a bucket (default: 6).
            </summary>
        </member>
        <member name="M:Ml2.Clss.OneR.MinBucketSize(System.Int32)">
            <summary>
            The minimum bucket size used for discretizing numeric attributes.
            </summary>    
        </member>
        <member name="M:Ml2.Clss.OneR.Debug(System.Boolean)">
            <summary>
            If set to true, classifier may output additional info to the console.
            </summary>    
        </member>
        <member name="T:Ml2.Clss.PART">
            <summary>
            Class for generating a PART decision list. Uses separate-and-conquer.
            Builds a partial C4.5 decision tree in each iteration and makes the "best"
            leaf into a rule.<br/><br/>For more information, see:<br/><br/>Eibe Frank, Ian
            H. Witten: Generating Accurate Rule Sets Without Global Optimization. In:
            Fifteenth International Conference on Machine Learning, 144-151,
            1998.<br/><br/>Options:<br/><br/>-C &lt;pruning confidence&gt; = 	Set confidence
            threshold for pruning.<br/>	(default 0.25)<br/>-M &lt;minimum number of
            objects&gt; = 	Set minimum number of objects per leaf.<br/>	(default 2)<br/>-R =
            	Use reduced error pruning.<br/>-N &lt;number of folds&gt; = 	Set number of
            folds for reduced error<br/>	pruning. One fold is used as pruning
            set.<br/>	(default 3)<br/>-B = 	Use binary splits only.<br/>-U = 	Generate unpruned
            decision list.<br/>-J = 	Do not use MDL correction for info gain on numeric
            attributes.<br/>-Q &lt;seed&gt; = 	Seed for random data shuffling (default
            1).
            </summary>
        </member>
        <member name="M:Ml2.Clss.PART.ConfidenceFactor(System.Single)">
            <summary>
            The confidence factor used for pruning (smaller values incur more
            pruning).
            </summary>    
        </member>
        <member name="M:Ml2.Clss.PART.MinNumObj(System.Int32)">
            <summary>
            The minimum number of instances per rule.
            </summary>    
        </member>
        <member name="M:Ml2.Clss.PART.ReducedErrorPruning(System.Boolean)">
            <summary>
            Whether reduced-error pruning is used instead of C.4.5 pruning.
            </summary>    
        </member>
        <member name="M:Ml2.Clss.PART.Unpruned(System.Boolean)">
            <summary>
            Whether pruning is performed.
            </summary>    
        </member>
        <member name="M:Ml2.Clss.PART.UseMDLcorrection(System.Boolean)">
            <summary>
            Whether MDL correction is used when finding splits on numeric attributes.
            </summary>    
        </member>
        <member name="M:Ml2.Clss.PART.NumFolds(System.Int32)">
            <summary>
            Determines the amount of data used for reduced-error pruning. One fold is
            used for pruning, the rest for growing the rules.
            </summary>    
        </member>
        <member name="M:Ml2.Clss.PART.BinarySplits(System.Boolean)">
            <summary>
            Whether to use binary splits on nominal attributes when building the
            partial trees.
            </summary>    
        </member>
        <member name="M:Ml2.Clss.PART.Debug(System.Boolean)">
            <summary>
            If set to true, classifier may output additional info to the console.
            </summary>    
        </member>
        <!-- Badly formed XML comment ignored for member "T:Ml2.Clss.RandomCommittee" -->
        <member name="M:Ml2.Clss.RandomCommittee.NumExecutionSlots(System.Int32)">
            <summary>
            The number of execution slots (threads) to use for constructing the
            ensemble.
            </summary>    
        </member>
        <member name="M:Ml2.Clss.RandomCommittee.NumIterations(System.Int32)">
            <summary>
            The number of iterations to be performed.
            </summary>    
        </member>
        <member name="M:Ml2.Clss.RandomCommittee.Classifier(Ml2.Clss.IBaseClassifier{weka.classifiers.Classifier})">
            <summary>
            The base classifier to be used.
            </summary>    
        </member>
        <member name="M:Ml2.Clss.RandomCommittee.Debug(System.Boolean)">
            <summary>
            If set to true, classifier may output additional info to the console.
            </summary>    
        </member>
        <!-- Badly formed XML comment ignored for member "T:Ml2.Clss.RandomForest" -->
        <member name="M:Ml2.Clss.RandomForest.MaxDepth(System.Int32)">
            <summary>
            The maximum depth of the trees, 0 for unlimited.
            </summary>    
        </member>
        <member name="M:Ml2.Clss.RandomForest.PrintTrees(System.Boolean)">
            <summary>
            Print the individual trees in the output
            </summary>    
        </member>
        <member name="M:Ml2.Clss.RandomForest.NumExecutionSlots(System.Int32)">
            <summary>
            The number of execution slots (threads) to use for constructing the
            ensemble.
            </summary>    
        </member>
        <member name="M:Ml2.Clss.RandomForest.NumTrees(System.Int32)">
            <summary>
            The number of trees to be generated.
            </summary>    
        </member>
        <member name="M:Ml2.Clss.RandomForest.NumFeatures(System.Int32)">
            <summary>
            The number of attributes to be used in random selection (see RandomTree).
            </summary>    
        </member>
        <member name="M:Ml2.Clss.RandomForest.Debug(System.Boolean)">
            <summary>
            If set to true, classifier may output additional info to the console.
            </summary>    
        </member>
        <!-- Badly formed XML comment ignored for member "T:Ml2.Clss.RandomSubSpace" -->
        <member name="M:Ml2.Clss.RandomSubSpace.SubSpaceSize(System.Double)">
            <summary>
            Size of each subSpace: if less than 1 as a percentage of the number of
            attributes, otherwise the absolute number of attributes.
            </summary>    
        </member>
        <member name="M:Ml2.Clss.RandomSubSpace.NumExecutionSlots(System.Int32)">
            <summary>
            The number of execution slots (threads) to use for constructing the
            ensemble.
            </summary>    
        </member>
        <member name="M:Ml2.Clss.RandomSubSpace.NumIterations(System.Int32)">
            <summary>
            The number of iterations to be performed.
            </summary>    
        </member>
        <member name="M:Ml2.Clss.RandomSubSpace.Classifier(Ml2.Clss.IBaseClassifier{weka.classifiers.Classifier})">
            <summary>
            The base classifier to be used.
            </summary>    
        </member>
        <member name="M:Ml2.Clss.RandomSubSpace.Debug(System.Boolean)">
            <summary>
            If set to true, classifier may output additional info to the console.
            </summary>    
        </member>
        <!-- Badly formed XML comment ignored for member "T:Ml2.Clss.RandomTree" -->
        <member name="M:Ml2.Clss.RandomTree.KValue(System.Int32)">
            <summary>
            
            </summary>    
        </member>
        <member name="M:Ml2.Clss.RandomTree.MaxDepth(System.Int32)">
            <summary>
            The maximum depth of the tree, 0 for unlimited.
            </summary>    
        </member>
        <member name="M:Ml2.Clss.RandomTree.AllowUnclassifiedInstances(System.Boolean)">
            <summary>
            Whether to allow unclassified instances.
            </summary>    
        </member>
        <member name="M:Ml2.Clss.RandomTree.MinNum(System.Double)">
            <summary>
            The minimum total weight of the instances in a leaf.
            </summary>    
        </member>
        <member name="M:Ml2.Clss.RandomTree.NumFolds(System.Int32)">
            <summary>
            Determines the amount of data used for backfitting. One fold is used for
            backfitting, the rest for growing the tree. (Default: 0, no backfitting)
            </summary>    
        </member>
        <member name="M:Ml2.Clss.RandomTree.Debug(System.Boolean)">
            <summary>
            If set to true, classifier may output additional info to the console.
            </summary>    
        </member>
        <member name="T:Ml2.Clss.RegressionByDiscretization">
            <summary>
            A regression scheme that employs any classifier on a copy of the data
            that has the class attribute discretized. The predicted value is the expected
            value of the mean class value for each discretized interval (based on the
            predicted probabilities for each interval). This class now also supports
            conditional density estimation by building a univariate density estimator from
            the target values in the training data, weighted by the class
            probabilities. <br/><br/>For more information on this process, see<br/><br/>Eibe Frank,
            Remco R. Bouckaert: Conditional Density Estimation with Class Probability
            Estimators. In: First Asian Conference on Machine Learning, Berlin, 65-81,
            2009.<br/><br/>Options:<br/><br/>-B &lt;int&gt; = 	Number of bins for
            equal-width discretization<br/>	(default 10).<br/><br/>-E = 	Whether to delete
            empty bins after discretization<br/>	(default false).<br/><br/>-A = 	Whether
            to minimize absolute error, rather than squared error.<br/>	(default
            false).<br/><br/>-F = 	Use equal-frequency instead of equal-width
            discretization.<br/>-K = 	What type of density estimator to use:
            0=histogram/1=kernel/2=normal (default: 0).<br/>-D = 	If set, classifier is run in debug mode
            and<br/>	may output additional info to the console<br/>-W = 	Full name of base
            classifier.<br/>	(default: weka.classifiers.trees.J48)<br/><br/>Options
            specific to classifier weka.classifiers.trees.J48: = <br/>-U = 	Use unpruned
            tree.<br/>-O = 	Do not collapse tree.<br/>-C &lt;pruning confidence&gt; =
            	Set confidence threshold for pruning.<br/>	(default 0.25)<br/>-M &lt;minimum
            number of instances&gt; = 	Set minimum number of instances per
            leaf.<br/>	(default 2)<br/>-R = 	Use reduced error pruning.<br/>-N &lt;number of
            folds&gt; = 	Set number of folds for reduced error<br/>	pruning. One fold is used
            as pruning set.<br/>	(default 3)<br/>-B = 	Use binary splits only.<br/>-S
            = 	Don't perform subtree raising.<br/>-L = 	Do not clean up after the tree
            has been built.<br/>-A = 	Laplace smoothing for predicted
            probabilities.<br/>-J = 	Do not use MDL correction for info gain on numeric
            attributes.<br/>-Q &lt;seed&gt; = 	Seed for random data shuffling (default 1).
            </summary>
        </member>
        <member name="M:Ml2.Clss.RegressionByDiscretization.NumBins(System.Int32)">
            <summary>
            Number of bins for discretization.
            </summary>    
        </member>
        <member name="M:Ml2.Clss.RegressionByDiscretization.DeleteEmptyBins(System.Boolean)">
            <summary>
            Whether to delete empty bins after discretization.
            </summary>    
        </member>
        <member name="M:Ml2.Clss.RegressionByDiscretization.UseEqualFrequency(System.Boolean)">
            <summary>
            If set to true, equal-frequency binning will be used instead of
            equal-width binning.
            </summary>    
        </member>
        <member name="M:Ml2.Clss.RegressionByDiscretization.MinimizeAbsoluteError(System.Boolean)">
            <summary>
            Whether to minimize absolute error.
            </summary>    
        </member>
        <member name="M:Ml2.Clss.RegressionByDiscretization.EstimatorType(Ml2.Clss.RegressionByDiscretization.EEstimatorType)">
            <summary>
            The density estimator to use.
            </summary>    
        </member>
        <member name="M:Ml2.Clss.RegressionByDiscretization.Classifier(Ml2.Clss.IBaseClassifier{weka.classifiers.Classifier})">
            <summary>
            The base classifier to be used.
            </summary>    
        </member>
        <member name="M:Ml2.Clss.RegressionByDiscretization.Debug(System.Boolean)">
            <summary>
            If set to true, classifier may output additional info to the console.
            </summary>    
        </member>
        <member name="T:Ml2.Clss.REPTree">
            <summary>
            Fast decision tree learner. Builds a decision/regression tree using
            information gain/variance and prunes it using reduced-error pruning (with
            backfitting). Only sorts values for numeric attributes once. Missing values are
            dealt with by splitting the corresponding instances into pieces (i.e. as in
            C4.5).<br/><br/>Options:<br/><br/>-M &lt;minimum number of instances&gt; =
            	Set minimum number of instances per leaf (default 2).<br/>-V &lt;minimum
            variance for split&gt; = 	Set minimum numeric class variance
            proportion<br/>	of train variance for split (default 1e-3).<br/>-N &lt;number of folds&gt;
            = 	Number of folds for reduced error pruning (default 3).<br/>-S
            &lt;seed&gt; = 	Seed for random data shuffling (default 1).<br/>-P = 	No
            pruning.<br/>-L = 	Maximum tree depth (default -1, no maximum)<br/>-I = 	Initial class
            value count (default 0)<br/>-R = 	Spread initial count over all class
            values (i.e. don't use 1 per value)
            </summary>
        </member>
        <member name="M:Ml2.Clss.REPTree.NoPruning(System.Boolean)">
            <summary>
            Whether pruning is performed.
            </summary>    
        </member>
        <member name="M:Ml2.Clss.REPTree.MinNum(System.Double)">
            <summary>
            The minimum total weight of the instances in a leaf.
            </summary>    
        </member>
        <member name="M:Ml2.Clss.REPTree.MinVarianceProp(System.Double)">
            <summary>
            The minimum proportion of the variance on all the data that needs to be
            present at a node in order for splitting to be performed in regression
            trees.
            </summary>    
        </member>
        <member name="M:Ml2.Clss.REPTree.NumFolds(System.Int32)">
            <summary>
            Determines the amount of data used for pruning. One fold is used for
            pruning, the rest for growing the rules.
            </summary>    
        </member>
        <member name="M:Ml2.Clss.REPTree.MaxDepth(System.Int32)">
            <summary>
            The maximum tree depth (-1 for no restriction).
            </summary>    
        </member>
        <member name="M:Ml2.Clss.REPTree.InitialCount(System.Double)">
            <summary>
            Initial class value count.
            </summary>    
        </member>
        <member name="M:Ml2.Clss.REPTree.SpreadInitialCount(System.Boolean)">
            <summary>
            Spread initial count across all values instead of using the count per
            value.
            </summary>    
        </member>
        <member name="M:Ml2.Clss.REPTree.Debug(System.Boolean)">
            <summary>
            If set to true, classifier may output additional info to the console.
            </summary>    
        </member>
        <member name="T:Ml2.Clss.SerializedClassifier">
            <summary>
            A wrapper around a serialized classifier model. This classifier loads a
            serialized models and uses it to make predictions.<br/><br/>Warning: since
            the serialized model doesn't get changed, cross-validation cannot bet used
            with this classifier.<br/><br/>Options:<br/><br/>-D = 	If set, classifier is
            run in debug mode and<br/>	may output additional info to the
            console<br/>-model &lt;filename&gt; = 	The file containing the serialized
            model.<br/>	(required)
            </summary>
        </member>
        <member name="M:Ml2.Clss.SerializedClassifier.Model(Ml2.Clss.IBaseClassifier{weka.classifiers.Classifier})">
            <summary>
            
            </summary>    
        </member>
        <member name="M:Ml2.Clss.SerializedClassifier.Debug(System.Boolean)">
            <summary>
            If set to true, classifier may output additional info to the console.
            </summary>    
        </member>
        <member name="T:Ml2.Clss.SGD">
            <summary>
            Implements stochastic gradient descent for learning various linear models
            (binary class SVM, binary class logistic regression and linear
            regression). Globally replaces all missing values and transforms nominal attributes
            into binary ones. It also normalizes all attributes, so the coefficients in
            the output are based on the normalized data.<br/>For numeric class
            attributes, the squared loss function (2) must be
            used.<br/><br/>Options:<br/><br/>-F = 	Set the loss function to minimize. 0 = hinge loss (SVM), 1 = log loss
            (logistic regression),<br/>	2 = squared loss (regression).<br/>	(default =
            0)<br/>-L = 	The learning rate. If normalization is<br/>	turned off (as it
            is automatically for streaming data), then the<br/>	default learning rate
            will need to be reduced (try 0.0001).<br/>	(default = 0.01).<br/>-R
            &lt;double&gt; = 	The lambda regularization constant (default = 0.0001)<br/>-E
            &lt;integer&gt; = 	The number of epochs to perform (batch learning only,
            default = 500)<br/>-N = 	Don't normalize the data<br/>-M = 	Don't replace missing
            values
            </summary>
        </member>
        <member name="M:Ml2.Clss.SGD.LossFunction(Ml2.Clss.SGD.ELossFunction)">
            <summary>
            The loss function to use. Hinge loss (SVM), log loss (logistic
            regression) or squared loss (regression).
            </summary>    
        </member>
        <member name="M:Ml2.Clss.SGD.Lambda(System.Double)">
            <summary>
            The regularization constant. (default = 0.0001)
            </summary>    
        </member>
        <member name="M:Ml2.Clss.SGD.LearningRate(System.Double)">
            <summary>
            The learning rate. If normalization is turned off (as it is automatically
            for streaming data), thenthe default learning rate will need to be reduced
            (try 0.0001).
            </summary>    
        </member>
        <member name="M:Ml2.Clss.SGD.Epochs(System.Int32)">
            <summary>
            The number of epochs to perform (batch learning). The total number of
            iterations is epochs * num instances.
            </summary>    
        </member>
        <member name="M:Ml2.Clss.SGD.DontNormalize(System.Boolean)">
            <summary>
            Turn normalization off
            </summary>    
        </member>
        <member name="M:Ml2.Clss.SGD.DontReplaceMissing(System.Boolean)">
            <summary>
            Turn off global replacement of missing values
            </summary>    
        </member>
        <member name="M:Ml2.Clss.SGD.Debug(System.Boolean)">
            <summary>
            If set to true, classifier may output additional info to the console.
            </summary>    
        </member>
        <member name="T:Ml2.Clss.SGDText">
            <summary>
            Implements stochastic gradient descent for learning a linear binary class
            SVM or binary class logistic regression on text data. Operates directly
            (and only) on String attributes. Other types of input attributes are accepted
            but ignored during training and
            classification.<br/><br/>Options:<br/><br/>-F = 	Set the loss function to minimize. 0 = hinge loss (SVM), 1 = log
            loss (logistic regression)<br/>	(default = 0)<br/>-outputProbs = 	Output
            probabilities for SVMs (fits a logsitic<br/>	model to the output of the
            SVM)<br/>-L = 	The learning rate (default = 0.01).<br/>-R &lt;double&gt; = 	The
            lambda regularization constant (default = 0.0001)<br/>-E &lt;integer&gt; =
            	The number of epochs to perform (batch learning only, default = 500)<br/>-W =
            	Use word frequencies instead of binary bag of words.<br/>-P &lt;#
            instances&gt; = 	How often to prune the dictionary of low frequency words (default
            = 0, i.e. don't prune)<br/>-M &lt;double&gt; = 	Minimum word frequency.
            Words with less than this frequence are ignored.<br/>	If periodic pruning is
            turned on then this is also used to determine which<br/>	words to remove
            from the dictionary (default = 3).<br/>-normalize = 	Normalize document
            length (use in conjunction with -norm and -lnorm)<br/>-norm &lt;num&gt; =
            	Specify the norm that each instance must have (default 1.0)<br/>-lnorm
            &lt;num&gt; = 	Specify L-norm to use (default 2.0)<br/>-lowercase = 	Convert all
            tokens to lowercase before adding to the dictionary.<br/>-stoplist = 	Ignore
            words that are in the stoplist.<br/>-stopwords &lt;file&gt; = 	A file
            containing stopwords to override the default ones.<br/>	Using this option
            automatically sets the flag ('-stoplist') to use the<br/>	stoplist if the file
            exists.<br/>	Format: one stopword per line, lines starting with '#'<br/>	are
            interpreted as comments and ignored.<br/>-tokenizer &lt;spec&gt; = 	The
            tokenizing algorihtm (classname plus parameters) to use.<br/>	(default:
            weka.core.tokenizers.WordTokenizer)<br/>-stemmer &lt;spec&gt; = 	The stemmering
            algorihtm (classname plus parameters) to use.
            </summary>
        </member>
        <member name="M:Ml2.Clss.SGDText.UseStopList(System.Boolean)">
            <summary>
            If true, ignores all words that are on the stoplist.
            </summary>    
        </member>
        <member name="M:Ml2.Clss.SGDText.LossFunction(Ml2.Clss.SGDText.ELossFunction)">
            <summary>
            The loss function to use. Hinge loss (SVM), log loss (logistic
            regression) or squared loss (regression).
            </summary>    
        </member>
        <member name="M:Ml2.Clss.SGDText.OutputProbsForSVM(System.Boolean)">
            <summary>
            Fit a logistic regression to the output of SVM for producing probability
            estimates
            </summary>    
        </member>
        <member name="M:Ml2.Clss.SGDText.Lambda(System.Double)">
            <summary>
            The regularization constant. (default = 0.0001)
            </summary>    
        </member>
        <member name="M:Ml2.Clss.SGDText.LearningRate(System.Double)">
            <summary>
            The learning rate.
            </summary>    
        </member>
        <member name="M:Ml2.Clss.SGDText.Epochs(System.Int32)">
            <summary>
            The number of epochs to perform (batch learning). The total number of
            iterations is epochs * num instances.
            </summary>    
        </member>
        <member name="M:Ml2.Clss.SGDText.UseWordFrequencies(System.Boolean)">
            <summary>
            Use word frequencies rather than binary bag of words representation
            </summary>    
        </member>
        <member name="M:Ml2.Clss.SGDText.PeriodicPruning(System.Int32)">
            <summary>
            How often (number of instances) to prune the dictionary of low frequency
            terms. 0 means don't prune. Setting a positive integer n means prune after
            every n instances
            </summary>    
        </member>
        <member name="M:Ml2.Clss.SGDText.MinWordFrequency(System.Double)">
            <summary>
            Ignore any words that don't occur at least min frequency times in the
            training data. If periodic pruning is turned on, then the dictionary is pruned
            according to this value
            </summary>    
        </member>
        <member name="M:Ml2.Clss.SGDText.NormalizeDocLength(System.Boolean)">
            <summary>
            If true then document length is normalized according to the settings for
            norm and lnorm
            </summary>    
        </member>
        <member name="M:Ml2.Clss.SGDText.Norm(System.Double)">
            <summary>
            The norm of the instances after normalization.
            </summary>    
        </member>
        <member name="M:Ml2.Clss.SGDText.LNorm(System.Double)">
            <summary>
            
            </summary>    
        </member>
        <member name="M:Ml2.Clss.SGDText.LowercaseTokens(System.Boolean)">
            <summary>
            Whether to convert all tokens to lowercase
            </summary>    
        </member>
        <member name="M:Ml2.Clss.SGDText.Tokenizer(weka.core.tokenizers.Tokenizer)">
            <summary>
            The tokenizing algorithm to use on the strings.
            </summary>    
        </member>
        <member name="M:Ml2.Clss.SGDText.Stemmer(weka.core.stemmers.Stemmer)">
            <summary>
            The stemming algorithm to use on the words.
            </summary>    
        </member>
        <member name="M:Ml2.Clss.SGDText.Bias(System.Double)">
            <summary>
            
            </summary>    
        </member>
        <member name="M:Ml2.Clss.SGDText.Debug(System.Boolean)">
            <summary>
            If set to true, classifier may output additional info to the console.
            </summary>    
        </member>
        <member name="T:Ml2.Clss.SimpleLinearRegression">
            <summary>
            Learns a simple linear regression model. Picks the attribute that results
            in the lowest squared error. Missing values are not allowed. Can only deal
            with numeric attributes.<br/><br/>Options:<br/><br/>-D = 	If set,
            classifier is run in debug mode and<br/>	may output additional info to the console
            </summary>
        </member>
        <member name="M:Ml2.Clss.SimpleLinearRegression.SuppressErrorMessage(System.Boolean)">
            <summary>
            
            </summary>    
        </member>
        <member name="M:Ml2.Clss.SimpleLinearRegression.Debug(System.Boolean)">
            <summary>
            If set to true, classifier may output additional info to the console.
            </summary>    
        </member>
        <member name="T:Ml2.Clss.SimpleLogistic">
            <summary>
            Classifier for building linear logistic regression models. LogitBoost
            with simple regression functions as base learners is used for fitting the
            logistic models. The optimal number of LogitBoost iterations to perform is
            cross-validated, which leads to automatic attribute selection. For more
            information see:<br/>Niels Landwehr, Mark Hall, Eibe Frank (2005). Logistic Model
            Trees.<br/><br/>Marc Sumner, Eibe Frank, Mark Hall: Speeding up Logistic
            Model Tree Induction. In: 9th European Conference on Principles and Practice
            of Knowledge Discovery in Databases, 675-683,
            2005.<br/><br/>Options:<br/><br/>-I &lt;iterations&gt; = 	Set fixed number of iterations for
            LogitBoost<br/>-S = 	Use stopping criterion on training set (instead
            of<br/>	cross-validation)<br/>-P = 	Use error on probabilities (rmse) instead
            of<br/>	misclassification error for stopping criterion<br/>-M &lt;iterations&gt; = 	Set
            maximum number of boosting iterations<br/>-H &lt;iterations&gt; = 	Set
            parameter for heuristic for early stopping of<br/>	LogitBoost.<br/>	If enabled,
            the minimum is selected greedily, stopping<br/>	if the current minimum has
            not changed for iter iterations.<br/>	By default, heuristic is enabled with
            value 50. Set to<br/>	zero to disable heuristic.<br/>-W &lt;beta&gt; =
            	Set beta for weight trimming for LogitBoost. Set to 0 for no weight
            trimming.<br/><br/>-A = 	The AIC is used to choose the best iteration (instead of CV
            or training error).<br/>
            </summary>
        </member>
        <!-- Badly formed XML comment ignored for member "M:Ml2.Clss.SimpleLogistic.NumBoostingIterations(System.Int32)" -->
        <member name="M:Ml2.Clss.SimpleLogistic.UseCrossValidation(System.Boolean)">
            <summary>
            Sets whether the number of LogitBoost iterations is to be cross-validated
            or the stopping criterion on the training set should be used. If not set
            (and no fixed number of iterations was given), the number of LogitBoost
            iterations is used that minimizes the error on the training set
            (misclassification error or error on probabilities depending on errorOnProbabilities).
            </summary>    
        </member>
        <member name="M:Ml2.Clss.SimpleLogistic.ErrorOnProbabilities(System.Boolean)">
            <summary>
            Use error on the probabilties as error measure when determining the best
            number of LogitBoost iterations. If set, the number of LogitBoost
            iterations is chosen that minimizes the root mean squared error (either on the
            training set or in the cross-validation, depending on useCrossValidation).
            </summary>    
        </member>
        <member name="M:Ml2.Clss.SimpleLogistic.MaxBoostingIterations(System.Int32)">
            <summary>
            Sets the maximum number of iterations for LogitBoost. Default value is
            500, for very small/large datasets a lower/higher value might be preferable.
            </summary>    
        </member>
        <member name="M:Ml2.Clss.SimpleLogistic.HeuristicStop(System.Int32)">
            <summary>
            If heuristicStop > 0, the heuristic for greedy stopping while
            cross-validating the number of LogitBoost iterations is enabled. This means LogitBoost
            is stopped if no new error minimum has been reached in the last
            heuristicStop iterations. It is recommended to use this heuristic, it gives a large
            speed-up especially on small datasets. The default value is 50.
            </summary>    
        </member>
        <member name="M:Ml2.Clss.SimpleLogistic.WeightTrimBeta(System.Double)">
            <summary>
            Set the beta value used for weight trimming in LogitBoost. Only instances
            carrying (1 - beta)% of the weight from previous iteration are used in the
            next iteration. Set to 0 for no weight trimming. The default value is 0.
            </summary>    
        </member>
        <member name="M:Ml2.Clss.SimpleLogistic.UseAIC(System.Boolean)">
            <summary>
            The AIC is used to determine when to stop LogitBoost iterations (instead
            of cross-validation or training error).
            </summary>    
        </member>
        <member name="M:Ml2.Clss.SimpleLogistic.Debug(System.Boolean)">
            <summary>
            If set to true, classifier may output additional info to the console.
            </summary>    
        </member>
        <member name="T:Ml2.Clss.SMO">
            <summary>
            Implements John Platt's sequential minimal optimization algorithm for
            training a support vector classifier.<br/><br/>This implementation globally
            replaces all missing values and transforms nominal attributes into binary
            ones. It also normalizes all attributes by default. (In that case the
            coefficients in the output are based on the normalized data, not the original data
            --- this is important for interpreting the
            classifier.)<br/><br/>Multi-class problems are solved using pairwise classification (1-vs-1 and if logistic
            models are built pairwise coupling according to Hastie and Tibshirani,
            1998).<br/><br/>To obtain proper probability estimates, use the option that
            fits logistic regression models to the outputs of the support vector machine.
            In the multi-class case the predicted probabilities are coupled using
            Hastie and Tibshirani's pairwise coupling method.<br/><br/>Note: for improved
            speed normalization should be turned off when operating on
            SparseInstances.<br/><br/>For more information on the SMO algorithm, see<br/><br/>J. Platt:
            Fast Training of Support Vector Machines using Sequential Minimal
            Optimization. In B. Schoelkopf and C. Burges and A. Smola, editors, Advances in
            Kernel Methods - Support Vector Learning, 1998.<br/><br/>S.S. Keerthi, S.K.
            Shevade, C. Bhattacharyya, K.R.K. Murthy (2001). Improvements to Platt's SMO
            Algorithm for SVM Classifier Design. Neural Computation.
            13(3):637-649.<br/><br/>Trevor Hastie, Robert Tibshirani: Classification by Pairwise Coupling.
            In: Advances in Neural Information Processing Systems,
            1998.<br/><br/>Options:<br/><br/>-D = 	If set, classifier is run in debug mode and<br/>	may
            output additional info to the console<br/>-no-checks = 	Turns off all checks
            - use with caution!<br/>	Turning them off assumes that data is purely
            numeric, doesn't<br/>	contain any missing values, and has a nominal class.
            Turning them<br/>	off also means that no header information will be stored if
            the<br/>	machine is linear. Finally, it also assumes that no instance
            has<br/>	a weight equal to 0.<br/>	(default: checks on)<br/>-C &lt;double&gt; =
            	The complexity constant C. (default 1)<br/>-N = 	Whether to
            0=normalize/1=standardize/2=neither. (default 0=normalize)<br/>-L &lt;double&gt; = 	The
            tolerance parameter. (default 1.0e-3)<br/>-P &lt;double&gt; = 	The epsilon for
            round-off error. (default 1.0e-12)<br/>-M = 	Fit logistic models to SVM
            outputs. <br/>-V &lt;double&gt; = 	The number of folds for the
            internal<br/>	cross-validation. (default -1, use training data)<br/>-W &lt;double&gt; =
            	The random number seed. (default 1)<br/>-K &lt;classname and parameters&gt;
            = 	The Kernel to use.<br/>	(default:
            weka.classifiers.functions.supportVector.PolyKernel)<br/><br/>Options specific to kernel
            weka.classifiers.functions.supportVector.PolyKernel: = <br/>-D = 	Enables debugging output (if
            available) to be printed.<br/>	(default: off)<br/>-no-checks = 	Turns off all
            checks - use with caution!<br/>	(default: checks on)<br/>-C &lt;num&gt; =
            	The size of the cache (a prime number), 0 for full cache and <br/>	-1 to
            turn it off.<br/>	(default: 250007)<br/>-E &lt;num&gt; = 	The Exponent to
            use.<br/>	(default: 1.0)<br/>-L = 	Use lower-order terms.<br/>	(default: no)
            </summary>
        </member>
        <member name="M:Ml2.Clss.SMO.ChecksTurnedOff(System.Boolean)">
            <summary>
            Turns time-consuming checks off - use with caution.
            </summary>    
        </member>
        <member name="M:Ml2.Clss.SMO.C(System.Double)">
            <summary>
            The complexity parameter C.
            </summary>    
        </member>
        <member name="M:Ml2.Clss.SMO.ToleranceParameter(System.Double)">
            <summary>
            The tolerance parameter (shouldn't be changed).
            </summary>    
        </member>
        <member name="M:Ml2.Clss.SMO.Epsilon(System.Double)">
            <summary>
            The epsilon for round-off error (shouldn't be changed).
            </summary>    
        </member>
        <member name="M:Ml2.Clss.SMO.FilterType(Ml2.Clss.SMO.EFilterType)">
            <summary>
            Determines how/if the data will be transformed.
            </summary>    
        </member>
        <member name="M:Ml2.Clss.SMO.BuildLogisticModels(System.Boolean)">
            <summary>
            Whether to fit logistic models to the outputs (for proper probability
            estimates).
            </summary>    
        </member>
        <member name="M:Ml2.Clss.SMO.NumFolds(System.Int32)">
            <summary>
            The number of folds for cross-validation used to generate training data
            for logistic models (-1 means use training data).
            </summary>    
        </member>
        <member name="M:Ml2.Clss.SMO.Kernel(weka.classifiers.functions.supportVector.Kernel)">
            <summary>
            The kernel to use.
            </summary>    
        </member>
        <member name="M:Ml2.Clss.SMO.Debug(System.Boolean)">
            <summary>
            If set to true, classifier may output additional info to the console.
            </summary>    
        </member>
        <member name="T:Ml2.Clss.SMOreg">
            <summary>
            SMOreg implements the support vector machine for regression. The
            parameters can be learned using various algorithms. The algorithm is selected by
            setting the RegOptimizer. The most popular algorithm (RegSMOImproved) is due
            to Shevade, Keerthi et al and this is the default
            RegOptimizer.<br/><br/>For more information see:<br/><br/>S.K. Shevade, S.S. Keerthi, C.
            Bhattacharyya, K.R.K. Murthy: Improvements to the SMO Algorithm for SVM Regression.
            In: IEEE Transactions on Neural Networks, 1999.<br/><br/>A.J. Smola, B.
            Schoelkopf (1998). A tutorial on support vector
            regression.<br/><br/>Options:<br/><br/>-C &lt;double&gt; = 	The complexity constant C.<br/>	(default
            1)<br/>-N = 	Whether to 0=normalize/1=standardize/2=neither.<br/>	(default
            0=normalize)<br/>-I &lt;classname and parameters&gt; = 	Optimizer class used for
            solving quadratic optimization problem<br/>	(default
            weka.classifiers.functions.supportVector.RegSMOImproved)<br/>-K &lt;classname and parameters&gt;
            = 	The Kernel to use.<br/>	(default:
            weka.classifiers.functions.supportVector.PolyKernel)<br/><br/>Options specific to optimizer ('-I')
            weka.classifiers.functions.supportVector.RegSMOImproved: = <br/>-T &lt;double&gt; = 	The
            tolerance parameter for checking the stopping criterion.<br/>	(default
            0.001)<br/>-V = 	Use variant 1 of the algorithm when true, otherwise use
            variant 2.<br/>	(default true)<br/>-P &lt;double&gt; = 	The epsilon for round-off
            error.<br/>	(default 1.0e-12)<br/>-L &lt;double&gt; = 	The epsilon
            parameter in epsilon-insensitive loss function.<br/>	(default 1.0e-3)<br/>-W
            &lt;double&gt; = 	The random number seed.<br/>	(default 1)<br/><br/>Options
            specific to kernel ('-K') weka.classifiers.functions.supportVector.PolyKernel:
            = <br/>-D = 	Enables debugging output (if available) to be
            printed.<br/>	(default: off)<br/>-no-checks = 	Turns off all checks - use with
            caution!<br/>	(default: checks on)<br/>-C &lt;num&gt; = 	The size of the cache (a prime
            number), 0 for full cache and <br/>	-1 to turn it off.<br/>	(default:
            250007)<br/>-E &lt;num&gt; = 	The Exponent to use.<br/>	(default: 1.0)<br/>-L =
            	Use lower-order terms.<br/>	(default: no)
            </summary>
        </member>
        <member name="M:Ml2.Clss.SMOreg.C(System.Double)">
            <summary>
            The complexity parameter C.
            </summary>    
        </member>
        <member name="M:Ml2.Clss.SMOreg.FilterType(Ml2.Clss.SMOreg.EFilterType)">
            <summary>
            Determines how/if the data will be transformed.
            </summary>    
        </member>
        <member name="M:Ml2.Clss.SMOreg.RegOptimizer(weka.classifiers.functions.supportVector.RegOptimizer)">
            <summary>
            The learning algorithm.
            </summary>    
        </member>
        <member name="M:Ml2.Clss.SMOreg.Kernel(weka.classifiers.functions.supportVector.Kernel)">
            <summary>
            The kernel to use.
            </summary>    
        </member>
        <member name="M:Ml2.Clss.SMOreg.Debug(System.Boolean)">
            <summary>
            If set to true, classifier may output additional info to the console.
            </summary>    
        </member>
        <member name="T:Ml2.Clss.Stacking">
            <summary>
            Combines several classifiers using the stacking method. Can do
            classification or regression.<br/><br/>For more information, see<br/><br/>David H.
            Wolpert (1992). Stacked generalization. Neural Networks.
            5:241-259.<br/><br/>Options:<br/><br/>-M &lt;scheme specification&gt; = 	Full name of meta
            classifier, followed by options.<br/>	(default:
            "weka.classifiers.rules.Zero")<br/>-X &lt;number of folds&gt; = 	Sets the number of cross-validation
            folds.<br/>-S &lt;num&gt; = 	Random number seed.<br/>	(default 1)<br/>-num-slots
            &lt;num&gt; = 	Number of execution slots.<br/>	(default 1 - i.e. no
            parallelism)<br/>-B &lt;classifier specification&gt; = 	Full class name of
            classifier to include, followed<br/>	by scheme options. May be specified multiple
            times.<br/>	(default: "weka.classifiers.rules.ZeroR")<br/>-D = 	If set,
            classifier is run in debug mode and<br/>	may output additional info to the
            console
            </summary>
        </member>
        <member name="M:Ml2.Clss.Stacking.NumFolds(System.Int32)">
            <summary>
            The number of folds used for cross-validation.
            </summary>    
        </member>
        <member name="M:Ml2.Clss.Stacking.MetaClassifier(Ml2.Clss.IBaseClassifier{weka.classifiers.Classifier})">
            <summary>
            The meta classifiers to be used.
            </summary>    
        </member>
        <member name="M:Ml2.Clss.Stacking.NumExecutionSlots(System.Int32)">
            <summary>
            The number of execution slots (threads) to use for constructing the
            ensemble.
            </summary>    
        </member>
        <member name="M:Ml2.Clss.Stacking.Classifiers(System.Collections.Generic.IEnumerable{Ml2.Clss.IBaseClassifier{weka.classifiers.Classifier}})">
            <summary>
            The base classifiers to be used.
            </summary>    
        </member>
        <member name="M:Ml2.Clss.Stacking.Debug(System.Boolean)">
            <summary>
            If set to true, classifier may output additional info to the console.
            </summary>    
        </member>
        <member name="T:Ml2.Clss.Vote">
            <summary>
            Class for combining classifiers. Different combinations of probability
            estimates for classification are available.<br/><br/>For more information
            see:<br/><br/>Ludmila I. Kuncheva (2004). Combining Pattern Classifiers:
            Methods and Algorithms. John Wiley and Sons, Inc..<br/><br/>J. Kittler, M.
            Hatef, Robert P.W. Duin, J. Matas (1998). On combining classifiers. IEEE
            Transactions on Pattern Analysis and Machine Intelligence.
            20(3):226-239.<br/><br/>Options:<br/><br/>-S &lt;num&gt; = 	Random number seed.<br/>	(default
            1)<br/>-B &lt;classifier specification&gt; = 	Full class name of classifier to
            include, followed<br/>	by scheme options. May be specified multiple
            times.<br/>	(default: "weka.classifiers.rules.ZeroR")<br/>-D = 	If set, classifier
            is run in debug mode and<br/>	may output additional info to the
            console<br/>-P &lt;path to serialized classifier&gt; = 	Full path to serialized
            classifier to include.<br/>	May be specified multiple times to
            include<br/>	multiple serialized classifiers. Note: it does<br/>	not make sense to use
            pre-built classifiers in<br/>	a cross-validation.<br/>-R
            &lt;AVG|PROD|MAJ|MIN|MAX|MED&gt; = 	The combination rule to use<br/>	(default: AVG)
            </summary>
        </member>
        <member name="M:Ml2.Clss.Vote.CombinationRule(Ml2.Clss.Vote.ECombinationRule)">
            <summary>
            The combination rule used.
            </summary>    
        </member>
        <member name="M:Ml2.Clss.Vote.Environment(weka.core.Environment)">
            <summary>
            
            </summary>    
        </member>
        <member name="M:Ml2.Clss.Vote.Classifiers(System.Collections.Generic.IEnumerable{Ml2.Clss.IBaseClassifier{weka.classifiers.Classifier}})">
            <summary>
            The base classifiers to be used.
            </summary>    
        </member>
        <member name="M:Ml2.Clss.Vote.Debug(System.Boolean)">
            <summary>
            If set to true, classifier may output additional info to the console.
            </summary>    
        </member>
        <member name="T:Ml2.Clss.VotedPerceptron">
            <summary>
            Implementation of the voted perceptron algorithm by Freund and Schapire.
            Globally replaces all missing values, and transforms nominal attributes
            into binary ones.<br/><br/>For more information, see:<br/><br/>Y. Freund, R.
            E. Schapire: Large margin classification using the perceptron algorithm. In:
            11th Annual Conference on Computational Learning Theory, New York, NY,
            209-217, 1998.<br/><br/>Options:<br/><br/>-I &lt;int&gt; = 	The number of
            iterations to be performed.<br/>	(default 1)<br/>-E &lt;double&gt; = 	The
            exponent for the polynomial kernel.<br/>	(default 1)<br/>-S &lt;int&gt; = 	The
            seed for the random number generation.<br/>	(default 1)<br/>-M &lt;int&gt; =
            	The maximum number of alterations allowed.<br/>	(default 10000)
            </summary>
        </member>
        <member name="M:Ml2.Clss.VotedPerceptron.MaxK(System.Int32)">
            <summary>
            The maximum number of alterations to the perceptron.
            </summary>    
        </member>
        <member name="M:Ml2.Clss.VotedPerceptron.NumIterations(System.Int32)">
            <summary>
            Number of iterations to be performed.
            </summary>    
        </member>
        <member name="M:Ml2.Clss.VotedPerceptron.Exponent(System.Double)">
            <summary>
            Exponent for the polynomial kernel.
            </summary>    
        </member>
        <member name="M:Ml2.Clss.VotedPerceptron.Debug(System.Boolean)">
            <summary>
            If set to true, classifier may output additional info to the console.
            </summary>    
        </member>
        <member name="T:Ml2.Clss.ZeroR">
            <summary>
            Class for building and using a 0-R classifier. Predicts the mean (for a
            numeric class) or the mode (for a nominal
            class).<br/><br/>Options:<br/><br/>-D = 	If set, classifier is run in debug mode and<br/>	may output
            additional info to the console
            </summary>
        </member>
        <member name="M:Ml2.Clss.ZeroR.Debug(System.Boolean)">
            <summary>
            If set to true, classifier may output additional info to the console.
            </summary>    
        </member>
        <member name="P:Ml2.Clstr.Clusterers.Cobweb">
            <summary>
            Class implementing the Cobweb and Classit clustering
            algorithms.<br/><br/>Note: the application of node operators (merging, splitting etc.) in terms
            of ordering and priority differs (and is somewhat ambiguous) between the
            original Cobweb and Classit papers. This algorithm always compares the best
            host, adding a new leaf, merging the two best hosts, and splitting the best
            host when considering where to place a new instance.<br/><br/>For more
            information see:<br/><br/>D. Fisher (1987). Knowledge acquisition via
            incremental conceptual clustering. Machine Learning. 2(2):139-172.<br/><br/>J. H.
            Gennari, P. Langley, D. Fisher (1990). Models of incremental concept
            formation. Artificial Intelligence. 40:11-61.<br/><br/>Options:<br/><br/>-A
            &lt;acuity&gt; = 	Acuity.<br/>	(default=1.0)<br/>-C &lt;cutoff&gt; =
            	Cutoff.<br/>	(default=0.002)<br/>-S &lt;num&gt; = 	Random number seed.<br/>	(default
            42)
            </summary>
        </member>
        <member name="P:Ml2.Clstr.Clusterers.EM">
            <summary>
            Simple EM (expectation maximisation) class.<br/><br/>EM assigns a
            probability distribution to each instance which indicates the probability of it
            belonging to each of the clusters. EM can decide how many clusters to create
            by cross validation, or you may specify apriori how many clusters to
            generate.<br/><br/>The cross validation performed to determine the number of
            clusters is done in the following steps:<br/>1. the number of clusters is set
            to 1<br/>2. the training set is split randomly into 10 folds.<br/>3. EM is
            performed 10 times using the 10 folds the usual CV way.<br/>4. the
            loglikelihood is averaged over all 10 results.<br/>5. if loglikelihood has increased
            the number of clusters is increased by 1 and the program continues at step
            2. <br/><br/>The number of folds is fixed to 10, as long as the number of
            instances in the training set is not smaller 10. If this is the case the
            number of folds is set equal to the number of
            instances.<br/><br/>Options:<br/><br/>-N &lt;num&gt; = 	number of clusters. If omitted or -1 specified,
            then <br/>	cross validation is used to select the number of clusters.<br/>-X
            &lt;num&gt; = 	Number of folds to use when cross-validating to find the
            best number of clusters.<br/>-max &lt;num&gt; = 	Maximum number of clusters to
            consider during cross-validation. If omitted or -1 specified, then
            <br/>	there is no upper limit on the number of clusters.<br/>-ll-cv &lt;num&gt; =
            	Minimum improvement in cross-validated log likelihood required<br/>	to
            consider increasing the number of clusters.<br/>	(default 1e-6)<br/>-I
            &lt;num&gt; = 	max iterations.<br/>	(default 100)<br/>-ll-iter &lt;num&gt; =
            	Minimum improvement in log likelihood required<br/>	to perform another
            iteration of the E and M steps.<br/>	(default 1e-6)<br/>-V = 	verbose.<br/>-M
            &lt;num&gt; = 	minimum allowable standard deviation for normal
            density<br/>	computation<br/>	(default 1e-6)<br/>-O = 	Display model in old format (good
            when there are many clusters)<br/><br/>-num-slots &lt;num&gt; = 	Number of
            execution slots.<br/>	(default 1 - i.e. no parallelism)<br/>-S &lt;num&gt; =
            	Random number seed.<br/>	(default 100)
            </summary>
        </member>
        <member name="P:Ml2.Clstr.Clusterers.FarthestFirst">
            <summary>
            Cluster data using the FarthestFirst algorithm.<br/><br/>For more
            information see:<br/><br/>Hochbaum, Shmoys (1985). A best possible heuristic for
            the k-center problem. Mathematics of Operations Research.
            10(2):180-184.<br/><br/>Sanjoy Dasgupta: Performance Guarantees for Hierarchical Clustering.
            In: 15th Annual Conference on Computational Learning Theory, 351-363,
            2002.<br/><br/>Notes:<br/>- works as a fast simple approximate clusterer<br/>-
            modelled after SimpleKMeans, might be a useful initializer for
            it<br/><br/>Options:<br/><br/>-N &lt;num&gt; = 	number of clusters. (default =
            2).<br/>-S &lt;num&gt; = 	Random number seed.<br/>	(default 1)
            </summary>
        </member>
        <member name="P:Ml2.Clstr.Clusterers.Filtered">
            <summary>
            Class for running an arbitrary clusterer on data that has been passed
            through an arbitrary filter. Like the clusterer, the structure of the filter
            is based exclusively on the training data and test instances will be
            processed by the filter without changing their
            structure.<br/><br/>Options:<br/><br/>-F &lt;filter specification&gt; = 	Full class name of filter to use,
            followed<br/>	by filter options.<br/>	eg:
            "weka.filters.unsupervised.attribute.Remove -V -R 1,2"<br/>(default: weka.filters.AllFilter)<br/>-W = 	Full
            name of base clusterer.<br/>	(default:
            weka.clusterers.SimpleKMeans)<br/><br/>Options specific to clusterer weka.clusterers.SimpleKMeans: = <br/>-N
            &lt;num&gt; = 	number of clusters.<br/>	(default 2).<br/>-P = 	Initialize using
            the k-means++ method.<br/><br/>-V = 	Display std. deviations for
            centroids.<br/><br/>-M = 	Replace missing values with mean/mode.<br/><br/>-A
            &lt;classname and options&gt; = 	Distance function to use.<br/>	(default:
            weka.core.EuclideanDistance)<br/>-I &lt;num&gt; = 	Maximum number of
            iterations.<br/><br/>-O = 	Preserve order of instances.<br/><br/>-fast = 	Enables faster
            distance calculations, using cut-off values.<br/>	Disables the
            calculation/output of squared errors/distances.<br/><br/>-num-slots &lt;num&gt; =
            	Number of execution slots.<br/>	(default 1 - i.e. no parallelism)<br/>-S
            &lt;num&gt; = 	Random number seed.<br/>	(default 10)
            </summary>
        </member>
        <member name="P:Ml2.Clstr.Clusterers.Hierarchical">
            <summary>
            Hierarchical clustering class.<br/>Implements a number of classic
            agglomorative (i.e. bottom up) hierarchical clustering methodsbased on
            .<br/><br/>Options:<br/><br/>-D = 	If set, classifier is run in debug mode
            and<br/>	may output additional info to the console<br/>-B = 	If set, distance is
            interpreted as branch length<br/>	otherwise it is node height.<br/>-N &lt;Nr Of
            Clusters&gt; = 	number of clusters<br/>-P = 	Flag to indicate the cluster
            should be printed in Newick format.<br/>-L
            [SINGLE|COMPLETE|AVERAGE|MEAN|CENTROID|WARD|ADJCOMLPETE|NEIGHBOR_JOINING] = Link type (Single, Complete,
            Average, Mean, Centroid, Ward, Adjusted complete, Neighbor joining)<br/>-A
            &lt;classname and options&gt; = 	Distance function to use.<br/>	(default:
            weka.core.EuclideanDistance)
            </summary>
        </member>
        <member name="P:Ml2.Clstr.Clusterers.MakeDensityBased">
            <summary>
            Class for wrapping a Clusterer to make it return a distribution and
            density. Fits normal distributions and discrete distributions within each
            cluster produced by the wrapped clusterer. Supports the
            NumberOfClustersRequestable interface only if the wrapped Clusterer
            does.<br/><br/>Options:<br/><br/>-M &lt;num&gt; = 	minimum allowable standard deviation for normal density
            computation <br/>	(default 1e-6)<br/>-W &lt;clusterer name&gt; = 	Clusterer
            to wrap.<br/>	(default weka.clusterers.SimpleKMeans)<br/><br/>Options
            specific to clusterer weka.clusterers.SimpleKMeans: = <br/>-N &lt;num&gt; =
            	number of clusters.<br/>	(default 2).<br/>-P = 	Initialize using the
            k-means++ method.<br/><br/>-V = 	Display std. deviations for centroids.<br/><br/>-M
            = 	Replace missing values with mean/mode.<br/><br/>-A &lt;classname and
            options&gt; = 	Distance function to use.<br/>	(default:
            weka.core.EuclideanDistance)<br/>-I &lt;num&gt; = 	Maximum number of iterations.<br/><br/>-O =
            	Preserve order of instances.<br/><br/>-fast = 	Enables faster distance
            calculations, using cut-off values.<br/>	Disables the calculation/output of
            squared errors/distances.<br/><br/>-num-slots &lt;num&gt; = 	Number of
            execution slots.<br/>	(default 1 - i.e. no parallelism)<br/>-S &lt;num&gt; =
            	Random number seed.<br/>	(default 10)
            </summary>
        </member>
        <member name="P:Ml2.Clstr.Clusterers.SimpleKMeans">
            <summary>
            Cluster data using the k means algorithm. Can use either the Euclidean
            distance (default) or the Manhattan distance. If the Manhattan distance is
            used, then centroids are computed as the component-wise median rather than
            mean. For more information see:<br/><br/>D. Arthur, S. Vassilvitskii:
            k-means++: the advantages of carefull seeding. In: Proceedings of the eighteenth
            annual ACM-SIAM symposium on Discrete algorithms, 1027-1035,
            2007.<br/><br/>Options:<br/><br/>-N &lt;num&gt; = 	number of clusters.<br/>	(default
            2).<br/>-P = 	Initialize using the k-means++ method.<br/><br/>-V = 	Display std.
            deviations for centroids.<br/><br/>-M = 	Replace missing values with
            mean/mode.<br/><br/>-A &lt;classname and options&gt; = 	Distance function to
            use.<br/>	(default: weka.core.EuclideanDistance)<br/>-I &lt;num&gt; = 	Maximum
            number of iterations.<br/><br/>-O = 	Preserve order of
            instances.<br/><br/>-fast = 	Enables faster distance calculations, using cut-off
            values.<br/>	Disables the calculation/output of squared
            errors/distances.<br/><br/>-num-slots &lt;num&gt; = 	Number of execution slots.<br/>	(default 1 - i.e. no
            parallelism)<br/>-S &lt;num&gt; = 	Random number seed.<br/>	(default 10)
            </summary>
        </member>
        <member name="T:Ml2.Clstr.Cobweb">
            <summary>
            Class implementing the Cobweb and Classit clustering
            algorithms.<br/><br/>Note: the application of node operators (merging, splitting etc.) in terms
            of ordering and priority differs (and is somewhat ambiguous) between the
            original Cobweb and Classit papers. This algorithm always compares the best
            host, adding a new leaf, merging the two best hosts, and splitting the best
            host when considering where to place a new instance.<br/><br/>For more
            information see:<br/><br/>D. Fisher (1987). Knowledge acquisition via
            incremental conceptual clustering. Machine Learning. 2(2):139-172.<br/><br/>J. H.
            Gennari, P. Langley, D. Fisher (1990). Models of incremental concept
            formation. Artificial Intelligence. 40:11-61.<br/><br/>Options:<br/><br/>-A
            &lt;acuity&gt; = 	Acuity.<br/>	(default=1.0)<br/>-C &lt;cutoff&gt; =
            	Cutoff.<br/>	(default=0.002)<br/>-S &lt;num&gt; = 	Random number seed.<br/>	(default
            42)
            </summary>
        </member>
        <member name="M:Ml2.Clstr.Cobweb.Acuity(System.Double)">
            <summary>
            set the minimum standard deviation for numeric attributes
            </summary>    
        </member>
        <member name="M:Ml2.Clstr.Cobweb.Cutoff(System.Double)">
            <summary>
            set the category utility threshold by which to prune nodes
            </summary>    
        </member>
        <member name="M:Ml2.Clstr.Cobweb.SaveInstanceData(System.Boolean)">
            <summary>
            save instance information for visualization purposes
            </summary>    
        </member>
        <member name="T:Ml2.Clstr.EM">
            <summary>
            Simple EM (expectation maximisation) class.<br/><br/>EM assigns a
            probability distribution to each instance which indicates the probability of it
            belonging to each of the clusters. EM can decide how many clusters to create
            by cross validation, or you may specify apriori how many clusters to
            generate.<br/><br/>The cross validation performed to determine the number of
            clusters is done in the following steps:<br/>1. the number of clusters is set
            to 1<br/>2. the training set is split randomly into 10 folds.<br/>3. EM is
            performed 10 times using the 10 folds the usual CV way.<br/>4. the
            loglikelihood is averaged over all 10 results.<br/>5. if loglikelihood has increased
            the number of clusters is increased by 1 and the program continues at step
            2. <br/><br/>The number of folds is fixed to 10, as long as the number of
            instances in the training set is not smaller 10. If this is the case the
            number of folds is set equal to the number of
            instances.<br/><br/>Options:<br/><br/>-N &lt;num&gt; = 	number of clusters. If omitted or -1 specified,
            then <br/>	cross validation is used to select the number of clusters.<br/>-X
            &lt;num&gt; = 	Number of folds to use when cross-validating to find the
            best number of clusters.<br/>-max &lt;num&gt; = 	Maximum number of clusters to
            consider during cross-validation. If omitted or -1 specified, then
            <br/>	there is no upper limit on the number of clusters.<br/>-ll-cv &lt;num&gt; =
            	Minimum improvement in cross-validated log likelihood required<br/>	to
            consider increasing the number of clusters.<br/>	(default 1e-6)<br/>-I
            &lt;num&gt; = 	max iterations.<br/>	(default 100)<br/>-ll-iter &lt;num&gt; =
            	Minimum improvement in log likelihood required<br/>	to perform another
            iteration of the E and M steps.<br/>	(default 1e-6)<br/>-V = 	verbose.<br/>-M
            &lt;num&gt; = 	minimum allowable standard deviation for normal
            density<br/>	computation<br/>	(default 1e-6)<br/>-O = 	Display model in old format (good
            when there are many clusters)<br/><br/>-num-slots &lt;num&gt; = 	Number of
            execution slots.<br/>	(default 1 - i.e. no parallelism)<br/>-S &lt;num&gt; =
            	Random number seed.<br/>	(default 100)
            </summary>
        </member>
        <member name="M:Ml2.Clstr.EM.Debug(System.Boolean)">
            <summary>
            If set to true, clusterer may output additional info to the console.
            </summary>    
        </member>
        <member name="M:Ml2.Clstr.EM.MaxIterations(System.Int32)">
            <summary>
            maximum number of iterations
            </summary>    
        </member>
        <member name="M:Ml2.Clstr.EM.NumFolds(System.Int32)">
            <summary>
            The number of folds to use when cross-validating to find the best number
            of clusters (default = 10)
            </summary>    
        </member>
        <member name="M:Ml2.Clstr.EM.MinLogLikelihoodImprovementIterating(System.Double)">
            <summary>
            The minimum improvement in log likelihood required to perform another
            iteration of the E and M steps
            </summary>    
        </member>
        <member name="M:Ml2.Clstr.EM.MinLogLikelihoodImprovementCV(System.Double)">
            <summary>
            The minimum improvement in cross-validated log likelihood required in
            order to consider increasing the number of clusters when cross-validiting to
            find the best number of clusters
            </summary>    
        </member>
        <member name="M:Ml2.Clstr.EM.NumClusters(System.Int32)">
            <summary>
            set number of clusters. -1 to select number of clusters automatically by
            cross validation.
            </summary>    
        </member>
        <member name="M:Ml2.Clstr.EM.MaximumNumberOfClusters(System.Int32)">
            <summary>
            The maximum number of clusters to consider during cross-validation to
            select the best number of clusters
            </summary>    
        </member>
        <member name="M:Ml2.Clstr.EM.MinStdDev(System.Double)">
            <summary>
            set minimum allowable standard deviation
            </summary>    
        </member>
        <member name="M:Ml2.Clstr.EM.DisplayModelInOldFormat(System.Boolean)">
            <summary>
            Use old format for model output. The old format is better when there are
            many clusters. The new format is better when there are fewer clusters and
            many attributes.
            </summary>    
        </member>
        <member name="M:Ml2.Clstr.EM.NumExecutionSlots(System.Int32)">
            <summary>
            The number of execution slots (threads) to use. Set equal to the number
            of available cpu/cores
            </summary>    
        </member>
        <member name="M:Ml2.Clstr.EM.MinStdDevPerAtt(System.Double[])">
            <summary>
            
            </summary>    
        </member>
        <member name="T:Ml2.Clstr.FarthestFirst">
            <summary>
            Cluster data using the FarthestFirst algorithm.<br/><br/>For more
            information see:<br/><br/>Hochbaum, Shmoys (1985). A best possible heuristic for
            the k-center problem. Mathematics of Operations Research.
            10(2):180-184.<br/><br/>Sanjoy Dasgupta: Performance Guarantees for Hierarchical Clustering.
            In: 15th Annual Conference on Computational Learning Theory, 351-363,
            2002.<br/><br/>Notes:<br/>- works as a fast simple approximate clusterer<br/>-
            modelled after SimpleKMeans, might be a useful initializer for
            it<br/><br/>Options:<br/><br/>-N &lt;num&gt; = 	number of clusters. (default =
            2).<br/>-S &lt;num&gt; = 	Random number seed.<br/>	(default 1)
            </summary>
        </member>
        <member name="M:Ml2.Clstr.FarthestFirst.NumClusters(System.Int32)">
            <summary>
            set number of clusters
            </summary>    
        </member>
        <member name="T:Ml2.Clstr.Filtered">
            <summary>
            Class for running an arbitrary clusterer on data that has been passed
            through an arbitrary filter. Like the clusterer, the structure of the filter
            is based exclusively on the training data and test instances will be
            processed by the filter without changing their
            structure.<br/><br/>Options:<br/><br/>-F &lt;filter specification&gt; = 	Full class name of filter to use,
            followed<br/>	by filter options.<br/>	eg:
            "weka.filters.unsupervised.attribute.Remove -V -R 1,2"<br/>(default: weka.filters.AllFilter)<br/>-W = 	Full
            name of base clusterer.<br/>	(default:
            weka.clusterers.SimpleKMeans)<br/><br/>Options specific to clusterer weka.clusterers.SimpleKMeans: = <br/>-N
            &lt;num&gt; = 	number of clusters.<br/>	(default 2).<br/>-P = 	Initialize using
            the k-means++ method.<br/><br/>-V = 	Display std. deviations for
            centroids.<br/><br/>-M = 	Replace missing values with mean/mode.<br/><br/>-A
            &lt;classname and options&gt; = 	Distance function to use.<br/>	(default:
            weka.core.EuclideanDistance)<br/>-I &lt;num&gt; = 	Maximum number of
            iterations.<br/><br/>-O = 	Preserve order of instances.<br/><br/>-fast = 	Enables faster
            distance calculations, using cut-off values.<br/>	Disables the
            calculation/output of squared errors/distances.<br/><br/>-num-slots &lt;num&gt; =
            	Number of execution slots.<br/>	(default 1 - i.e. no parallelism)<br/>-S
            &lt;num&gt; = 	Random number seed.<br/>	(default 10)
            </summary>
        </member>
        <member name="M:Ml2.Clstr.Filtered.Filter(Ml2.Fltr.IBaseFilter{weka.filters.Filter})">
            <summary>
            The filter to be used.
            </summary>    
        </member>
        <member name="M:Ml2.Clstr.Filtered.Clusterer(Ml2.Clstr.IBaseClusterer{weka.clusterers.Clusterer})">
            <summary>
            The base clusterer to be used.
            </summary>    
        </member>
        <member name="T:Ml2.Clstr.Hierarchical">
            <summary>
            Hierarchical clustering class.<br/>Implements a number of classic
            agglomorative (i.e. bottom up) hierarchical clustering methodsbased on
            .<br/><br/>Options:<br/><br/>-D = 	If set, classifier is run in debug mode
            and<br/>	may output additional info to the console<br/>-B = 	If set, distance is
            interpreted as branch length<br/>	otherwise it is node height.<br/>-N &lt;Nr Of
            Clusters&gt; = 	number of clusters<br/>-P = 	Flag to indicate the cluster
            should be printed in Newick format.<br/>-L
            [SINGLE|COMPLETE|AVERAGE|MEAN|CENTROID|WARD|ADJCOMLPETE|NEIGHBOR_JOINING] = Link type (Single, Complete,
            Average, Mean, Centroid, Ward, Adjusted complete, Neighbor joining)<br/>-A
            &lt;classname and options&gt; = 	Distance function to use.<br/>	(default:
            weka.core.EuclideanDistance)
            </summary>
        </member>
        <member name="M:Ml2.Clstr.Hierarchical.NumClusters(System.Int32)">
            <summary>
            Sets the number of clusters. If a single hierarchy is desired, set this
            to 1.
            </summary>    
        </member>
        <member name="M:Ml2.Clstr.Hierarchical.Debug(System.Boolean)">
            <summary>
            If set to true, classifier may output additional info to the console.
            </summary>    
        </member>
        <member name="M:Ml2.Clstr.Hierarchical.DistanceIsBranchLength(System.Boolean)">
            <summary>
            If set to false, the distance between clusters is interpreted as the
            height of the node linking the clusters. This is appropriate for example for
            single link clustering. However, for neighbor joining, the distance is better
            interpreted as branch length. Set this flag to get the latter
            interpretation.
            </summary>    
        </member>
        <member name="M:Ml2.Clstr.Hierarchical.LinkType(Ml2.Clstr.Hierarchical.ELinkType)">
            <summary>
            Sets the method used to measure the distance between two clusters.
            SINGLE: find single link distance aka minimum link, which is the closest distance
            between any item in cluster1 and any item in cluster2 COMPLETE: find
            complete link distance aka maximum link, which is the largest distance between
            any item in cluster1 and any item in cluster2 ADJCOMLPETE: as COMPLETE, but
            with adjustment, which is the largest within cluster distance AVERAGE:
            finds average distance between the elements of the two clusters MEAN:
            calculates the mean distance of a merged cluster (akak Group-average agglomerative
            clustering) CENTROID: finds the distance of the centroids of the clusters
            WARD: finds the distance of the change in caused by merging the cluster. The
            information of a cluster is calculated as the error sum of squares of the
            centroids of the cluster and its members. NEIGHBOR_JOINING use neighbor
            joining algorithm.
            </summary>    
        </member>
        <member name="M:Ml2.Clstr.Hierarchical.DistanceFunction(weka.core.DistanceFunction)">
            <summary>
            Sets the distance function, which measures the distance between two
            individual. instances (or possibly the distance between an instance and the
            centroid of a clusterdepending on the Link type).
            </summary>    
        </member>
        <member name="M:Ml2.Clstr.Hierarchical.PrintNewick(System.Boolean)">
            <summary>
            Flag to indicate whether the cluster should be print in Newick format.
            This can be useful for display in other programs. However, for large datasets
            a lot of text may be produced, which may not be a nuisance when the Newick
            format is not required
            </summary>    
        </member>
        <member name="T:Ml2.Clstr.MakeDensityBased">
            <summary>
            Class for wrapping a Clusterer to make it return a distribution and
            density. Fits normal distributions and discrete distributions within each
            cluster produced by the wrapped clusterer. Supports the
            NumberOfClustersRequestable interface only if the wrapped Clusterer
            does.<br/><br/>Options:<br/><br/>-M &lt;num&gt; = 	minimum allowable standard deviation for normal density
            computation <br/>	(default 1e-6)<br/>-W &lt;clusterer name&gt; = 	Clusterer
            to wrap.<br/>	(default weka.clusterers.SimpleKMeans)<br/><br/>Options
            specific to clusterer weka.clusterers.SimpleKMeans: = <br/>-N &lt;num&gt; =
            	number of clusters.<br/>	(default 2).<br/>-P = 	Initialize using the
            k-means++ method.<br/><br/>-V = 	Display std. deviations for centroids.<br/><br/>-M
            = 	Replace missing values with mean/mode.<br/><br/>-A &lt;classname and
            options&gt; = 	Distance function to use.<br/>	(default:
            weka.core.EuclideanDistance)<br/>-I &lt;num&gt; = 	Maximum number of iterations.<br/><br/>-O =
            	Preserve order of instances.<br/><br/>-fast = 	Enables faster distance
            calculations, using cut-off values.<br/>	Disables the calculation/output of
            squared errors/distances.<br/><br/>-num-slots &lt;num&gt; = 	Number of
            execution slots.<br/>	(default 1 - i.e. no parallelism)<br/>-S &lt;num&gt; =
            	Random number seed.<br/>	(default 10)
            </summary>
        </member>
        <member name="M:Ml2.Clstr.MakeDensityBased.Clusterer(Ml2.Clstr.IBaseClusterer{weka.clusterers.Clusterer})">
            <summary>
            the clusterer to wrap
            </summary>    
        </member>
        <member name="M:Ml2.Clstr.MakeDensityBased.MinStdDev(System.Double)">
            <summary>
            set minimum allowable standard deviation
            </summary>    
        </member>
        <member name="M:Ml2.Clstr.MakeDensityBased.NumClusters(System.Int32)">
            <summary>
            
            </summary>    
        </member>
        <member name="T:Ml2.Clstr.SimpleKMeans">
            <summary>
            Cluster data using the k means algorithm. Can use either the Euclidean
            distance (default) or the Manhattan distance. If the Manhattan distance is
            used, then centroids are computed as the component-wise median rather than
            mean. For more information see:<br/><br/>D. Arthur, S. Vassilvitskii:
            k-means++: the advantages of carefull seeding. In: Proceedings of the eighteenth
            annual ACM-SIAM symposium on Discrete algorithms, 1027-1035,
            2007.<br/><br/>Options:<br/><br/>-N &lt;num&gt; = 	number of clusters.<br/>	(default
            2).<br/>-P = 	Initialize using the k-means++ method.<br/><br/>-V = 	Display std.
            deviations for centroids.<br/><br/>-M = 	Replace missing values with
            mean/mode.<br/><br/>-A &lt;classname and options&gt; = 	Distance function to
            use.<br/>	(default: weka.core.EuclideanDistance)<br/>-I &lt;num&gt; = 	Maximum
            number of iterations.<br/><br/>-O = 	Preserve order of
            instances.<br/><br/>-fast = 	Enables faster distance calculations, using cut-off
            values.<br/>	Disables the calculation/output of squared
            errors/distances.<br/><br/>-num-slots &lt;num&gt; = 	Number of execution slots.<br/>	(default 1 - i.e. no
            parallelism)<br/>-S &lt;num&gt; = 	Random number seed.<br/>	(default 10)
            </summary>
        </member>
        <member name="M:Ml2.Clstr.SimpleKMeans.NumClusters(System.Int32)">
            <summary>
            set number of clusters
            </summary>    
        </member>
        <member name="M:Ml2.Clstr.SimpleKMeans.NumExecutionSlots(System.Int32)">
            <summary>
            The number of execution slots (threads) to use. Set equal to the number
            of available cpu/cores
            </summary>    
        </member>
        <member name="M:Ml2.Clstr.SimpleKMeans.DisplayStdDevs(System.Boolean)">
            <summary>
            Display std deviations of numeric attributes and counts of nominal
            attributes.
            </summary>    
        </member>
        <member name="M:Ml2.Clstr.SimpleKMeans.MaxIterations(System.Int32)">
            <summary>
            set maximum number of iterations
            </summary>    
        </member>
        <member name="M:Ml2.Clstr.SimpleKMeans.DistanceFunction(weka.core.DistanceFunction)">
            <summary>
            The distance function to use for instances comparison (default:
            weka.core.EuclideanDistance).
            </summary>    
        </member>
        <member name="M:Ml2.Clstr.SimpleKMeans.InitializeUsingKMeansPlusPlusMethod(System.Boolean)">
            <summary>
            Initialize cluster centers using the probabilistic farthest first method
            of the k-means++ algorithm
            </summary>    
        </member>
        <member name="M:Ml2.Clstr.SimpleKMeans.DontReplaceMissingValues(System.Boolean)">
            <summary>
            Replace missing values globally with mean/mode.
            </summary>    
        </member>
        <member name="M:Ml2.Clstr.SimpleKMeans.PreserveInstancesOrder(System.Boolean)">
            <summary>
            Preserve order of instances.
            </summary>    
        </member>
        <member name="M:Ml2.Clstr.SimpleKMeans.FastDistanceCalc(System.Boolean)">
            <summary>
            Uses cut-off values for speeding up distance calculation, but suppresses
            also the calculation and output of the within cluster sum of squared
            errors/sum of distances.
            </summary>    
        </member>
        <member name="T:Ml2.Fltr.Add">
            <summary>
            An instance filter that adds a new attribute to the dataset. The new
            attribute will contain all missing values.<br/><br/>Options:<br/><br/>-T
            &lt;NUM|NOM|STR|DAT&gt; = 	The type of attribute to create:<br/>	NUM = Numeric
            attribute<br/>	NOM = Nominal attribute<br/>	STR = String attribute<br/>	DAT =
            Date attribute<br/>	(default: NUM)<br/>-C &lt;index&gt; = 	Specify where
            to insert the column. First and last<br/>	are valid indexes.(default:
            last)<br/>-N &lt;name&gt; = 	Name of the new attribute.<br/>	(default:
            'Unnamed')<br/>-L &lt;label1,label2,...&gt; = 	Create nominal attribute with given
            labels<br/>	(default: numeric attribute)<br/>-F &lt;format&gt; = 	The format
            of the date values (see ISO-8601)<br/>	(default: yyyy-MM-dd'T'HH:mm:ss)
            </summary>
        </member>
        <member name="M:Ml2.Fltr.Add.AttributeName(System.String)">
            <summary>
            Set the new attribute's name.
            </summary>    
        </member>
        <member name="M:Ml2.Fltr.Add.NominalLabels(System.String)">
            <summary>
            The list of value labels (nominal attribute creation only). The list must
            be comma-separated, eg: "red,green,blue". If this is empty, the created
            attribute will be numeric.
            </summary>    
        </member>
        <member name="M:Ml2.Fltr.Add.AttributeType(Ml2.Fltr.Add.EAttributeType)">
            <summary>
            Defines the type of the attribute to generate.
            </summary>    
        </member>
        <member name="M:Ml2.Fltr.Add.AttributeIndex(System.String)">
            <summary>
            The position (starting from 1) where the attribute will be inserted
            (first and last are valid indices).
            </summary>    
        </member>
        <member name="M:Ml2.Fltr.Add.DateFormat(System.String)">
            <summary>
            The format of the date values (see ISO-8601).
            </summary>    
        </member>
        <member name="T:Ml2.Fltr.AddClassification">
            <summary>
            A filter for adding the classification, the class distribution and an
            error flag to a dataset with a classifier. The classifier is either trained on
            the data itself or provided as serialized
            model.<br/><br/>Options:<br/><br/>-D = 	Turns on output of debugging information.<br/>-W &lt;classifier
            specification&gt; = 	Full class name of classifier to use, followed<br/>	by
            scheme options. eg:<br/>		"weka.classifiers.bayes.NaiveBayes
            -D"<br/>	(default: weka.classifiers.rules.ZeroR)<br/>-serialized &lt;file&gt; = 	Instead of
            training a classifier on the data, one can also provide<br/>	a serialized
            model and use that for tagging the data.<br/>-classification = 	Adds an
            attribute with the actual classification.<br/>	(default:
            off)<br/>-remove-old-class = 	Removes the old class attribute.<br/>	(default:
            off)<br/>-distribution = 	Adds attributes with the distribution for all classes <br/>	(for
            numeric classes this will be identical to the attribute <br/>	output with
            '-classification').<br/>	(default: off)<br/>-error = 	Adds an attribute
            indicating whether the classifier output <br/>	a wrong classification (for
            numeric classes this is the numeric <br/>	difference).<br/>	(default: off)
            </summary>
        </member>
        <member name="M:Ml2.Fltr.AddClassification.OutputClassification(System.Boolean)">
            <summary>
            Whether to add an attribute with the actual classification.
            </summary>    
        </member>
        <member name="M:Ml2.Fltr.AddClassification.RemoveOldClass(System.Boolean)">
            <summary>
            Whether to remove the old class attribute.
            </summary>    
        </member>
        <member name="M:Ml2.Fltr.AddClassification.OutputDistribution(System.Boolean)">
            <summary>
            Whether to add attributes with the distribution for all classes (for
            numeric classes this will be identical to the attribute output with
            'outputClassification').
            </summary>    
        </member>
        <member name="M:Ml2.Fltr.AddClassification.OutputErrorFlag(System.Boolean)">
            <summary>
            Whether to add an attribute indicating whether the classifier output a
            wrong classification (for numeric classes this is the numeric difference).
            </summary>    
        </member>
        <member name="M:Ml2.Fltr.AddClassification.Classifier(Ml2.Clss.IBaseClassifier{weka.classifiers.Classifier})">
            <summary>
            The classifier to use for classification.
            </summary>    
        </member>
        <member name="M:Ml2.Fltr.AddClassification.Debug(System.Boolean)">
            <summary>
            Turns on output of debugging information.
            </summary>    
        </member>
        <member name="T:Ml2.Fltr.AddCluster">
            <summary>
            A filter that adds a new nominal attribute representing the cluster
            assigned to each instance by the specified clustering algorithm.<br/>Either the
            clustering algorithm gets built with the first batch of data or one
            specifies are serialized clusterer model file to use
            instead.<br/><br/>Options:<br/><br/>-W &lt;clusterer specification&gt; = 	Full class name of clusterer
            to use, followed<br/>	by scheme options.
            eg:<br/>		"weka.clusterers.SimpleKMeans -N 3"<br/>	(default: weka.clusterers.SimpleKMeans)<br/>-serialized
            &lt;file&gt; = 	Instead of building a clusterer on the data, one can also
            provide<br/>	a serialized model and use that for adding the clusters.<br/>-I
            &lt;att1,att2-att4,...&gt; = 	The range of attributes the clusterer should
            ignore.<br/>
            </summary>
        </member>
        <member name="M:Ml2.Fltr.AddCluster.Clusterer(Ml2.Clstr.IBaseClusterer{weka.clusterers.Clusterer})">
            <summary>
            The clusterer to assign clusters with.
            </summary>    
        </member>
        <member name="M:Ml2.Fltr.AddCluster.IgnoredAttributeIndices(System.String)">
            <summary>
            The range of attributes to be ignored by the clusterer. eg:
            first-3,5,9-last
            </summary>    
        </member>
        <member name="T:Ml2.Fltr.AddExpression">
            <summary>
            An instance filter that creates a new attribute by applying a
            mathematical expression to existing attributes. The expression can contain attribute
            references and numeric constants. Supported operators are :<br/>+, -, *, /,
            ^, log, abs, cos, exp, sqrt, floor, ceil, rint, tan, sin, (,
            )<br/>Attributes are specified by prefixing with 'a', eg. a7 is attribute number 7
            (starting from 1).<br/>Example expression :
            a1^2*a5/log(a7*4.0).<br/><br/>Options:<br/><br/>-E &lt;expression&gt; = 	Specify the expression to apply. Eg
            a1^2*a5/log(a7*4.0).<br/>	Supported opperators: ,+, -, *, /, ^, log, abs, cos,
            <br/>	exp, sqrt, floor, ceil, rint, tan, sin, (, )<br/>	(default:
            a1^2)<br/>-N &lt;name&gt; = 	Specify the name for the new attribute. (default is
            the expression provided with -E)<br/>-D = 	Debug. Names attribute with the
            postfix parse of the expression.
            </summary>
        </member>
        <member name="M:Ml2.Fltr.AddExpression.Name(System.String)">
            <summary>
            Set the name of the new attribute.
            </summary>    
        </member>
        <member name="M:Ml2.Fltr.AddExpression.Expression(System.String)">
            <summary>
            Set the math expression to apply. Eg. a1^2*a5/log(a7*4.0)
            </summary>    
        </member>
        <member name="M:Ml2.Fltr.AddExpression.Debug(System.Boolean)">
            <summary>
            Set debug mode. If true then the new attribute will be named with the
            postfix parse of the supplied expression.
            </summary>    
        </member>
        <member name="T:Ml2.Fltr.AddID">
            <summary>
            An instance filter that adds an ID attribute to the dataset. The new
            attribute contains a unique ID for each instance.<br/>Note: The ID is not reset
            for the second batch of files (using -b and -r and
            -s).<br/><br/>Options:<br/><br/>-C &lt;index&gt; = 	Specify where to insert the ID. First and
            last<br/>	are valid indexes.(default first)<br/>-N &lt;name&gt; = 	Name of the
            new attribute.<br/>	(default = 'ID')
            </summary>
        </member>
        <member name="M:Ml2.Fltr.AddID.AttributeName(System.String)">
            <summary>
            Set the new attribute's name.
            </summary>    
        </member>
        <member name="M:Ml2.Fltr.AddID.IDIndex(System.String)">
            <summary>
            
            </summary>    
        </member>
        <member name="T:Ml2.Fltr.AddNoise">
            <summary>
            An instance filter that changes a percentage of a given attributes
            values. The attribute must be nominal. Missing value can be treated as value
            itself.<br/><br/>Options:<br/><br/>-C &lt;col&gt; = 	Index of the attribute to
            be changed <br/>	(default last attribute)<br/>-M = 	Treat missing values as
            an extra value <br/><br/>-P &lt;num&gt; = 	Specify the percentage of noise
            introduced <br/>	to the data (default 10)<br/>-S &lt;num&gt; = 	Specify
            the random number seed (default 1)
            </summary>
        </member>
        <member name="M:Ml2.Fltr.AddNoise.AttributeIndex(System.String)">
            <summary>
            Index of the attribute that is to changed.
            </summary>    
        </member>
        <member name="M:Ml2.Fltr.AddNoise.UseMissing(System.Boolean)">
            <summary>
            Flag to set if missing values are used.
            </summary>    
        </member>
        <member name="M:Ml2.Fltr.AddNoise.Percent(System.Int32)">
            <summary>
            Percentage of introduced noise to data.
            </summary>    
        </member>
        <member name="T:Ml2.Fltr.AddUserFields">
            <summary>
            A filter that adds new attributes with user specified type and constant
            value. Numeric, nominal, string and date attributes can be created.
            Attribute name, and value can be set with environment variables. Date attributes
            can also specify a formatting string by which to parse the supplied date
            value. Alternatively, a current time stamp can be specified by supplying the
            special string "now" as the value for a date
            attribute.<br/><br/>Options:<br/><br/>-A &lt;name:type:value&gt; = 	New field specification
            (name@type@value).<br/>	 Environment variables may be used for any/all parts of
            the<br/>	specification. Type can be one of (numeric, nominal, string or
            date).<br/>	The value for date be a specific date string or the special
            string<br/>	"now" to indicate the current date-time. A specific date format<br/>	string for
            parsing specific date values can be specified by suffixing<br/>	the type
            specification - e.g. "myTime@date:MM-dd-yyyy@08-23-2009".This option may be
            specified multiple times
            </summary>
        </member>
        <member name="M:Ml2.Fltr.AddUserFields.Environment(weka.core.Environment)">
            <summary>
            
            </summary>    
        </member>
        <member name="T:Ml2.Fltr.AddValues">
            <summary>
            Adds the labels from the given list to an attribute if they are missing.
            The labels can also be sorted in an ascending manner. If no labels are
            provided then only the (optional) sorting
            applies.<br/><br/>Options:<br/><br/>-C &lt;col&gt; = 	Sets the attribute index<br/>	(default last).<br/>-L
            &lt;label1,label2,...&gt; = 	Comma-separated list of labels to
            add.<br/>	(default: none)<br/>-S = 	Turns on the sorting of the labels.
            </summary>
        </member>
        <member name="M:Ml2.Fltr.AddValues.AttributeIndex(System.String)">
            <summary>
            Sets which attribute to process. This attribute must be nominal ("first"
            and "last" are valid values)
            </summary>    
        </member>
        <member name="M:Ml2.Fltr.AddValues.Labels(System.String)">
            <summary>
            Comma-separated list of lables to add.
            </summary>    
        </member>
        <member name="M:Ml2.Fltr.AddValues.Sort(System.Boolean)">
            <summary>
            Whether to sort the labels alphabetically.
            </summary>    
        </member>
        <member name="T:Ml2.Fltr.AllFilter">
            <summary>
            An instance filter that passes all instances through unmodified.
            Primarily for testing purposes.
            </summary>
        </member>
        <member name="T:Ml2.Fltr.AttributeSelection">
            <summary>
            A supervised attribute filter that can be used to select attributes. It
            is very flexible and allows various search and evaluation methods to be
            combined.<br/><br/>Options:<br/><br/>-S &lt;"Name of search class [search
            options]"&gt; = 	Sets search method for subset evaluators.<br/>	eg. -S
            "weka.attributeSelection.BestFirst -S 8"<br/>-E &lt;"Name of attribute/subset
            evaluation class [evaluator options]"&gt; = 	Sets attribute/subset
            evaluator.<br/>	eg. -E "weka.attributeSelection.CfsSubsetEval -L"<br/><br/>Options
            specific to evaluator weka.attributeSelection.CfsSubsetEval: = <br/>-M = 	Treat
            missing values as a separate value.<br/>-L = 	Don't include locally
            predictive attributes.<br/><br/>Options specific to search
            weka.attributeSelection.BestFirst: = <br/>-P &lt;start set&gt; = 	Specify a starting set of
            attributes.<br/>	Eg. 1,3,5-7.<br/>-D &lt;0 = backward | 1 = forward | 2 =
            bi-directional&gt; = 	Direction of search. (default = 1).<br/>-N &lt;num&gt; =
            	Number of non-improving nodes to<br/>	consider before terminating
            search.<br/>-S &lt;num&gt; = 	Size of lookup cache for evaluated
            subsets.<br/>	Expressed as a multiple of the number of<br/>	attributes in the data set. (default
            = 1)
            </summary>
        </member>
        <member name="M:Ml2.Fltr.AttributeSelection.Evaluator(Ml2.AttrSel.Evals.BaseAttributeSelectionEvaluator{weka.attributeSelection.ASEvaluation})">
            <summary>
            Determines how attributes/attribute subsets are evaluated.
            </summary>    
        </member>
        <member name="M:Ml2.Fltr.AttributeSelection.Search(Ml2.AttrSel.Algs.BaseAttributeSelectionAlgorithm{weka.attributeSelection.ASSearch})">
            <summary>
            Determines the search method.
            </summary>    
        </member>
        <member name="T:Ml2.Fltr.Center">
            <summary>
            Centers all numeric attributes in the given dataset to have zero mean
            (apart from the class attribute, if
            set).<br/><br/>Options:<br/><br/>-unset-class-temporarily = 	Unsets the class index temporarily before the filter
            is<br/>	applied to the data.<br/>	(default: no)
            </summary>
        </member>
        <member name="M:Ml2.Fltr.Center.IgnoreClass(System.Boolean)">
            <summary>
            The class index will be unset temporarily before the filter is applied.
            </summary>    
        </member>
        <member name="T:Ml2.Fltr.ChangeDateFormat">
            <summary>
            Changes the date format used by a date attribute. This is most useful for
            converting to a format with less precision, for example, from an absolute
            date to day of year, etc. This changes the format string, and changes the
            date values to those that would be parsed by the new
            format.<br/><br/>Options:<br/><br/>-C &lt;col&gt; = 	Sets the attribute index (default
            last).<br/>-F &lt;value index&gt; = 	Sets the output date format string (default
            corresponds to ISO-8601).
            </summary>
        </member>
        <member name="M:Ml2.Fltr.ChangeDateFormat.AttributeIndex(System.String)">
            <summary>
            Sets which attribute to process. This attribute must be of type date
            ("first" and "last" are valid values)
            </summary>    
        </member>
        <member name="M:Ml2.Fltr.ChangeDateFormat.DateFormat(System.String)">
            <summary>
            The date format to change to. This should be a format understood by
            Java's SimpleDateFormat class.
            </summary>    
        </member>
        <member name="M:Ml2.Fltr.ChangeDateFormat.DateFormat(java.text.SimpleDateFormat)">
            <summary>
            The date format to change to. This should be a format understood by
            Java's SimpleDateFormat class.
            </summary>    
        </member>
        <member name="T:Ml2.Fltr.ClassAssigner">
            <summary>
            Filter that can set and unset the class
            index.<br/><br/>Options:<br/><br/>-D = 	Turns on output of debugging information.<br/>-C
            &lt;num|first|last|0&gt; = 	The index of the class attribute. Index starts with 1,
            'first'<br/>	and 'last' are accepted, '0' unsets the class index.<br/>	(default: last)
            </summary>
        </member>
        <member name="M:Ml2.Fltr.ClassAssigner.ClassIndex(System.String)">
            <summary>
            The index of the class attribute, starts with 1, 'first' and 'last' are
            accepted as well, '0' unsets the class index.
            </summary>    
        </member>
        <member name="M:Ml2.Fltr.ClassAssigner.Debug(System.Boolean)">
            <summary>
            Turns on output of debugging information.
            </summary>    
        </member>
        <member name="T:Ml2.Fltr.ClassOrder">
            <summary>
            Changes the order of the classes so that the class values are no longer
            of in the order specified in the header. The values will be in the order
            specified by the user -- it could be either in ascending/descending order by
            the class frequency or in random order. Note that this filter currently does
            not change the header, only the class values of the instances, so there is
            not much point in using it in conjunction with the FilteredClassifier. The
            value can also be converted back using 'originalValue(double value)'
            procedure.<br/><br/>Options:<br/><br/>-R &lt;seed&gt; = 	Specify the seed of
            randomization<br/>	used to randomize the class<br/>	order (default: 1)<br/>-C
            &lt;order&gt; = 	Specify the class order to be<br/>	sorted, could be 0:
            ascending<br/>	1: descending and 2: random.(default: 0)
            </summary>
        </member>
        <member name="M:Ml2.Fltr.ClassOrder.SetClassOrder(System.Int32)">
            <summary>
            
            </summary>    
        </member>
        <member name="T:Ml2.Fltr.ClusterMembership">
            <summary>
            A filter that uses a density-based clusterer to generate cluster
            membership values; filtered instances are composed of these values plus the class
            attribute (if set in the input data). If a (nominal) class attribute is set,
            the clusterer is run separately for each class. The class attribute (if
            set) and any user-specified attributes are ignored during the clustering
            operation<br/><br/>Options:<br/><br/>-W &lt;clusterer name&gt; = 	Full name of
            clusterer to use. eg:<br/>		weka.clusterers.EM<br/>	Additional options
            after the '--'.<br/>	(default: weka.clusterers.EM)<br/>-I
            &lt;att1,att2-att4,...&gt; = 	The range of attributes the clusterer should ignore.<br/>	(the
            class attribute is automatically ignored)
            </summary>
        </member>
        <member name="M:Ml2.Fltr.ClusterMembership.DensityBasedClusterer(weka.clusterers.DensityBasedClusterer)">
            <summary>
            The clusterer that will generate membership values for the instances.
            </summary>    
        </member>
        <member name="M:Ml2.Fltr.ClusterMembership.IgnoredAttributeIndices(System.String)">
            <summary>
            The range of attributes to be ignored by the clusterer. eg:
            first-3,5,9-last
            </summary>    
        </member>
        <member name="T:Ml2.Fltr.Copy">
            <summary>
            An instance filter that copies a range of attributes in the dataset. This
            is used in conjunction with other filters that overwrite attribute values
            during the course of their operation -- this filter allows the original
            attributes to be kept as well as the new
            attributes.<br/><br/>Options:<br/><br/>-R &lt;index1,index2-index4,...&gt; = 	Specify list of columns to copy.
            First and last are valid<br/>	indexes. (default none)<br/>-V = 	Invert
            matching sense (i.e. copy all non-specified columns)
            </summary>
        </member>
        <member name="M:Ml2.Fltr.Copy.AttributeIndices(System.String)">
            <summary>
            Specify range of attributes to act on. This is a comma separated list of
            attribute indices, with "first" and "last" valid values. Specify an
            inclusive range with "-". E.g: "first-3,5,6-10,last".
            </summary>    
        </member>
        <member name="M:Ml2.Fltr.Copy.InvertSelection(System.Boolean)">
            <summary>
            Sets copy selected vs unselected action. If set to false, only the
            specified attributes will be copied; If set to true, non-specified attributes
            will be copied.
            </summary>    
        </member>
        <member name="M:Ml2.Fltr.Copy.AttributeIndicesArray(System.Int32[])">
            <summary>
            
            </summary>    
        </member>
        <member name="T:Ml2.Fltr.Discretize">
            <summary>
            An instance filter that discretizes a range of numeric attributes in the
            dataset into nominal attributes. Discretization is by simple binning. Skips
            the class attribute if
            set.<br/><br/>Options:<br/><br/>-unset-class-temporarily = 	Unsets the class index temporarily before the filter
            is<br/>	applied to the data.<br/>	(default: no)<br/>-B &lt;num&gt; = 	Specifies the
            (maximum) number of bins to divide numeric attributes into.<br/>	(default =
            10)<br/>-M &lt;num&gt; = 	Specifies the desired weight of instances per bin
            for<br/>	equal-frequency binning. If this is set to a positive<br/>	number
            then the -B option will be ignored.<br/>	(default = -1)<br/>-F = 	Use
            equal-frequency instead of equal-width discretization.<br/>-O = 	Optimize number
            of bins using leave-one-out estimate<br/>	of estimated entropy (for
            equal-width discretization).<br/>	If this is set then the -B option will be
            ignored.<br/>-R &lt;col1,col2-col4,...&gt; = 	Specifies list of columns to
            Discretize. First and last are valid indexes.<br/>	(default: first-last)<br/>-V =
            	Invert matching sense of column indexes.<br/>-D = 	Output binary
            attributes for discretized attributes.<br/>-Y = 	Use bin numbers rather than ranges
            for discretized attributes.
            </summary>
        </member>
        <member name="M:Ml2.Fltr.Discretize.AttributeIndices(System.String)">
            <summary>
            Specify range of attributes to act on. This is a comma separated list of
            attribute indices, with "first" and "last" valid values. Specify an
            inclusive range with "-". E.g: "first-3,5,6-10,last".
            </summary>    
        </member>
        <member name="M:Ml2.Fltr.Discretize.Bins(System.Int32)">
            <summary>
            Number of bins.
            </summary>    
        </member>
        <member name="M:Ml2.Fltr.Discretize.UseEqualFrequency(System.Boolean)">
            <summary>
            If set to true, equal-frequency binning will be used instead of
            equal-width binning.
            </summary>    
        </member>
        <member name="M:Ml2.Fltr.Discretize.InvertSelection(System.Boolean)">
            <summary>
            Set attribute selection mode. If false, only selected (numeric)
            attributes in the range will be discretized; if true, only non-selected attributes
            will be discretized.
            </summary>    
        </member>
        <member name="M:Ml2.Fltr.Discretize.MakeBinary(System.Boolean)">
            <summary>
            Make resulting attributes binary.
            </summary>    
        </member>
        <member name="M:Ml2.Fltr.Discretize.UseBinNumbers(System.Boolean)">
            <summary>
            Use bin numbers (eg BXofY) rather than ranges for for discretized
            attributes
            </summary>    
        </member>
        <member name="M:Ml2.Fltr.Discretize.FindNumBins(System.Boolean)">
            <summary>
            Optimize number of equal-width bins using leave-one-out. Doesn't work for
            equal-frequency binning
            </summary>    
        </member>
        <member name="M:Ml2.Fltr.Discretize.DesiredWeightOfInstancesPerInterval(System.Double)">
            <summary>
            Sets the desired weight of instances per interval for equal-frequency
            binning.
            </summary>    
        </member>
        <member name="M:Ml2.Fltr.Discretize.AttributeIndicesArray(System.Int32[])">
            <summary>
            
            </summary>    
        </member>
        <member name="M:Ml2.Fltr.Discretize.IgnoreClass(System.Boolean)">
            <summary>
            The class index will be unset temporarily before the filter is applied.
            </summary>    
        </member>
        <member name="T:Ml2.Fltr.FirstOrder">
            <summary>
            This instance filter takes a range of N numeric attributes and replaces
            them with N-1 numeric attributes, the values of which are the difference
            between consecutive attribute values from the original instance. eg:
            <br/><br/>Original attribute values<br/><br/> 0.1, 0.2, 0.3, 0.1, 0.3<br/><br/>New
            attribute values<br/><br/> 0.1, 0.1, -0.2, 0.2<br/><br/>The range of
            attributes used is taken in numeric order. That is, a range spec of 7-11,3-5 will
            use the attribute ordering 3,4,5,7,8,9,10,11 for the differences, NOT
            7,8,9,10,11,3,4,5.<br/><br/>Options:<br/><br/>-R
            &lt;index1,index2-index4,...&gt; = 	Specify list of columns to take the differences between.<br/>	First
            and last are valid indexes.<br/>	(default none)
            </summary>
        </member>
        <member name="M:Ml2.Fltr.FirstOrder.AttributeIndices(System.String)">
            <summary>
            Specify range of attributes to act on. This is a comma separated list of
            attribute indices, with "first" and "last" valid values. Specify an
            inclusive range with "-". E.g: "first-3,5,6-10,last".
            </summary>    
        </member>
        <member name="M:Ml2.Fltr.FirstOrder.AttributeIndicesArray(System.Int32[])">
            <summary>
            
            </summary>    
        </member>
        <!-- Badly formed XML comment ignored for member "T:Ml2.Fltr.InterquartileRange" -->
        <member name="M:Ml2.Fltr.InterquartileRange.AttributeIndices(System.String)">
            <summary>
            Specify range of attributes to act on; this is a comma separated list of
            attribute indices, with "first" and "last" valid values; specify an
            inclusive range with "-", eg: "first-3,5,6-10,last".
            </summary>    
        </member>
        <member name="M:Ml2.Fltr.InterquartileRange.OutlierFactor(System.Double)">
            <summary>
            The factor for determining the thresholds for outliers.
            </summary>    
        </member>
        <member name="M:Ml2.Fltr.InterquartileRange.ExtremeValuesFactor(System.Double)">
            <summary>
            The factor for determining the thresholds for extreme values.
            </summary>    
        </member>
        <member name="M:Ml2.Fltr.InterquartileRange.ExtremeValuesAsOutliers(System.Boolean)">
            <summary>
            Whether to tag extreme values also as outliers.
            </summary>    
        </member>
        <member name="M:Ml2.Fltr.InterquartileRange.DetectionPerAttribute(System.Boolean)">
            <summary>
            Generates Outlier/ExtremeValue attribute pair for each numeric attribute,
            not just a single pair for all numeric attributes together.
            </summary>    
        </member>
        <member name="M:Ml2.Fltr.InterquartileRange.OutputOffsetMultiplier(System.Boolean)">
            <summary>
            Generates an additional attribute 'Offset' that contains the multiplier
            the value is off the median: value = median + 'multiplier' * IQR
            </summary>    
        </member>
        <member name="M:Ml2.Fltr.InterquartileRange.AttributeIndicesArray(System.Int32[])">
            <summary>
            
            </summary>    
        </member>
        <member name="M:Ml2.Fltr.InterquartileRange.Debug(System.Boolean)">
            <summary>
            Turns on output of debugging information.
            </summary>    
        </member>
        <member name="T:Ml2.Fltr.KernelFilter">
            <summary>
            Converts the given set of predictor variables into a kernel matrix. The
            class value remains unchangedm, as long as the preprocessing filter doesn't
            change it.<br/>By default, the data is preprocessed with the Center filter,
            but the user can choose any filter (NB: one must be careful that the
            filter does not alter the class attribute unintentionally). With
            weka.filters.AllFilter the preprocessing gets disabled.<br/><br/>For more information
            regarding preprocessing the data, see:<br/><br/>K.P. Bennett, M.J. Embrechts:
            An Optimization Perspective on Kernel Partial Least Squares Regression. In:
            Advances in Learning Theory: Methods, Models and Applications, 227-249,
            2003.<br/><br/>Options:<br/><br/>-D = 	Turns on output of debugging
            information.<br/>-no-checks = 	Turns off all checks - use with caution!<br/>	Turning
            them off assumes that data is purely numeric, doesn't<br/>	contain any
            missing values, and has a nominal class. Turning them<br/>	off also means that
            no header information will be stored if the<br/>	machine is linear.
            Finally, it also assumes that no instance has<br/>	a weight equal to
            0.<br/>	(default: checks on)<br/>-F &lt;filename&gt; = 	The file to initialize the
            filter with (optional).<br/>-C &lt;num&gt; = 	The class index for the file to
            initialize with,<br/>	First and last are valid (optional, default:
            last).<br/>-K &lt;classname and parameters&gt; = 	The Kernel to use.<br/>	(default:
            weka.classifiers.functions.supportVector.PolyKernel)<br/>-kernel-factor =
            	Defines a factor for the kernel.<br/>		- RBFKernel: a factor for
            gamma<br/>			Standardize: 1/(2*N)<br/>			Normalize..: 6/N<br/>	Available parameters
            are:<br/>		N for # of instances, A for # of attributes<br/>	(default:
            1)<br/>-P &lt;classname and parameters&gt; = 	The Filter used for preprocessing
            (use weka.filters.AllFilter<br/>	to disable preprocessing).<br/>	(default:
            weka.filters.unsupervised.attribute.Center)<br/><br/>Options specific to
            kernel weka.classifiers.functions.supportVector.PolyKernel: = <br/>-D =
            	Enables debugging output (if available) to be printed.<br/>	(default:
            off)<br/>-no-checks = 	Turns off all checks - use with caution!<br/>	(default: checks
            on)<br/>-C &lt;num&gt; = 	The size of the cache (a prime number), 0 for
            full cache and <br/>	-1 to turn it off.<br/>	(default: 250007)<br/>-E
            &lt;num&gt; = 	The Exponent to use.<br/>	(default: 1.0)<br/>-L = 	Use lower-order
            terms.<br/>	(default: no)<br/><br/>Options specific to preprocessing filter
            weka.filters.unsupervised.attribute.Center: = <br/>-unset-class-temporarily
            = 	Unsets the class index temporarily before the filter is<br/>	applied to
            the data.<br/>	(default: no)
            </summary>
        </member>
        <member name="M:Ml2.Fltr.KernelFilter.ChecksTurnedOff(System.Boolean)">
            <summary>
            Turns time-consuming checks off - use with caution.
            </summary>    
        </member>
        <member name="M:Ml2.Fltr.KernelFilter.InitFileClassIndex(System.String)">
            <summary>
            The class index of the dataset to initialize the filter with (first and
            last are valid).
            </summary>    
        </member>
        <member name="M:Ml2.Fltr.KernelFilter.Kernel(weka.classifiers.functions.supportVector.Kernel)">
            <summary>
            The kernel to use.
            </summary>    
        </member>
        <member name="M:Ml2.Fltr.KernelFilter.KernelFactorExpression(System.String)">
            <summary>
            The factor for the kernel, with A = # of attributes and N = # of
            instances.
            </summary>    
        </member>
        <member name="M:Ml2.Fltr.KernelFilter.Preprocessing(Ml2.Fltr.IBaseFilter{weka.filters.Filter})">
            <summary>
            Sets the filter to use for preprocessing (use the AllFilter for no
            preprocessing).
            </summary>    
        </member>
        <member name="M:Ml2.Fltr.KernelFilter.Debug(System.Boolean)">
            <summary>
            Turns on output of debugging information.
            </summary>    
        </member>
        <member name="T:Ml2.Fltr.MakeIndicator">
            <summary>
            A filter that creates a new dataset with a boolean attribute replacing a
            nominal attribute. In the new dataset, a value of 1 is assigned to an
            instance that exhibits a particular range of attribute values, a 0 to an
            instance that doesn't. The boolean attribute is coded as numeric by
            default.<br/><br/>Options:<br/><br/>-C &lt;col&gt; = 	Sets the attribute index.<br/>-V
            &lt;index1,index2-index4,...&gt; = 	Specify the list of values to indicate.
            First and last are<br/>	valid indexes (default last)<br/>-N &lt;index&gt; =
            	Set if new boolean attribute nominal.
            </summary>
        </member>
        <member name="M:Ml2.Fltr.MakeIndicator.AttributeIndex(System.String)">
            <summary>
            Sets which attribute should be replaced by the indicator. This attribute
            must be nominal.
            </summary>    
        </member>
        <member name="M:Ml2.Fltr.MakeIndicator.ValueIndex(System.Int32)">
            <summary>
            
            </summary>    
        </member>
        <member name="M:Ml2.Fltr.MakeIndicator.Numeric(System.Boolean)">
            <summary>
            Determines whether the output indicator attribute is numeric. If this is
            set to false, the output attribute will be nominal.
            </summary>    
        </member>
        <member name="M:Ml2.Fltr.MakeIndicator.ValueIndices(System.String)">
            <summary>
            Specify range of nominal values to act on. This is a comma separated list
            of attribute indices (numbered from 1), with "first" and "last" valid
            values. Specify an inclusive range with "-". E.g: "first-3,5,6-10,last".
            </summary>    
        </member>
        <member name="M:Ml2.Fltr.MakeIndicator.ValueIndicesArray(System.Int32[])">
            <summary>
            
            </summary>    
        </member>
        <member name="T:Ml2.Fltr.MathExpression">
            <summary>
            Modify numeric attributes according to a given expression
            <br/><br/>Options:<br/><br/>-unset-class-temporarily = 	Unsets the class index temporarily
            before the filter is<br/>	applied to the data.<br/>	(default: no)<br/>-E
            &lt;expression&gt; = 	Specify the expression to apply. Eg.
            pow(A,6)/(MEAN+MAX)<br/>	Supported operators are +, -, *, /, pow, log,<br/>	abs, cos, exp,
            sqrt, tan, sin, ceil, floor, rint, (, ), <br/>	MEAN, MAX, MIN, SD, COUNT,
            SUM, SUMSQUARED, ifelse. The 'A'<br/>	letter refers to the value of the
            attribute being processed.<br/>	Other attribute values (numeric only) can be
            accessed through<br/>	the variables A1, A2, A3, ...<br/>-R
            &lt;index1,index2-index4,...&gt; = 	Specify list of columns to ignore. First and last are
            valid<br/>	indexes. (default none)<br/>-V = 	Invert matching sense (i.e. only
            modify specified columns)
            </summary>
        </member>
        <member name="M:Ml2.Fltr.MathExpression.InvertSelection(System.Boolean)">
            <summary>
            Determines whether action is to select or unselect. If set to true, only
            the specified attributes will be modified; If set to false, specified
            attributes will not be modified.
            </summary>    
        </member>
        <!-- Badly formed XML comment ignored for member "M:Ml2.Fltr.MathExpression.Expression(System.String)" -->
        <member name="M:Ml2.Fltr.MathExpression.IgnoreRange(System.String)">
            <summary>
            Specify range of attributes to act on. This is a comma separated list of
            attribute indices, with "first" and "last" valid values. Specify an
            inclusive range with "-". E.g: "first-3,5,6-10,last".
            </summary>    
        </member>
        <member name="M:Ml2.Fltr.MathExpression.IgnoreClass(System.Boolean)">
            <summary>
            The class index will be unset temporarily before the filter is applied.
            </summary>    
        </member>
        <member name="T:Ml2.Fltr.MergeManyValues">
            <summary>
            Merges many values of a nominal attribute into one
            value.<br/><br/>Options:<br/><br/>-C &lt;col&gt; = 	Sets the attribute index<br/>	(default:
            last)<br/>-L &lt;label&gt; = 	Sets the label of the newly merged
            classes<br/>	(default: 'merged')<br/>-R &lt;range&gt; = 	Sets the merge range. 'first and
            'last' are accepted as well.'<br/>	E.g.: first-5,7,9,20-last<br/>	(default:
            1,2)
            </summary>
        </member>
        <member name="M:Ml2.Fltr.MergeManyValues.AttributeIndex(System.String)">
            <summary>
            Sets which attribute to process. This attribute must be nominal ("first"
            and "last" are valid values)
            </summary>    
        </member>
        <member name="M:Ml2.Fltr.MergeManyValues.Label(System.String)">
            <summary>
            The new label for the merged values.
            </summary>    
        </member>
        <member name="M:Ml2.Fltr.MergeManyValues.MergeValueRange(System.String)">
            <summary>
            The range of values to merge.
            </summary>    
        </member>
        <member name="T:Ml2.Fltr.MergeTwoValues">
            <summary>
            Merges two values of a nominal attribute into one
            value.<br/><br/>Options:<br/><br/>-C &lt;col&gt; = 	Sets the attribute index (default
            last).<br/>-F &lt;value index&gt; = 	Sets the first value's index (default
            first).<br/>-S &lt;value index&gt; = 	Sets the second value's index (default last).
            </summary>
        </member>
        <member name="M:Ml2.Fltr.MergeTwoValues.AttributeIndex(System.String)">
            <summary>
            Sets which attribute to process. This attribute must be nominal ("first"
            and "last" are valid values)
            </summary>    
        </member>
        <member name="M:Ml2.Fltr.MergeTwoValues.FirstValueIndex(System.String)">
            <summary>
            Sets the first value to be merged. ("first" and "last" are valid values)
            </summary>    
        </member>
        <member name="M:Ml2.Fltr.MergeTwoValues.SecondValueIndex(System.String)">
            <summary>
            Sets the second value to be merged. ("first" and "last" are valid values)
            </summary>    
        </member>
        <member name="T:Ml2.Fltr.MultiFilter">
            <summary>
            Applies several filters successively. In case all supplied filters are
            StreamableFilters, it will act as a streamable one,
            too.<br/><br/>Options:<br/><br/>-D = 	Turns on output of debugging information.<br/>-F &lt;classname
            [options]&gt; = 	A filter to apply (can be specified multiple times).
            </summary>
        </member>
        <member name="M:Ml2.Fltr.MultiFilter.Filters(System.Collections.Generic.IEnumerable{Ml2.Fltr.IBaseFilter{weka.filters.Filter}})">
            <summary>
            The base filters to be used.
            </summary>    
        </member>
        <member name="M:Ml2.Fltr.MultiFilter.Debug(System.Boolean)">
            <summary>
            Turns on output of debugging information.
            </summary>    
        </member>
        <member name="T:Ml2.Fltr.NominalToBinary">
            <summary>
            Converts all nominal attributes into binary numeric attributes. An
            attribute with k values is transformed into k binary attributes if the class is
            nominal (using the one-attribute-per-value approach). Binary attributes are
            left binary, if option '-A' is not given.If the class is numeric, you might
            want to use the supervised version of this
            filter.<br/><br/>Options:<br/><br/>-N = 	Sets if binary attributes are to be coded as nominal ones.<br/>-A
            = 	For each nominal value a new attribute is created, <br/>	not only if
            there are more than 2 values.<br/>-R &lt;col1,col2-col4,...&gt; = 	Specifies
            list of columns to act on. First and last are <br/>	valid
            indexes.<br/>	(default: first-last)<br/>-V = 	Invert matching sense of column indexes.
            </summary>
        </member>
        <member name="M:Ml2.Fltr.NominalToBinary.AttributeIndices(System.String)">
            <summary>
            Specify range of attributes to act on. This is a comma separated list of
            attribute indices, with "first" and "last" valid values. Specify an
            inclusive range with "-". E.g: "first-3,5,6-10,last".
            </summary>    
        </member>
        <member name="M:Ml2.Fltr.NominalToBinary.BinaryAttributesNominal(System.Boolean)">
            <summary>
            Whether resulting binary attributes will be nominal.
            </summary>    
        </member>
        <member name="M:Ml2.Fltr.NominalToBinary.TransformAllValues(System.Boolean)">
            <summary>
            Whether all nominal values are turned into new attributes, not only if
            there are more than 2.
            </summary>    
        </member>
        <member name="M:Ml2.Fltr.NominalToBinary.InvertSelection(System.Boolean)">
            <summary>
            Set attribute selection mode. If false, only selected (numeric)
            attributes in the range will be discretized; if true, only non-selected attributes
            will be discretized.
            </summary>    
        </member>
        <member name="T:Ml2.Fltr.NominalToString">
            <summary>
            Converts a nominal attribute (that is, a set number of values) to string
            (that is, an unspecified number of values).<br/><br/>Options:<br/><br/>-C
            &lt;col&gt; = 	Sets the range of attributes to convert (default last).
            </summary>
        </member>
        <member name="M:Ml2.Fltr.NominalToString.AttributeIndexes(System.String)">
            <summary>
            Sets a range attributes to process. Any non-nominal attributes in the
            range are left untouched ("first" and "last" are valid values)
            </summary>    
        </member>
        <member name="T:Ml2.Fltr.NonSparseToSparse">
            <summary>
            An instance filter that converts all incoming instances into sparse
            format.<br/><br/>Options:<br/><br/>-M = 	Treat missing values as zero.<br/>-F =
            	Add a dummy first value for nominal attributes.
            </summary>
        </member>
        <member name="M:Ml2.Fltr.NonSparseToSparse.TreatMissingValuesAsZero(System.Boolean)">
            <summary>
            Treat missing values in the same way as zeros.
            </summary>    
        </member>
        <member name="M:Ml2.Fltr.NonSparseToSparse.InsertDummyNominalFirstValue(System.Boolean)">
            <summary>
            Insert a dummy value before the first declared value for all nominal
            attributes. Useful when converting market basket data that has been encoded for
            Apriori to sparse format. Typically used in conjuction with treat missing
            values as zero.
            </summary>    
        </member>
        <member name="T:Ml2.Fltr.Normalize">
            <summary>
            Normalizes all numeric values in the given dataset (apart from the class
            attribute, if set). The resulting values are by default in [0,1] for the
            data used to compute the normalization intervals. But with the scale and
            translation parameters one can change that, e.g., with scale = 2.0 and
            translation = -1.0 you get values in the range
            [-1,+1].<br/><br/>Options:<br/><br/>-unset-class-temporarily = 	Unsets the class index temporarily before the
            filter is<br/>	applied to the data.<br/>	(default: no)<br/>-S &lt;num&gt; =
            	The scaling factor for the output range.<br/>	(default: 1.0)<br/>-T
            &lt;num&gt; = 	The translation of the output range.<br/>	(default: 0.0)
            </summary>
        </member>
        <member name="M:Ml2.Fltr.Normalize.Scale(System.Double)">
            <summary>
            The factor for scaling the output range (default: 1).
            </summary>    
        </member>
        <member name="M:Ml2.Fltr.Normalize.Translation(System.Double)">
            <summary>
            The translation of the output range (default: 0).
            </summary>    
        </member>
        <member name="M:Ml2.Fltr.Normalize.IgnoreClass(System.Boolean)">
            <summary>
            The class index will be unset temporarily before the filter is applied.
            </summary>    
        </member>
        <member name="T:Ml2.Fltr.NumericCleaner">
            <summary>
            A filter that 'cleanses' the numeric data from values that are too small,
            too big or very close to a certain value (e.g., 0) and sets these values
            to a pre-defined default.<br/><br/>Options:<br/><br/>-D = 	Turns on output
            of debugging information.<br/>-min &lt;double&gt; = 	The minimum threshold.
            (default -Double.MAX_VALUE)<br/>-min-default &lt;double&gt; = 	The
            replacement for values smaller than the minimum threshold.<br/>	(default
            -Double.MAX_VALUE)<br/>-max &lt;double&gt; = 	The maximum threshold. (default
            Double.MAX_VALUE)<br/>-max-default &lt;double&gt; = 	The replacement for values
            larger than the maximum threshold.<br/>	(default
            Double.MAX_VALUE)<br/>-closeto &lt;double&gt; = 	The number values are checked for closeness. (default
            0)<br/>-closeto-default &lt;double&gt; = 	The replacement for values that
            are close to '-closeto'.<br/>	(default 0)<br/>-closeto-tolerance
            &lt;double&gt; = 	The tolerance below which numbers are considered being close to
            <br/>	to each other. (default 1E-6)<br/>-decimals &lt;int&gt; = 	The number of
            decimals to round to, -1 means no rounding at all.<br/>	(default -1)<br/>-R
            &lt;col1,col2,...&gt; = 	The list of columns to cleanse, e.g., first-last
            or first-3,5-last.<br/>	(default first-last)<br/>-V = 	Inverts the matching
            sense.<br/>-include-class = 	Whether to include the class in the
            cleansing.<br/>	The class column will always be skipped, if this flag is
            not<br/>	present. (default no)
            </summary>
        </member>
        <member name="M:Ml2.Fltr.NumericCleaner.MinThreshold(System.Double)">
            <summary>
            The minimum threshold below values are replaced by a default.
            </summary>    
        </member>
        <member name="M:Ml2.Fltr.NumericCleaner.MinDefault(System.Double)">
            <summary>
            The default value to replace values that are below the minimum threshold.
            </summary>    
        </member>
        <member name="M:Ml2.Fltr.NumericCleaner.MaxThreshold(System.Double)">
            <summary>
            The maximum threshold above values are replaced by a default.
            </summary>    
        </member>
        <member name="M:Ml2.Fltr.NumericCleaner.MaxDefault(System.Double)">
            <summary>
            The default value to replace values that are above the maximum threshold.
            </summary>    
        </member>
        <member name="M:Ml2.Fltr.NumericCleaner.CloseTo(System.Double)">
            <summary>
            The number values are checked for whether they are too close to and get
            replaced by a default.
            </summary>    
        </member>
        <member name="M:Ml2.Fltr.NumericCleaner.CloseToDefault(System.Double)">
            <summary>
            The default value to replace values with that are too close.
            </summary>    
        </member>
        <member name="M:Ml2.Fltr.NumericCleaner.CloseToTolerance(System.Double)">
            <summary>
            The value below which values are considered close to.
            </summary>    
        </member>
        <member name="M:Ml2.Fltr.NumericCleaner.AttributeIndices(System.String)">
            <summary>
            The selection of columns to use in the cleansing processs, first and last
            are valid indices.
            </summary>    
        </member>
        <member name="M:Ml2.Fltr.NumericCleaner.InvertSelection(System.Boolean)">
            <summary>
            If enabled the selection of the columns is inverted.
            </summary>    
        </member>
        <member name="M:Ml2.Fltr.NumericCleaner.IncludeClass(System.Boolean)">
            <summary>
            If disabled, the class attribute will be always left out of the cleaning
            process.
            </summary>    
        </member>
        <member name="M:Ml2.Fltr.NumericCleaner.Decimals(System.Int32)">
            <summary>
            The number of decimals to round to, -1 means no rounding at all.
            </summary>    
        </member>
        <member name="M:Ml2.Fltr.NumericCleaner.Debug(System.Boolean)">
            <summary>
            Turns on output of debugging information.
            </summary>    
        </member>
        <member name="T:Ml2.Fltr.NumericToBinary">
            <summary>
            Converts all numeric attributes into binary attributes (apart from the
            class attribute, if set): if the value of the numeric attribute is exactly
            zero, the value of the new attribute will be zero. If the value of the
            numeric attribute is missing, the value of the new attribute will be missing.
            Otherwise, the value of the new attribute will be one. The new attributes will
            be nominal.<br/><br/>Options:<br/><br/>-unset-class-temporarily = 	Unsets
            the class index temporarily before the filter is<br/>	applied to the
            data.<br/>	(default: no)
            </summary>
        </member>
        <member name="M:Ml2.Fltr.NumericToBinary.IgnoreClass(System.Boolean)">
            <summary>
            The class index will be unset temporarily before the filter is applied.
            </summary>    
        </member>
        <member name="T:Ml2.Fltr.NumericToNominal">
            <summary>
            A filter for turning numeric attributes into nominal ones. Unlike
            discretization, it just takes all numeric values and adds them to the list of
            nominal values of that attribute. Useful after CSV imports, to enforce certain
            attributes to become nominal, e.g., the class attribute, containing values
            from 1 to 5.<br/><br/>Options:<br/><br/>-R &lt;col1,col2-col4,...&gt; =
            	Specifies list of columns to Discretize. First and last are valid
            indexes.<br/>	(default: first-last)<br/>-V = 	Invert matching sense of column indexes.
            </summary>
        </member>
        <member name="M:Ml2.Fltr.NumericToNominal.InvertSelection(System.Boolean)">
            <summary>
            Set attribute selection mode. If false, only selected (numeric)
            attributes in the range will be 'nominalized'; if true, only non-selected attributes
            will be 'nominalized'.
            </summary>    
        </member>
        <member name="M:Ml2.Fltr.NumericToNominal.AttributeIndices(System.String)">
            <summary>
            Specify range of attributes to act on. This is a comma separated list of
            attribute indices, with "first" and "last" valid values. Specify an
            inclusive range with "-". E.g: "first-3,5,6-10,last".
            </summary>    
        </member>
        <member name="M:Ml2.Fltr.NumericToNominal.AttributeIndicesArray(System.Int32[])">
            <summary>
            
            </summary>    
        </member>
        <member name="M:Ml2.Fltr.NumericToNominal.Debug(System.Boolean)">
            <summary>
            Turns on output of debugging information.
            </summary>    
        </member>
        <member name="T:Ml2.Fltr.NumericTransform">
            <summary>
            Transforms numeric attributes using a given transformation
            method.<br/><br/>Options:<br/><br/>-R &lt;index1,index2-index4,...&gt; = 	Specify list of
            columns to transform. First and last are<br/>	valid indexes (default
            none). Non-numeric columns are <br/>	skipped.<br/>-V = 	Invert matching
            sense.<br/>-C &lt;string&gt; = 	Sets the class containing transformation
            method.<br/>	(default java.lang.Math)<br/>-M &lt;string&gt; = 	Sets the method.
            (default abs)
            </summary>
        </member>
        <member name="M:Ml2.Fltr.NumericTransform.AttributeIndices(System.String)">
            <summary>
            Specify range of attributes to act on. This is a comma separated list of
            attribute indices, with "first" and "last" valid values. Specify an
            inclusive range with "-". E.g: "first-3,5,6-10,last".
            </summary>    
        </member>
        <member name="M:Ml2.Fltr.NumericTransform.InvertSelection(System.Boolean)">
            <summary>
            Whether to process the inverse of the given attribute ranges.
            </summary>    
        </member>
        <member name="M:Ml2.Fltr.NumericTransform.ClassName(System.String)">
            <summary>
            Name of the class containing the method used for the transformation.
            </summary>    
        </member>
        <member name="M:Ml2.Fltr.NumericTransform.MethodName(System.String)">
            <summary>
            Name of the method used for the transformation.
            </summary>    
        </member>
        <member name="M:Ml2.Fltr.NumericTransform.AttributeIndicesArray(System.Int32[])">
            <summary>
            
            </summary>    
        </member>
        <member name="T:Ml2.Fltr.Obfuscate">
            <summary>
            A simple instance filter that renames the relation, all attribute names
            and all nominal (and string) attribute values. For exchanging sensitive
            datasets. Currently doesn't like string or relational attributes.
            </summary>
        </member>
        <member name="T:Ml2.Fltr.PartitionedMultiFilter">
            <summary>
            A filter that applies filters on subsets of attributes and assembles the
            output into a new dataset. Attributes that are not covered by any of the
            ranges can be either retained or removed from the
            output.<br/><br/>Options:<br/><br/>-D = 	Turns on output of debugging information.<br/>-F
            &lt;classname [options]&gt; = 	A filter to apply (can be specified multiple
            times).<br/>-R &lt;range&gt; = 	An attribute range (can be specified multiple
            times).<br/>	For each filter a range must be supplied. 'first' and 'last'<br/>	are
            valid indices. 'inv(...)' around the range denotes an<br/>	inverted
            range.<br/>-U = 	Flag for leaving unused attributes out of the output, by
            default<br/>	these are included in the filter output.
            </summary>
        </member>
        <member name="M:Ml2.Fltr.PartitionedMultiFilter.RemoveUnused(System.Boolean)">
            <summary>
            If true then unused attributes (ones that are not covered by any of the
            ranges) will be removed from the output.
            </summary>    
        </member>
        <member name="M:Ml2.Fltr.PartitionedMultiFilter.Filters(System.Collections.Generic.IEnumerable{Ml2.Fltr.IBaseFilter{weka.filters.Filter}})">
            <summary>
            The base filters to be used.
            </summary>    
        </member>
        <member name="M:Ml2.Fltr.PartitionedMultiFilter.Ranges(weka.core.Range[])">
            <summary>
            The attribute ranges to be used; 'inv(...)' denotes an inverted range.
            </summary>    
        </member>
        <member name="M:Ml2.Fltr.PartitionedMultiFilter.Debug(System.Boolean)">
            <summary>
            Turns on output of debugging information.
            </summary>    
        </member>
        <member name="T:Ml2.Fltr.PartitionMembership">
            <summary>
            A filter that uses a PartitionGenerator to generate partition membership
            values; filtered instances are composed of these values plus the class
            attribute (if set in the input data) and rendered as sparse
            instances.<br/><br/>Options:<br/><br/>-W &lt;name of partition generator&gt; = 	Full name of
            partition generator to use,
            e.g.:<br/>		weka.classifiers.trees.J48<br/>	Additional options after the '--'.<br/>	(default: weka.classifiers.trees.J48)
            </summary>
        </member>
        <member name="M:Ml2.Fltr.PartitionMembership.PartitionGenerator(weka.core.PartitionGenerator)">
            <summary>
            The partition generator that will generate membership values for the
            instances.
            </summary>    
        </member>
        <member name="T:Ml2.Fltr.PKIDiscretize">
            <summary>
            Discretizes numeric attributes using equal frequency binning, where the
            number of bins is equal to the square root of the number of non-missing
            values.<br/><br/>For more information, see:<br/><br/>Ying Yang, Geoffrey I.
            Webb: Proportional k-Interval Discretization for Naive-Bayes Classifiers. In:
            12th European Conference on Machine Learning, 564-575,
            2001.<br/><br/>Options:<br/><br/>-unset-class-temporarily = 	Unsets the class index temporarily
            before the filter is<br/>	applied to the data.<br/>	(default: no)<br/>-R
            &lt;col1,col2-col4,...&gt; = 	Specifies list of columns to Discretize. First
            and last are valid indexes.<br/>	(default: first-last)<br/>-V = 	Invert
            matching sense of column indexes.<br/>-D = 	Output binary attributes for
            discretized attributes.
            </summary>
        </member>
        <member name="M:Ml2.Fltr.PKIDiscretize.FindNumBins(System.Boolean)">
            <summary>
            Ignored.
            </summary>    
        </member>
        <member name="M:Ml2.Fltr.PKIDiscretize.UseEqualFrequency(System.Boolean)">
            <summary>
            Always true.
            </summary>    
        </member>
        <member name="M:Ml2.Fltr.PKIDiscretize.Bins(System.Int32)">
            <summary>
            Ignored.
            </summary>    
        </member>
        <member name="M:Ml2.Fltr.PKIDiscretize.AttributeIndices(System.String)">
            <summary>
            Specify range of attributes to act on. This is a comma separated list of
            attribute indices, with "first" and "last" valid values. Specify an
            inclusive range with "-". E.g: "first-3,5,6-10,last".
            </summary>    
        </member>
        <member name="M:Ml2.Fltr.PKIDiscretize.InvertSelection(System.Boolean)">
            <summary>
            Set attribute selection mode. If false, only selected (numeric)
            attributes in the range will be discretized; if true, only non-selected attributes
            will be discretized.
            </summary>    
        </member>
        <member name="M:Ml2.Fltr.PKIDiscretize.MakeBinary(System.Boolean)">
            <summary>
            Make resulting attributes binary.
            </summary>    
        </member>
        <member name="M:Ml2.Fltr.PKIDiscretize.UseBinNumbers(System.Boolean)">
            <summary>
            Use bin numbers (eg BXofY) rather than ranges for for discretized
            attributes
            </summary>    
        </member>
        <member name="M:Ml2.Fltr.PKIDiscretize.DesiredWeightOfInstancesPerInterval(System.Double)">
            <summary>
            Sets the desired weight of instances per interval for equal-frequency
            binning.
            </summary>    
        </member>
        <member name="M:Ml2.Fltr.PKIDiscretize.AttributeIndicesArray(System.Int32[])">
            <summary>
            
            </summary>    
        </member>
        <member name="M:Ml2.Fltr.PKIDiscretize.IgnoreClass(System.Boolean)">
            <summary>
            The class index will be unset temporarily before the filter is applied.
            </summary>    
        </member>
        <member name="T:Ml2.Fltr.PrincipalComponents">
            <summary>
            Performs a principal components analysis and transformation of the
            data.<br/>Dimensionality reduction is accomplished by choosing enough
            eigenvectors to account for some percentage of the variance in the original data --
            default 0.95 (95%).<br/>Based on code of the attribute selection scheme
            'PrincipalComponents' by Mark Hall and Gabi
            Schmidberger.<br/><br/>Options:<br/><br/>-C = 	Center (rather than standardize) the<br/>	data and compute PCA
            using the covariance (rather<br/>	 than the correlation) matrix.<br/>-R
            &lt;num&gt; = 	Retain enough PC attributes to account<br/>	for this proportion
            of variance in the original data.<br/>	(default: 0.95)<br/>-A &lt;num&gt; =
            	Maximum number of attributes to include in <br/>	transformed attribute
            names.<br/>	(-1 = include all, default: 5)<br/>-M &lt;num&gt; = 	Maximum
            number of PC attributes to retain.<br/>	(-1 = include all, default: -1)
            </summary>
        </member>
        <member name="M:Ml2.Fltr.PrincipalComponents.VarianceCovered(System.Double)">
            <summary>
            Retain enough PC attributes to account for this proportion of variance.
            </summary>    
        </member>
        <member name="M:Ml2.Fltr.PrincipalComponents.MaximumAttributeNames(System.Int32)">
            <summary>
            The maximum number of attributes to include in transformed attribute
            names.
            </summary>    
        </member>
        <member name="M:Ml2.Fltr.PrincipalComponents.MaximumAttributes(System.Int32)">
            <summary>
            The maximum number of PC attributes to retain.
            </summary>    
        </member>
        <member name="M:Ml2.Fltr.PrincipalComponents.CenterData(System.Boolean)">
            <summary>
            Center (rather than standardize) the data. PCA will be computed from the
            covariance (rather than correlation) matrix
            </summary>    
        </member>
        <member name="T:Ml2.Fltr.Randomize">
            <summary>
            Randomly shuffles the order of instances passed through it. The random
            number generator is reset with the seed value whenever a new set of instances
            is passed in.<br/><br/>Options:<br/><br/>-S &lt;num&gt; = 	Specify the
            random number seed (default 42)
            </summary>
        </member>
        <member name="T:Ml2.Fltr.RandomProjection">
            <summary>
            Reduces the dimensionality of the data by projecting it onto a lower
            dimensional subspace using a random matrix with columns of unit length (i.e. It
            will reduce the number of attributes in the data while preserving much of
            its variation like PCA, but at a much less computational cost).<br/>It
            first applies the NominalToBinary filter to convert all attributes to numeric
            before reducing the dimension. It preserves the class
            attribute.<br/><br/>For more information, see:<br/><br/>Dmitriy Fradkin, David Madigan:
            Experiments with random projections for machine learning. In: KDD '03: Proceedings
            of the ninth ACM SIGKDD international conference on Knowledge discovery and
            data mining, New York, NY, USA, 517-522, 003.<br/><br/>Options:<br/><br/>-N
            &lt;number&gt; = 	The number of dimensions (attributes) the data should be
            reduced to<br/>	(default 10; exclusive of the class attribute, if it is
            set).<br/>-D [SPARSE1|SPARSE2|GAUSSIAN] = 	The distribution to use for
            calculating the random matrix.<br/>	Sparse1 is:<br/>	 sqrt(3)*{-1 with prob(1/6),
            0 with prob(2/3), +1 with prob(1/6)}<br/>	Sparse2 is:<br/>	 {-1 with
            prob(1/2), +1 with prob(1/2)}<br/><br/>-P &lt;percent&gt; = 	The percentage of
            dimensions (attributes) the data should<br/>	be reduced to (exclusive of the
            class attribute, if it is set). This -N<br/>	option is ignored if this
            option is present or is greater<br/>	than zero.<br/>-M = 	Replace missing
            values using the ReplaceMissingValues filter<br/>-R &lt;num&gt; = 	The random
            seed for the random number generator used for<br/>	calculating the random
            matrix (default 42).
            </summary>
        </member>
        <member name="M:Ml2.Fltr.RandomProjection.Percent(System.Double)">
            <summary>
            The percentage of dimensions (attributes) the data should be reduced to
            (inclusive of the class attribute). This NumberOfAttributes option is
            ignored if this option is present or is greater than zero.
            </summary>    
        </member>
        <member name="M:Ml2.Fltr.RandomProjection.NumberOfAttributes(System.Int32)">
            <summary>
            The number of dimensions (attributes) the data should be reduced to.
            </summary>    
        </member>
        <member name="M:Ml2.Fltr.RandomProjection.Distribution(Ml2.Fltr.RandomProjection.EDistribution)">
            <summary>
            The distribution to use for calculating the random matrix. Sparse1 is:
            sqrt(3) * { -1 with prob(1/6), 0 with prob(2/3), +1 with prob(1/6) } Sparse2
            is: { -1 with prob(1/2), +1 with prob(1/2) }
            </summary>    
        </member>
        <member name="M:Ml2.Fltr.RandomProjection.ReplaceMissingValues(System.Boolean)">
            <summary>
            If set the filter uses
            weka.filters.unsupervised.attribute.ReplaceMissingValues to replace the missing values
            </summary>    
        </member>
        <!-- Badly formed XML comment ignored for member "T:Ml2.Fltr.RandomSubset" -->
        <!-- Badly formed XML comment ignored for member "M:Ml2.Fltr.RandomSubset.NumAttributes(System.Double)" -->
        <member name="M:Ml2.Fltr.RandomSubset.Debug(System.Boolean)">
            <summary>
            Turns on output of debugging information.
            </summary>    
        </member>
        <member name="T:Ml2.Fltr.Remove">
            <summary>
            A filter that removes a range of attributes from the dataset. Will
            re-order the remaining attributes if invert matching sense is turned on and the
            attribute column indices are not specified in ascending
            order.<br/><br/>Options:<br/><br/>-R &lt;index1,index2-index4,...&gt; = 	Specify list of
            columns to delete. First and last are valid<br/>	indexes. (default none)<br/>-V =
            	Invert matching sense (i.e. only keep specified columns)
            </summary>
        </member>
        <member name="M:Ml2.Fltr.Remove.AttributeIndices(System.String)">
            <summary>
            Specify range of attributes to act on. This is a comma separated list of
            attribute indices, with "first" and "last" valid values. Specify an
            inclusive range with "-". E.g: "first-3,5,6-10,last".
            </summary>    
        </member>
        <member name="M:Ml2.Fltr.Remove.InvertSelection(System.Boolean)">
            <summary>
            Determines whether action is to select or delete. If set to true, only
            the specified attributes will be kept; If set to false, specified attributes
            will be deleted.
            </summary>    
        </member>
        <member name="M:Ml2.Fltr.Remove.AttributeIndicesArray(System.Int32[])">
            <summary>
            
            </summary>    
        </member>
        <member name="T:Ml2.Fltr.RemoveByName">
            <summary>
            Removes attributes based on a regular expression matched against their
            names.<br/><br/>Options:<br/><br/>-D = 	Turns on output of debugging
            information.<br/>-E &lt;regular expression&gt; = 	The regular expression to match
            the attribute names against.<br/>	(default: ^.*id$)<br/>-V = 	Flag for
            inverting the matching sense. If set, attributes are kept<br/>	instead of
            deleted.<br/>	(default: off)
            </summary>
        </member>
        <member name="M:Ml2.Fltr.RemoveByName.Expression(System.String)">
            <summary>
            The regular expression to match the attribute names against.
            </summary>    
        </member>
        <member name="M:Ml2.Fltr.RemoveByName.InvertSelection(System.Boolean)">
            <summary>
            Determines whether action is to select or delete. If set to true, only
            the specified attributes will be kept; If set to false, specified attributes
            will be deleted.
            </summary>    
        </member>
        <member name="M:Ml2.Fltr.RemoveByName.Debug(System.Boolean)">
            <summary>
            Turns on output of debugging information.
            </summary>    
        </member>
        <member name="T:Ml2.Fltr.RemoveFolds">
            <summary>
            This filter takes a dataset and outputs a specified fold for cross
            validation. If you want the folds to be stratified use the supervised
            version.<br/><br/>Options:<br/><br/>-V = 	Specifies if inverse of selection is to be
            output.<br/><br/>-N &lt;number of folds&gt; = 	Specifies number of folds
            dataset is split into. <br/>	(default 10)<br/><br/>-F &lt;fold&gt; =
            	Specifies which fold is selected. (default 1)<br/><br/>-S &lt;seed&gt; = 	Specifies
            random number seed. (default 0, no randomizing)<br/>
            </summary>
        </member>
        <member name="M:Ml2.Fltr.RemoveFolds.InvertSelection(System.Boolean)">
            <summary>
            Whether to invert the selection.
            </summary>    
        </member>
        <member name="M:Ml2.Fltr.RemoveFolds.NumFolds(System.Int32)">
            <summary>
            The number of folds to split the dataset into.
            </summary>    
        </member>
        <member name="M:Ml2.Fltr.RemoveFolds.Fold(System.Int32)">
            <summary>
            The fold which is selected.
            </summary>    
        </member>
        <member name="T:Ml2.Fltr.RemoveFrequentValues">
            <summary>
            Determines which values (frequent or infrequent ones) of an (nominal)
            attribute are retained and filters the instances accordingly. In case of
            values with the same frequency, they are kept in the way they appear in the
            original instances object. E.g. if you have the values "1,2,3,4" with the
            frequencies "10,5,5,3" and you chose to keep the 2 most common values, the
            values "1,2" would be returned, since the value "2" comes before "3", even
            though they have the same frequency.<br/><br/>Options:<br/><br/>-C &lt;num&gt; =
            	Choose attribute to be used for selection.<br/>-N &lt;num&gt; = 	Number
            of values to retain for the sepcified attribute, <br/>	i.e. the ones with
            the most instances (default 2).<br/>-L = 	Instead of values with the most
            instances the ones with the <br/>	least are retained.<br/><br/>-H = 	When
            selecting on nominal attributes, removes header<br/>	references to excluded
            values.<br/>-V = 	Invert matching sense.
            </summary>
        </member>
        <member name="M:Ml2.Fltr.RemoveFrequentValues.AttributeIndex(System.String)">
            <summary>
            Choose attribute to be used for selection (default last).
            </summary>    
        </member>
        <member name="M:Ml2.Fltr.RemoveFrequentValues.NumValues(System.Int32)">
            <summary>
            The number of values to retain.
            </summary>    
        </member>
        <member name="M:Ml2.Fltr.RemoveFrequentValues.UseLeastValues(System.Boolean)">
            <summary>
            Retains values with least instance instead of most.
            </summary>    
        </member>
        <member name="M:Ml2.Fltr.RemoveFrequentValues.ModifyHeader(System.Boolean)">
            <summary>
            When selecting on nominal attributes, removes header references to
            excluded values.
            </summary>    
        </member>
        <member name="M:Ml2.Fltr.RemoveFrequentValues.InvertSelection(System.Boolean)">
            <summary>
            Invert matching sense.
            </summary>    
        </member>
        <!-- Badly formed XML comment ignored for member "T:Ml2.Fltr.RemoveMisclassified" -->
        <member name="M:Ml2.Fltr.RemoveMisclassified.Classifier(Ml2.Clss.IBaseClassifier{weka.classifiers.Classifier})">
            <summary>
            The classifier upon which to base the misclassifications.
            </summary>    
        </member>
        <!-- Badly formed XML comment ignored for member "M:Ml2.Fltr.RemoveMisclassified.ClassIndex(System.Int32)" -->
        <!-- Badly formed XML comment ignored for member "M:Ml2.Fltr.RemoveMisclassified.NumFolds(System.Int32)" -->
        <member name="M:Ml2.Fltr.RemoveMisclassified.Threshold(System.Double)">
            <summary>
            Threshold for the max allowable error when predicting a numeric class.
            Should be >= 0.
            </summary>    
        </member>
        <!-- Badly formed XML comment ignored for member "M:Ml2.Fltr.RemoveMisclassified.MaxIterations(System.Int32)" -->
        <member name="M:Ml2.Fltr.RemoveMisclassified.Invert(System.Boolean)">
            <summary>
            Whether or not to invert the selection. If true, correctly classified
            instances will be discarded.
            </summary>    
        </member>
        <member name="T:Ml2.Fltr.RemovePercentage">
            <summary>
            A filter that removes a given percentage of a
            dataset.<br/><br/>Options:<br/><br/>-P &lt;percentage&gt; = 	Specifies percentage of instances to
            select. (default 50)<br/><br/>-V = 	Specifies if inverse of selection is to be
            output.<br/>
            </summary>
        </member>
        <member name="M:Ml2.Fltr.RemovePercentage.Percentage(System.Double)">
            <summary>
            The percentage of the data to select.
            </summary>    
        </member>
        <member name="M:Ml2.Fltr.RemovePercentage.InvertSelection(System.Boolean)">
            <summary>
            Whether to invert the selection.
            </summary>    
        </member>
        <member name="T:Ml2.Fltr.RemoveRange">
            <summary>
            A filter that removes a given range of instances of a
            dataset.<br/><br/>Options:<br/><br/>-R &lt;inst1,inst2-inst4,...&gt; = 	Specifies list of
            instances to select. First and last<br/>	are valid indexes.
            (required)<br/><br/>-V = 	Specifies if inverse of selection is to be output.<br/>
            </summary>
        </member>
        <member name="M:Ml2.Fltr.RemoveRange.InstancesIndices(System.String)">
            <summary>
            The range of instances to select. First and last are valid indexes.
            </summary>    
        </member>
        <member name="M:Ml2.Fltr.RemoveRange.InvertSelection(System.Boolean)">
            <summary>
            Whether to invert the selection.
            </summary>    
        </member>
        <member name="T:Ml2.Fltr.RemoveType">
            <summary>
            Removes attributes of a given type.<br/><br/>Options:<br/><br/>-T
            &lt;nominal|numeric|string|date|relational&gt; = 	Attribute type to delete. Valid
            options are "nominal", <br/>	"numeric", "string", "date" and
            "relational".<br/>	(default "string")<br/>-V = 	Invert matching sense (i.e. only keep
            specified columns)
            </summary>
        </member>
        <member name="M:Ml2.Fltr.RemoveType.InvertSelection(System.Boolean)">
            <summary>
            Determines whether action is to select or delete. If set to true, only
            the specified attributes will be kept; If set to false, specified attributes
            will be deleted.
            </summary>    
        </member>
        <member name="M:Ml2.Fltr.RemoveType.AttributeType(Ml2.Fltr.RemoveType.EAttributeType)">
            <summary>
            The type of attribute to remove.
            </summary>    
        </member>
        <member name="T:Ml2.Fltr.RemoveUseless">
            <summary>
            This filter removes attributes that do not vary at all or that vary too
            much. All constant attributes are deleted automatically, along with any that
            exceed the maximum percentage of variance parameter. The maximum variance
            test is only applied to nominal attributes.<br/><br/>Options:<br/><br/>-M
            &lt;max variance %&gt; = 	Maximum variance percentage allowed (default 99)
            </summary>
        </member>
        <member name="M:Ml2.Fltr.RemoveUseless.MaximumVariancePercentageAllowed(System.Double)">
            <summary>
            Set the threshold for the highest variance allowed before a nominal
            attribute will be deleted.Specifically, if (number_of_distinct_values /
            total_number_of_values * 100) is greater than this value then the attribute will be
            removed.
            </summary>    
        </member>
        <member name="T:Ml2.Fltr.RemoveWithValues">
            <summary>
            Filters instances according to the value of an
            attribute.<br/><br/>Options:<br/><br/>-C &lt;num&gt; = 	Choose attribute to be used for
            selection.<br/>-S &lt;num&gt; = 	Numeric value to be used for selection on
            numeric<br/>	attribute.<br/>	Instances with values smaller than given value will<br/>	be
            selected. (default 0)<br/>-L &lt;index1,index2-index4,...&gt; = 	Range of
            label indices to be used for selection on<br/>	nominal
            attribute.<br/>	First and last are valid indexes. (default all values)<br/>-M = 	Missing values
            count as a match. This setting is<br/>	independent of the -V
            option.<br/>	(default missing values don't match)<br/>-V = 	Invert matching
            sense.<br/>-H = 	When selecting on nominal attributes, removes header<br/>	references
            to excluded values.<br/>-F = 	Do not apply the filter to instances that
            arrive after the first<br/>	(training) batch. The default is to apply the
            filter (i.e.<br/>	the filter may not return an instance if it matches the remove
            criteria)
            </summary>
        </member>
        <member name="M:Ml2.Fltr.RemoveWithValues.AttributeIndex(System.String)">
            <summary>
            Choose attribute to be used for selection (default last).
            </summary>    
        </member>
        <member name="M:Ml2.Fltr.RemoveWithValues.ModifyHeader(System.Boolean)">
            <summary>
            When selecting on nominal attributes, removes header references to
            excluded values.
            </summary>    
        </member>
        <member name="M:Ml2.Fltr.RemoveWithValues.InvertSelection(System.Boolean)">
            <summary>
            Invert matching sense.
            </summary>    
        </member>
        <member name="M:Ml2.Fltr.RemoveWithValues.NominalIndicesArr(System.Int32[])">
            <summary>
            
            </summary>    
        </member>
        <member name="M:Ml2.Fltr.RemoveWithValues.SplitPoint(System.Double)">
            <summary>
            Numeric value to be used for selection on numeric attribute. Instances
            with values smaller than given value will be selected.
            </summary>    
        </member>
        <member name="M:Ml2.Fltr.RemoveWithValues.NominalIndices(System.String)">
            <summary>
            Range of label indices to be used for selection on nominal attribute.
            First and last are valid indexes.
            </summary>    
        </member>
        <member name="M:Ml2.Fltr.RemoveWithValues.MatchMissingValues(System.Boolean)">
            <summary>
            Missing values count as a match. This setting is independent of the
            invertSelection option.
            </summary>    
        </member>
        <member name="M:Ml2.Fltr.RemoveWithValues.DontFilterAfterFirstBatch(System.Boolean)">
            <summary>
            Whether to apply the filtering process to instances that are input after
            the first (training) batch. The default is false so instances in subsequent
            batches can potentially get 'consumed' by the filter.
            </summary>    
        </member>
        <!-- Badly formed XML comment ignored for member "T:Ml2.Fltr.RenameAttribute" -->
        <member name="M:Ml2.Fltr.RenameAttribute.Find(System.String)">
            <summary>
            The regular expression that the attribute names must match.
            </summary>    
        </member>
        <member name="M:Ml2.Fltr.RenameAttribute.Replace(System.String)">
            <summary>
            The regular expression to use for replacing the matching attribute names
            with.
            </summary>    
        </member>
        <member name="M:Ml2.Fltr.RenameAttribute.ReplaceAll(System.Boolean)">
            <summary>
            If set to true, then all occurrences of the match will be replaced;
            otherwise only the first.
            </summary>    
        </member>
        <member name="M:Ml2.Fltr.RenameAttribute.AttributeIndices(System.String)">
            <summary>
            Specify range of attributes to act on; this is a comma separated list of
            attribute indices, with "first" and "last" valid values; specify an
            inclusive range with "-"; eg: "first-3,5,6-10,last".
            </summary>    
        </member>
        <member name="M:Ml2.Fltr.RenameAttribute.InvertSelection(System.Boolean)">
            <summary>
            If set to true, the selection will be inverted; eg: the attribute indices
            '2-4' then mean everything apart from '2-4'.
            </summary>    
        </member>
        <member name="M:Ml2.Fltr.RenameAttribute.Debug(System.Boolean)">
            <summary>
            Turns on output of debugging information.
            </summary>    
        </member>
        <member name="T:Ml2.Fltr.Reorder">
            <summary>
            A filter that generates output with a new order of the attributes. Useful
            if one wants to move an attribute to the end to use it as class attribute
            (e.g. with using "-R 2-last,1").<br/>But it's not only possible to change
            the order of all the attributes, but also to leave out attributes. E.g. if
            you have 10 attributes, you can generate the following output order:
            1,3,5,7,9,10 or 10,1-5.<br/>You can also duplicate attributes, e.g. for further
            processing later on: e.g. 1,1,1,4,4,4,2,2,2 where the second and the third
            column of each attribute are processed differently and the first one, i.e.
            the original one is kept.<br/>One can simply inverse the order of the
            attributes via 'last-first'.<br/>After appyling the filter, the index of the class
            attribute is the last attribute.<br/><br/>Options:<br/><br/>-R
            &lt;index1,index2-index4,...&gt; = 	Specify list of columns to copy. First and last
            are valid<br/>	indexes. (default first-last)
            </summary>
        </member>
        <member name="M:Ml2.Fltr.Reorder.AttributeIndicesArray(System.Int32[])">
            <summary>
            
            </summary>    
        </member>
        <member name="M:Ml2.Fltr.Reorder.AttributeIndices(System.String)">
            <summary>
            Specify range of attributes to act on. This is a comma separated list of
            attribute indices, with "first" and "last" valid values. Specify an
            inclusive range with "-". E.g: "first-3,5,6-10,last".
            </summary>    
        </member>
        <member name="T:Ml2.Fltr.ReplaceMissingValues">
            <summary>
            Replaces all missing values for nominal and numeric attributes in a
            dataset with the modes and means from the training
            data.<br/><br/>Options:<br/><br/>-unset-class-temporarily = 	Unsets the class index temporarily before
            the filter is<br/>	applied to the data.<br/>	(default: no)
            </summary>
        </member>
        <member name="M:Ml2.Fltr.ReplaceMissingValues.IgnoreClass(System.Boolean)">
            <summary>
            The class index will be unset temporarily before the filter is applied.
            </summary>    
        </member>
        <member name="T:Ml2.Fltr.ReplaceMissingWithUserConstant">
            <summary>
            Replaces all missing values for nominal, string, numeric and date
            attributes in the dataset with user-supplied constant
            values.<br/><br/>Options:<br/><br/>-A &lt;index1,index2-index4,... | att-name1,att-name2,...&gt; =
            	Specify list of attributes to replace missing values for <br/>	(as weka range
            list of indices or a comma separated list of attribute
            names).<br/>	(default: consider all attributes)<br/>-N = 	Specify the replacement constant for
            nominal/string attributes<br/>-R = 	Specify the replacement constant for
            numeric attributes<br/>	(default: 0)<br/>-D = 	Specify the replacement
            constant for date attributes<br/>-F = 	Specify the date format for parsing the
            replacement date constant<br/>	(default:
            yyyy-MM-dd'T'HH:mm:ss)<br/>-unset-class-temporarily = 	Unsets the class index temporarily before the filter
            is<br/>	applied to the data.<br/>	(default: no)
            </summary>
        </member>
        <member name="M:Ml2.Fltr.ReplaceMissingWithUserConstant.Attributes(System.String)">
            <summary>
            Specify range of attributes to act on. This is a comma separated list of
            attribute indices, with "first" and "last" valid values. Specify an
            inclusive range with "-". E.g: "first-3,5,6-10,last". Can alternatively specify a
            comma separated list of attribute names. Note that you can't mix indices
            and attribute names in the same list
            </summary>    
        </member>
        <member name="M:Ml2.Fltr.ReplaceMissingWithUserConstant.NominalStringReplacementValue(System.String)">
            <summary>
            The constant to replace missing values in nominal/string attributes with
            </summary>    
        </member>
        <member name="M:Ml2.Fltr.ReplaceMissingWithUserConstant.NumericReplacementValue(System.String)">
            <summary>
            The constant to replace missing values in numeric attributes with
            </summary>    
        </member>
        <member name="M:Ml2.Fltr.ReplaceMissingWithUserConstant.DateReplacementValue(System.String)">
            <summary>
            The constant to replace missing values in date attributes with
            </summary>    
        </member>
        <member name="M:Ml2.Fltr.ReplaceMissingWithUserConstant.DateFormat(System.String)">
            <summary>
            The formatting string to use for parsing the date replacement value
            </summary>    
        </member>
        <member name="M:Ml2.Fltr.ReplaceMissingWithUserConstant.Environment(weka.core.Environment)">
            <summary>
            
            </summary>    
        </member>
        <member name="M:Ml2.Fltr.ReplaceMissingWithUserConstant.IgnoreClass(System.Boolean)">
            <summary>
            The class index will be unset temporarily before the filter is applied.
            </summary>    
        </member>
        <member name="T:Ml2.Fltr.Resample">
            <summary>
            Produces a random subsample of a dataset using either sampling with
            replacement or without replacement. The original dataset must fit entirely in
            memory. The number of instances in the generated dataset may be specified.
            When used in batch mode, subsequent batches are NOT
            resampled.<br/><br/>Options:<br/><br/>-S &lt;num&gt; = 	Specify the random number seed (default
            1)<br/>-Z &lt;num&gt; = 	The size of the output dataset, as a percentage
            of<br/>	the input dataset (default 100)<br/>-no-replacement = 	Disables
            replacement of instances<br/>	(default: with replacement)<br/>-V = 	Inverts the
            selection - only available with '-no-replacement'.
            </summary>
        </member>
        <member name="M:Ml2.Fltr.Resample.SampleSizePercent(System.Double)">
            <summary>
            Size of the subsample as a percentage of the original dataset.
            </summary>    
        </member>
        <member name="M:Ml2.Fltr.Resample.NoReplacement(System.Boolean)">
            <summary>
            Disables the replacement of instances.
            </summary>    
        </member>
        <member name="M:Ml2.Fltr.Resample.InvertSelection(System.Boolean)">
            <summary>
            Inverts the selection (only if instances are drawn WITHOUT replacement).
            </summary>    
        </member>
        <member name="T:Ml2.Fltr.ReservoirSample">
            <summary>
            Produces a random subsample of a dataset using the reservoir sampling
            Algorithm "R" by Vitter. The original data set does not have to fit into main
            memory, but the reservoir does. <br/><br/>Options:<br/><br/>-S &lt;num&gt;
            = 	Specify the random number seed (default 1)<br/>-Z &lt;num&gt; = 	The
            size of the output dataset - number of instances<br/>	(default 100)
            </summary>
        </member>
        <member name="M:Ml2.Fltr.ReservoirSample.SampleSize(System.Int32)">
            <summary>
            Size of the subsample (reservoir). i.e. the number of instances.
            </summary>    
        </member>
        <member name="T:Ml2.Fltr.SortLabels">
            <summary>
            A simple filter for sorting the labels of nominal
            attributes.<br/><br/>Options:<br/><br/>-D = 	Turns on output of debugging information.<br/>-R
            &lt;index1,index2-index4,...&gt; = 	Specify list of string attributes to
            convert to words.<br/>	(default: select all relational attributes)<br/>-V =
            	Inverts the matching sense of the selection.<br/>-S &lt;CASE|NON-CASE&gt; =
            	Determines the type of sorting:<br/>	CASE = Case-sensitive<br/>	NON-CASE =
            Case-insensitive<br/>	(default: CASE)
            </summary>
        </member>
        <member name="M:Ml2.Fltr.SortLabels.AttributeIndices(System.String)">
            <summary>
            Specify range of attributes to act on; this is a comma separated list of
            attribute indices, with "first" and "last" valid values; Specify an
            inclusive range with "-"; eg: "first-3,5,6-10,last".
            </summary>    
        </member>
        <member name="M:Ml2.Fltr.SortLabels.InvertSelection(System.Boolean)">
            <summary>
            Set attribute selection mode. If false, only selected attributes in the
            range will be worked on; if true, only non-selected attributes will be
            processed.
            </summary>    
        </member>
        <member name="M:Ml2.Fltr.SortLabels.SortType(Ml2.Fltr.SortLabels.ESortType)">
            <summary>
            The type of sorting to use.
            </summary>    
        </member>
        <member name="M:Ml2.Fltr.SortLabels.Debug(System.Boolean)">
            <summary>
            Turns on output of debugging information.
            </summary>    
        </member>
        <member name="T:Ml2.Fltr.SparseToNonSparse">
            <summary>
            An instance filter that converts all incoming sparse instances into
            non-sparse format.
            </summary>
        </member>
        <member name="T:Ml2.Fltr.SpreadSubsample">
            <summary>
            Produces a random subsample of a dataset. The original dataset must fit
            entirely in memory. This filter allows you to specify the maximum "spread"
            between the rarest and most common class. For example, you may specify that
            there be at most a 2:1 difference in class frequencies. When used in batch
            mode, subsequent batches are NOT resampled.<br/><br/>Options:<br/><br/>-S
            &lt;num&gt; = 	Specify the random number seed (default 1)<br/>-M &lt;num&gt;
            = 	The maximum class distribution spread.<br/>	0 = no maximum spread, 1 =
            uniform distribution, 10 = allow at most<br/>	a 10:1 ratio between the
            classes (default 0)<br/>-W = 	Adjust weights so that total weight per class is
            maintained.<br/>	Individual instance weighting is not preserved. (default
            no<br/>	weights adjustment<br/>-X &lt;num&gt; = 	The maximum count for any
            class value (default 0 = unlimited).<br/>
            </summary>
        </member>
        <member name="M:Ml2.Fltr.SpreadSubsample.DistributionSpread(System.Double)">
            <summary>
            The maximum class distribution spread. (0 = no maximum spread, 1 =
            uniform distribution, 10 = allow at most a 10:1 ratio between the classes).
            </summary>    
        </member>
        <member name="M:Ml2.Fltr.SpreadSubsample.MaxCount(System.Double)">
            <summary>
            The maximum count for any class value (0 = unlimited).
            </summary>    
        </member>
        <member name="M:Ml2.Fltr.SpreadSubsample.AdjustWeights(System.Boolean)">
            <summary>
            Wether instance weights will be adjusted to maintain total weight per
            class.
            </summary>    
        </member>
        <member name="T:Ml2.Fltr.Standardize">
            <summary>
            Standardizes all numeric attributes in the given dataset to have zero
            mean and unit variance (apart from the class attribute, if
            set).<br/><br/>Options:<br/><br/>-unset-class-temporarily = 	Unsets the class index
            temporarily before the filter is<br/>	applied to the data.<br/>	(default: no)
            </summary>
        </member>
        <member name="M:Ml2.Fltr.Standardize.IgnoreClass(System.Boolean)">
            <summary>
            The class index will be unset temporarily before the filter is applied.
            </summary>    
        </member>
        <member name="T:Ml2.Fltr.StratifiedRemoveFolds">
            <summary>
            This filter takes a dataset and outputs a specified fold for cross
            validation. If you do not want the folds to be stratified use the unsupervised
            version.<br/><br/>Options:<br/><br/>-V = 	Specifies if inverse of selection
            is to be output.<br/><br/>-N &lt;number of folds&gt; = 	Specifies number of
            folds dataset is split into. <br/>	(default 10)<br/><br/>-F &lt;fold&gt; =
            	Specifies which fold is selected. (default 1)<br/><br/>-S &lt;seed&gt; =
            	Specifies random number seed. (default 0, no randomizing)<br/>
            </summary>
        </member>
        <member name="M:Ml2.Fltr.StratifiedRemoveFolds.InvertSelection(System.Boolean)">
            <summary>
            Whether to invert the selection.
            </summary>    
        </member>
        <member name="M:Ml2.Fltr.StratifiedRemoveFolds.NumFolds(System.Int32)">
            <summary>
            The number of folds to split the dataset into.
            </summary>    
        </member>
        <member name="M:Ml2.Fltr.StratifiedRemoveFolds.Fold(System.Int32)">
            <summary>
            The fold which is selected.
            </summary>    
        </member>
        <member name="T:Ml2.Fltr.StringToNominal">
            <summary>
            Converts a range of string attributes (unspecified number of values) to
            nominal (set number of values). You should ensure that all string values
            that will appear are represented in the first batch of the
            data.<br/><br/>Options:<br/><br/>-R &lt;col&gt; = 	Sets the range of attribute indices
            (default last).<br/>-V &lt;col&gt; = 	Invert the range specified by -R.
            </summary>
        </member>
        <member name="M:Ml2.Fltr.StringToNominal.AttributeRange(System.Int32[])">
            <summary>
            Sets which attributes to process. This attributes must be string
            attributes ("first" and "last" are valid values as well as ranges and lists)
            </summary>    
        </member>
        <member name="T:Ml2.Fltr.StringToWordVector">
            <summary>
            Converts String attributes into a set of attributes representing word
            occurrence (depending on the tokenizer) information from the text contained in
            the strings. The set of words (attributes) is determined by the first
            batch filtered (typically training data).<br/><br/>Options:<br/><br/>-C =
            	Output word counts rather than boolean word presence.<br/><br/>-R
            &lt;index1,index2-index4,...&gt; = 	Specify list of string attributes to convert to
            words (as weka Range).<br/>	(default: select all string attributes)<br/>-V =
            	Invert matching sense of column indexes.<br/>-P &lt;attribute name
            prefix&gt; = 	Specify a prefix for the created attribute names.<br/>	(default:
            "")<br/>-W &lt;number of words to keep&gt; = 	Specify approximate number of word
            fields to create.<br/>	Surplus words will be discarded..<br/>	(default:
            1000)<br/>-prune-rate &lt;rate as a percentage of dataset&gt; = 	Specify the
            rate (e.g., every 10% of the input dataset) at which to periodically prune
            the dictionary.<br/>	-W prunes after creating a full dictionary. You may
            not have enough memory for this approach.<br/>	(default: no periodic
            pruning)<br/>-T = 	Transform the word frequencies into log(1+fij)<br/>	where fij is
            the frequency of word i in jth document(instance).<br/><br/>-I =
            	Transform each word frequency into:<br/>	fij*log(num of Documents/num of documents
            containing word i)<br/>	 where fij if frequency of word i in jth
            document(instance)<br/>-N = 	Whether to 0=not normalize/1=normalize all
            data/2=normalize test data only<br/>	to average length of training documents (default
            0=don't normalize).<br/>-L = 	Convert all tokens to lowercase before adding
            to the dictionary.<br/>-S = 	Ignore words that are in the
            stoplist.<br/>-stemmer &lt;spec&gt; = 	The stemmering algorihtm (classname plus parameters)
            to use.<br/>-M &lt;int&gt; = 	The minimum term frequency (default =
            1).<br/>-O = 	If this is set, the maximum number of words and the <br/>	minimum
            term frequency is not enforced on a per-class <br/>	basis but based on the
            documents in all the classes <br/>	(even if a class attribute is
            set).<br/>-stopwords &lt;file&gt; = 	A file containing stopwords to override the default
            ones.<br/>	Using this option automatically sets the flag ('-S') to use
            the<br/>	stoplist if the file exists.<br/>	Format: one stopword per line,
            lines starting with '#'<br/>	are interpreted as comments and
            ignored.<br/>-tokenizer &lt;spec&gt; = 	The tokenizing algorihtm (classname plus parameters)
            to use.<br/>	(default: weka.core.tokenizers.WordTokenizer)
            </summary>
        </member>
        <member name="M:Ml2.Fltr.StringToWordVector.SelectedRange(System.String)">
            <summary>
            
            </summary>    
        </member>
        <member name="M:Ml2.Fltr.StringToWordVector.InvertSelection(System.Boolean)">
            <summary>
            Set attribute selection mode. If false, only selected attributes in the
            range will be worked on; if true, only non-selected attributes will be
            processed.
            </summary>    
        </member>
        <member name="M:Ml2.Fltr.StringToWordVector.AttributeNamePrefix(System.String)">
            <summary>
            Prefix for the created attribute names. (default: "")
            </summary>    
        </member>
        <member name="M:Ml2.Fltr.StringToWordVector.WordsToKeep(System.Int32)">
            <summary>
            The number of words (per class if there is a class attribute assigned) to
            attempt to keep.
            </summary>    
        </member>
        <member name="M:Ml2.Fltr.StringToWordVector.PeriodicPruning(System.Double)">
            <summary>
            Specify the rate (x% of the input dataset) at which to periodically prune
            the dictionary. wordsToKeep prunes after creating a full dictionary. You
            may not have enough memory for this approach.
            </summary>    
        </member>
        <member name="M:Ml2.Fltr.StringToWordVector.MinTermFreq(System.Int32)">
            <summary>
            Sets the minimum term frequency. This is enforced on a per-class basis.
            </summary>    
        </member>
        <member name="M:Ml2.Fltr.StringToWordVector.OutputWordCounts(System.Boolean)">
            <summary>
            Output word counts rather than boolean 0 or 1(indicating presence or
            absence of a word).
            </summary>    
        </member>
        <member name="M:Ml2.Fltr.StringToWordVector.TFTransform(System.Boolean)">
            <summary>
            
            </summary>    
        </member>
        <member name="M:Ml2.Fltr.StringToWordVector.IDFTransform(System.Boolean)">
            <summary>
            
            </summary>    
        </member>
        <member name="M:Ml2.Fltr.StringToWordVector.DoNotOperateOnPerClassBasis(System.Boolean)">
            <summary>
            If this is set, the maximum number of words and the minimum term
            frequency is not enforced on a per-class basis but based on the documents in all
            the classes (even if a class attribute is set).
            </summary>    
        </member>
        <member name="M:Ml2.Fltr.StringToWordVector.NormalizeDocLength(Ml2.Fltr.StringToWordVector.ENormalizeDocLength)">
            <summary>
            Sets whether if the word frequencies for a document (instance) should be
            normalized or not.
            </summary>    
        </member>
        <member name="M:Ml2.Fltr.StringToWordVector.LowerCaseTokens(System.Boolean)">
            <summary>
            If set then all the word tokens are converted to lower case before being
            added to the dictionary.
            </summary>    
        </member>
        <member name="M:Ml2.Fltr.StringToWordVector.UseStoplist(System.Boolean)">
            <summary>
            Ignores all the words that are on the stoplist, if set to true.
            </summary>    
        </member>
        <member name="M:Ml2.Fltr.StringToWordVector.Stemmer(weka.core.stemmers.Stemmer)">
            <summary>
            The stemming algorithm to use on the words.
            </summary>    
        </member>
        <member name="M:Ml2.Fltr.StringToWordVector.Tokenizer(weka.core.tokenizers.Tokenizer)">
            <summary>
            The tokenizing algorithm to use on the strings.
            </summary>    
        </member>
        <member name="M:Ml2.Fltr.StringToWordVector.AttributeIndices(System.String)">
            <summary>
            Specify range of attributes to act on. This is a comma separated list of
            attribute indices, with "first" and "last" valid values. Specify an
            inclusive range with "-". E.g: "first-3,5,6-10,last".
            </summary>    
        </member>
        <member name="M:Ml2.Fltr.StringToWordVector.AttributeIndicesArray(System.Int32[])">
            <summary>
            
            </summary>    
        </member>
        <!-- Badly formed XML comment ignored for member "T:Ml2.Fltr.SubsetByExpression" -->
        <member name="M:Ml2.Fltr.SubsetByExpression.Expression(System.String)">
            <summary>
            The expression to used for filtering the dataset.
            </summary>    
        </member>
        <member name="M:Ml2.Fltr.SubsetByExpression.FilterAfterFirstBatch(System.Boolean)">
            <summary>
            Whether to apply the filtering process to instances that are input after
            the first (training) batch. The default is false so that, when used in a
            FilteredClassifier, test instances do not potentially get 'consumed' by the
            filter an a prediction is always made.
            </summary>    
        </member>
        <member name="M:Ml2.Fltr.SubsetByExpression.Debug(System.Boolean)">
            <summary>
            Turns on output of debugging information.
            </summary>    
        </member>
        <member name="M:Ml2.Fltr.SupervisedAttributeFilters.AddClassification">
            <summary>
            A filter for adding the classification, the class distribution and an
            error flag to a dataset with a classifier. The classifier is either trained on
            the data itself or provided as serialized
            model.<br/><br/>Options:<br/><br/>-D = 	Turns on output of debugging information.<br/>-W &lt;classifier
            specification&gt; = 	Full class name of classifier to use, followed<br/>	by
            scheme options. eg:<br/>		"weka.classifiers.bayes.NaiveBayes
            -D"<br/>	(default: weka.classifiers.rules.ZeroR)<br/>-serialized &lt;file&gt; = 	Instead of
            training a classifier on the data, one can also provide<br/>	a serialized
            model and use that for tagging the data.<br/>-classification = 	Adds an
            attribute with the actual classification.<br/>	(default:
            off)<br/>-remove-old-class = 	Removes the old class attribute.<br/>	(default:
            off)<br/>-distribution = 	Adds attributes with the distribution for all classes <br/>	(for
            numeric classes this will be identical to the attribute <br/>	output with
            '-classification').<br/>	(default: off)<br/>-error = 	Adds an attribute
            indicating whether the classifier output <br/>	a wrong classification (for
            numeric classes this is the numeric <br/>	difference).<br/>	(default: off)
            </summary>
        </member>
        <member name="M:Ml2.Fltr.SupervisedAttributeFilters.AttributeSelection">
            <summary>
            A supervised attribute filter that can be used to select attributes. It
            is very flexible and allows various search and evaluation methods to be
            combined.<br/><br/>Options:<br/><br/>-S &lt;"Name of search class [search
            options]"&gt; = 	Sets search method for subset evaluators.<br/>	eg. -S
            "weka.attributeSelection.BestFirst -S 8"<br/>-E &lt;"Name of attribute/subset
            evaluation class [evaluator options]"&gt; = 	Sets attribute/subset
            evaluator.<br/>	eg. -E "weka.attributeSelection.CfsSubsetEval -L"<br/><br/>Options
            specific to evaluator weka.attributeSelection.CfsSubsetEval: = <br/>-M = 	Treat
            missing values as a separate value.<br/>-L = 	Don't include locally
            predictive attributes.<br/><br/>Options specific to search
            weka.attributeSelection.BestFirst: = <br/>-P &lt;start set&gt; = 	Specify a starting set of
            attributes.<br/>	Eg. 1,3,5-7.<br/>-D &lt;0 = backward | 1 = forward | 2 =
            bi-directional&gt; = 	Direction of search. (default = 1).<br/>-N &lt;num&gt; =
            	Number of non-improving nodes to<br/>	consider before terminating
            search.<br/>-S &lt;num&gt; = 	Size of lookup cache for evaluated
            subsets.<br/>	Expressed as a multiple of the number of<br/>	attributes in the data set. (default
            = 1)
            </summary>
        </member>
        <member name="M:Ml2.Fltr.SupervisedAttributeFilters.ClassOrder">
            <summary>
            Changes the order of the classes so that the class values are no longer
            of in the order specified in the header. The values will be in the order
            specified by the user -- it could be either in ascending/descending order by
            the class frequency or in random order. Note that this filter currently does
            not change the header, only the class values of the instances, so there is
            not much point in using it in conjunction with the FilteredClassifier. The
            value can also be converted back using 'originalValue(double value)'
            procedure.<br/><br/>Options:<br/><br/>-R &lt;seed&gt; = 	Specify the seed of
            randomization<br/>	used to randomize the class<br/>	order (default: 1)<br/>-C
            &lt;order&gt; = 	Specify the class order to be<br/>	sorted, could be 0:
            ascending<br/>	1: descending and 2: random.(default: 0)
            </summary>
        </member>
        <!-- Badly formed XML comment ignored for member "M:Ml2.Fltr.SupervisedAttributeFilters.Discretize" -->
        <member name="M:Ml2.Fltr.SupervisedAttributeFilters.NominalToBinary">
            <summary>
            Converts all nominal attributes into binary numeric attributes. An
            attribute with k values is transformed into k binary attributes if the class is
            nominal (using the one-attribute-per-value approach). Binary attributes are
            left binary, if option '-A' is not given.If the class is numeric, k - 1 new
            binary attributes are generated in the manner described in "Classification
            and Regression Trees" by Breiman et al. (i.e. taking the average class
            value associated with each attribute value into account)<br/><br/>For more
            information, see:<br/><br/>L. Breiman, J.H. Friedman, R.A. Olshen, C.J. Stone
            (1984). Classification and Regression Trees. Wadsworth
            Inc.<br/><br/>Options:<br/><br/>-N = 	Sets if binary attributes are to be coded as nominal
            ones.<br/>-A = 	For each nominal value a new attribute is created, <br/>	not
            only if there are more than 2 values.
            </summary>
        </member>
        <member name="M:Ml2.Fltr.SupervisedInstanceFilters.Resample">
            <summary>
            Produces a random subsample of a dataset using either sampling with
            replacement or without replacement.<br/>The original dataset must fit entirely
            in memory. The number of instances in the generated dataset may be
            specified. The dataset must have a nominal class attribute. If not, use the
            unsupervised version. The filter can be made to maintain the class distribution in
            the subsample, or to bias the class distribution toward a uniform
            distribution. When used in batch mode (i.e. in the FilteredClassifier), subsequent
            batches are NOT resampled.<br/><br/>Options:<br/><br/>-S &lt;num&gt; =
            	Specify the random number seed (default 1)<br/>-Z &lt;num&gt; = 	The size of
            the output dataset, as a percentage of<br/>	the input dataset (default
            100)<br/>-B &lt;num&gt; = 	Bias factor towards uniform class distribution.<br/>	0
            = distribution in input data -- 1 = uniform distribution.<br/>	(default
            0)<br/>-no-replacement = 	Disables replacement of instances<br/>	(default:
            with replacement)<br/>-V = 	Inverts the selection - only available with
            '-no-replacement'.
            </summary>
        </member>
        <member name="M:Ml2.Fltr.SupervisedInstanceFilters.SpreadSubsample">
            <summary>
            Produces a random subsample of a dataset. The original dataset must fit
            entirely in memory. This filter allows you to specify the maximum "spread"
            between the rarest and most common class. For example, you may specify that
            there be at most a 2:1 difference in class frequencies. When used in batch
            mode, subsequent batches are NOT resampled.<br/><br/>Options:<br/><br/>-S
            &lt;num&gt; = 	Specify the random number seed (default 1)<br/>-M &lt;num&gt;
            = 	The maximum class distribution spread.<br/>	0 = no maximum spread, 1 =
            uniform distribution, 10 = allow at most<br/>	a 10:1 ratio between the
            classes (default 0)<br/>-W = 	Adjust weights so that total weight per class is
            maintained.<br/>	Individual instance weighting is not preserved. (default
            no<br/>	weights adjustment<br/>-X &lt;num&gt; = 	The maximum count for any
            class value (default 0 = unlimited).<br/>
            </summary>
        </member>
        <member name="M:Ml2.Fltr.SupervisedInstanceFilters.StratifiedRemoveFolds">
            <summary>
            This filter takes a dataset and outputs a specified fold for cross
            validation. If you do not want the folds to be stratified use the unsupervised
            version.<br/><br/>Options:<br/><br/>-V = 	Specifies if inverse of selection
            is to be output.<br/><br/>-N &lt;number of folds&gt; = 	Specifies number of
            folds dataset is split into. <br/>	(default 10)<br/><br/>-F &lt;fold&gt; =
            	Specifies which fold is selected. (default 1)<br/><br/>-S &lt;seed&gt; =
            	Specifies random number seed. (default 0, no randomizing)<br/>
            </summary>
        </member>
        <member name="T:Ml2.Fltr.SwapValues">
            <summary>
            Swaps two values of a nominal attribute.<br/><br/>Options:<br/><br/>-C
            &lt;col&gt; = 	Sets the attribute index (default last).<br/>-F &lt;value
            index&gt; = 	Sets the first value's index (default first).<br/>-S &lt;value
            index&gt; = 	Sets the second value's index (default last).
            </summary>
        </member>
        <member name="M:Ml2.Fltr.SwapValues.AttributeIndex(System.String)">
            <summary>
            Sets which attribute to process. This attribute must be nominal ("first"
            and "last" are valid values)
            </summary>    
        </member>
        <member name="M:Ml2.Fltr.SwapValues.FirstValueIndex(System.String)">
            <summary>
            The index of the first value.("first" and "last" are valid values)
            </summary>    
        </member>
        <member name="M:Ml2.Fltr.SwapValues.SecondValueIndex(System.String)">
            <summary>
            The index of the second value.("first" and "last" are valid values)
            </summary>    
        </member>
        <member name="T:Ml2.Fltr.TimeSeriesDelta">
            <summary>
            An instance filter that assumes instances form time-series data and
            replaces attribute values in the current instance with the difference between
            the current value and the equivalent attribute attribute value of some
            previous (or future) instance. For instances where the time-shifted value is
            unknown either the instance may be dropped, or missing values used. Skips the
            class attribute if it is set.<br/><br/>Options:<br/><br/>-R
            &lt;index1,index2-index4,...&gt; = 	Specify list of columns to translate in time. First
            and<br/>	last are valid indexes. (default none)<br/>-V = 	Invert matching
            sense (i.e. calculate for all non-specified columns)<br/>-I &lt;num&gt; = 	The
            number of instances forward to translate values<br/>	between. A negative
            number indicates taking values from<br/>	a past instance. (default -1)<br/>-M
            = 	For instances at the beginning or end of the dataset where<br/>	the
            translated values are not known, remove those instances<br/>	(default is to
            use missing values).
            </summary>
        </member>
        <member name="M:Ml2.Fltr.TimeSeriesDelta.AttributeIndices(System.String)">
            <summary>
            Specify range of attributes to act on. This is a comma separated list of
            attribute indices, with "first" and "last" valid values. Specify an
            inclusive range with "-". E.g: "first-3,5,6-10,last".
            </summary>    
        </member>
        <member name="M:Ml2.Fltr.TimeSeriesDelta.InvertSelection(System.Boolean)">
            <summary>
            Invert matching sense. ie calculate for all non-specified columns.
            </summary>    
        </member>
        <member name="M:Ml2.Fltr.TimeSeriesDelta.FillWithMissing(System.Boolean)">
            <summary>
            For instances at the beginning or end of the dataset where the translated
            values are not known, use missing values (default is to remove those
            instances)
            </summary>    
        </member>
        <member name="M:Ml2.Fltr.TimeSeriesDelta.InstanceRange(System.Int32)">
            <summary>
            The number of instances forward/backward to merge values between. A
            negative number indicates taking values from a past instance.
            </summary>    
        </member>
        <member name="M:Ml2.Fltr.TimeSeriesDelta.AttributeIndicesArray(System.Int32[])">
            <summary>
            
            </summary>    
        </member>
        <member name="T:Ml2.Fltr.TimeSeriesTranslate">
            <summary>
            An instance filter that assumes instances form time-series data and
            replaces attribute values in the current instance with the equivalent attribute
            values of some previous (or future) instance. For instances where the
            desired value is unknown either the instance may be dropped, or missing values
            used. Skips the class attribute if it is set.<br/><br/>Options:<br/><br/>-R
            &lt;index1,index2-index4,...&gt; = 	Specify list of columns to translate in
            time. First and<br/>	last are valid indexes. (default none)<br/>-V =
            	Invert matching sense (i.e. calculate for all non-specified columns)<br/>-I
            &lt;num&gt; = 	The number of instances forward to translate
            values<br/>	between. A negative number indicates taking values from<br/>	a past instance.
            (default -1)<br/>-M = 	For instances at the beginning or end of the dataset
            where<br/>	the translated values are not known, remove those
            instances<br/>	(default is to use missing values).
            </summary>
        </member>
        <member name="M:Ml2.Fltr.TimeSeriesTranslate.AttributeIndices(System.String)">
            <summary>
            Specify range of attributes to act on. This is a comma separated list of
            attribute indices, with "first" and "last" valid values. Specify an
            inclusive range with "-". E.g: "first-3,5,6-10,last".
            </summary>    
        </member>
        <member name="M:Ml2.Fltr.TimeSeriesTranslate.InvertSelection(System.Boolean)">
            <summary>
            Invert matching sense. ie calculate for all non-specified columns.
            </summary>    
        </member>
        <member name="M:Ml2.Fltr.TimeSeriesTranslate.FillWithMissing(System.Boolean)">
            <summary>
            For instances at the beginning or end of the dataset where the translated
            values are not known, use missing values (default is to remove those
            instances)
            </summary>    
        </member>
        <member name="M:Ml2.Fltr.TimeSeriesTranslate.InstanceRange(System.Int32)">
            <summary>
            The number of instances forward/backward to merge values between. A
            negative number indicates taking values from a past instance.
            </summary>    
        </member>
        <member name="M:Ml2.Fltr.TimeSeriesTranslate.AttributeIndicesArray(System.Int32[])">
            <summary>
            
            </summary>    
        </member>
        <member name="M:Ml2.Fltr.UnsupervisedAttributeFilters.Add">
            <summary>
            An instance filter that adds a new attribute to the dataset. The new
            attribute will contain all missing values.<br/><br/>Options:<br/><br/>-T
            &lt;NUM|NOM|STR|DAT&gt; = 	The type of attribute to create:<br/>	NUM = Numeric
            attribute<br/>	NOM = Nominal attribute<br/>	STR = String attribute<br/>	DAT =
            Date attribute<br/>	(default: NUM)<br/>-C &lt;index&gt; = 	Specify where
            to insert the column. First and last<br/>	are valid indexes.(default:
            last)<br/>-N &lt;name&gt; = 	Name of the new attribute.<br/>	(default:
            'Unnamed')<br/>-L &lt;label1,label2,...&gt; = 	Create nominal attribute with given
            labels<br/>	(default: numeric attribute)<br/>-F &lt;format&gt; = 	The format
            of the date values (see ISO-8601)<br/>	(default: yyyy-MM-dd'T'HH:mm:ss)
            </summary>
        </member>
        <member name="M:Ml2.Fltr.UnsupervisedAttributeFilters.AddCluster">
            <summary>
            A filter that adds a new nominal attribute representing the cluster
            assigned to each instance by the specified clustering algorithm.<br/>Either the
            clustering algorithm gets built with the first batch of data or one
            specifies are serialized clusterer model file to use
            instead.<br/><br/>Options:<br/><br/>-W &lt;clusterer specification&gt; = 	Full class name of clusterer
            to use, followed<br/>	by scheme options.
            eg:<br/>		"weka.clusterers.SimpleKMeans -N 3"<br/>	(default: weka.clusterers.SimpleKMeans)<br/>-serialized
            &lt;file&gt; = 	Instead of building a clusterer on the data, one can also
            provide<br/>	a serialized model and use that for adding the clusters.<br/>-I
            &lt;att1,att2-att4,...&gt; = 	The range of attributes the clusterer should
            ignore.<br/>
            </summary>
        </member>
        <member name="M:Ml2.Fltr.UnsupervisedAttributeFilters.AddExpression">
            <summary>
            An instance filter that creates a new attribute by applying a
            mathematical expression to existing attributes. The expression can contain attribute
            references and numeric constants. Supported operators are :<br/>+, -, *, /,
            ^, log, abs, cos, exp, sqrt, floor, ceil, rint, tan, sin, (,
            )<br/>Attributes are specified by prefixing with 'a', eg. a7 is attribute number 7
            (starting from 1).<br/>Example expression :
            a1^2*a5/log(a7*4.0).<br/><br/>Options:<br/><br/>-E &lt;expression&gt; = 	Specify the expression to apply. Eg
            a1^2*a5/log(a7*4.0).<br/>	Supported opperators: ,+, -, *, /, ^, log, abs, cos,
            <br/>	exp, sqrt, floor, ceil, rint, tan, sin, (, )<br/>	(default:
            a1^2)<br/>-N &lt;name&gt; = 	Specify the name for the new attribute. (default is
            the expression provided with -E)<br/>-D = 	Debug. Names attribute with the
            postfix parse of the expression.
            </summary>
        </member>
        <member name="M:Ml2.Fltr.UnsupervisedAttributeFilters.AddID">
            <summary>
            An instance filter that adds an ID attribute to the dataset. The new
            attribute contains a unique ID for each instance.<br/>Note: The ID is not reset
            for the second batch of files (using -b and -r and
            -s).<br/><br/>Options:<br/><br/>-C &lt;index&gt; = 	Specify where to insert the ID. First and
            last<br/>	are valid indexes.(default first)<br/>-N &lt;name&gt; = 	Name of the
            new attribute.<br/>	(default = 'ID')
            </summary>
        </member>
        <member name="M:Ml2.Fltr.UnsupervisedAttributeFilters.AddNoise">
            <summary>
            An instance filter that changes a percentage of a given attributes
            values. The attribute must be nominal. Missing value can be treated as value
            itself.<br/><br/>Options:<br/><br/>-C &lt;col&gt; = 	Index of the attribute to
            be changed <br/>	(default last attribute)<br/>-M = 	Treat missing values as
            an extra value <br/><br/>-P &lt;num&gt; = 	Specify the percentage of noise
            introduced <br/>	to the data (default 10)<br/>-S &lt;num&gt; = 	Specify
            the random number seed (default 1)
            </summary>
        </member>
        <member name="M:Ml2.Fltr.UnsupervisedAttributeFilters.AddUserFields">
            <summary>
            A filter that adds new attributes with user specified type and constant
            value. Numeric, nominal, string and date attributes can be created.
            Attribute name, and value can be set with environment variables. Date attributes
            can also specify a formatting string by which to parse the supplied date
            value. Alternatively, a current time stamp can be specified by supplying the
            special string "now" as the value for a date
            attribute.<br/><br/>Options:<br/><br/>-A &lt;name:type:value&gt; = 	New field specification
            (name@type@value).<br/>	 Environment variables may be used for any/all parts of
            the<br/>	specification. Type can be one of (numeric, nominal, string or
            date).<br/>	The value for date be a specific date string or the special
            string<br/>	"now" to indicate the current date-time. A specific date format<br/>	string for
            parsing specific date values can be specified by suffixing<br/>	the type
            specification - e.g. "myTime@date:MM-dd-yyyy@08-23-2009".This option may be
            specified multiple times
            </summary>
        </member>
        <member name="M:Ml2.Fltr.UnsupervisedAttributeFilters.AddValues">
            <summary>
            Adds the labels from the given list to an attribute if they are missing.
            The labels can also be sorted in an ascending manner. If no labels are
            provided then only the (optional) sorting
            applies.<br/><br/>Options:<br/><br/>-C &lt;col&gt; = 	Sets the attribute index<br/>	(default last).<br/>-L
            &lt;label1,label2,...&gt; = 	Comma-separated list of labels to
            add.<br/>	(default: none)<br/>-S = 	Turns on the sorting of the labels.
            </summary>
        </member>
        <member name="M:Ml2.Fltr.UnsupervisedAttributeFilters.Center">
            <summary>
            Centers all numeric attributes in the given dataset to have zero mean
            (apart from the class attribute, if
            set).<br/><br/>Options:<br/><br/>-unset-class-temporarily = 	Unsets the class index temporarily before the filter
            is<br/>	applied to the data.<br/>	(default: no)
            </summary>
        </member>
        <member name="M:Ml2.Fltr.UnsupervisedAttributeFilters.ChangeDateFormat">
            <summary>
            Changes the date format used by a date attribute. This is most useful for
            converting to a format with less precision, for example, from an absolute
            date to day of year, etc. This changes the format string, and changes the
            date values to those that would be parsed by the new
            format.<br/><br/>Options:<br/><br/>-C &lt;col&gt; = 	Sets the attribute index (default
            last).<br/>-F &lt;value index&gt; = 	Sets the output date format string (default
            corresponds to ISO-8601).
            </summary>
        </member>
        <member name="M:Ml2.Fltr.UnsupervisedAttributeFilters.ClassAssigner">
            <summary>
            Filter that can set and unset the class
            index.<br/><br/>Options:<br/><br/>-D = 	Turns on output of debugging information.<br/>-C
            &lt;num|first|last|0&gt; = 	The index of the class attribute. Index starts with 1,
            'first'<br/>	and 'last' are accepted, '0' unsets the class index.<br/>	(default: last)
            </summary>
        </member>
        <member name="M:Ml2.Fltr.UnsupervisedAttributeFilters.ClusterMembership">
            <summary>
            A filter that uses a density-based clusterer to generate cluster
            membership values; filtered instances are composed of these values plus the class
            attribute (if set in the input data). If a (nominal) class attribute is set,
            the clusterer is run separately for each class. The class attribute (if
            set) and any user-specified attributes are ignored during the clustering
            operation<br/><br/>Options:<br/><br/>-W &lt;clusterer name&gt; = 	Full name of
            clusterer to use. eg:<br/>		weka.clusterers.EM<br/>	Additional options
            after the '--'.<br/>	(default: weka.clusterers.EM)<br/>-I
            &lt;att1,att2-att4,...&gt; = 	The range of attributes the clusterer should ignore.<br/>	(the
            class attribute is automatically ignored)
            </summary>
        </member>
        <member name="M:Ml2.Fltr.UnsupervisedAttributeFilters.Copy">
            <summary>
            An instance filter that copies a range of attributes in the dataset. This
            is used in conjunction with other filters that overwrite attribute values
            during the course of their operation -- this filter allows the original
            attributes to be kept as well as the new
            attributes.<br/><br/>Options:<br/><br/>-R &lt;index1,index2-index4,...&gt; = 	Specify list of columns to copy.
            First and last are valid<br/>	indexes. (default none)<br/>-V = 	Invert
            matching sense (i.e. copy all non-specified columns)
            </summary>
        </member>
        <member name="M:Ml2.Fltr.UnsupervisedAttributeFilters.Discretize">
            <summary>
            An instance filter that discretizes a range of numeric attributes in the
            dataset into nominal attributes. Discretization is by simple binning. Skips
            the class attribute if
            set.<br/><br/>Options:<br/><br/>-unset-class-temporarily = 	Unsets the class index temporarily before the filter
            is<br/>	applied to the data.<br/>	(default: no)<br/>-B &lt;num&gt; = 	Specifies the
            (maximum) number of bins to divide numeric attributes into.<br/>	(default =
            10)<br/>-M &lt;num&gt; = 	Specifies the desired weight of instances per bin
            for<br/>	equal-frequency binning. If this is set to a positive<br/>	number
            then the -B option will be ignored.<br/>	(default = -1)<br/>-F = 	Use
            equal-frequency instead of equal-width discretization.<br/>-O = 	Optimize number
            of bins using leave-one-out estimate<br/>	of estimated entropy (for
            equal-width discretization).<br/>	If this is set then the -B option will be
            ignored.<br/>-R &lt;col1,col2-col4,...&gt; = 	Specifies list of columns to
            Discretize. First and last are valid indexes.<br/>	(default: first-last)<br/>-V =
            	Invert matching sense of column indexes.<br/>-D = 	Output binary
            attributes for discretized attributes.<br/>-Y = 	Use bin numbers rather than ranges
            for discretized attributes.
            </summary>
        </member>
        <member name="M:Ml2.Fltr.UnsupervisedAttributeFilters.FirstOrder">
            <summary>
            This instance filter takes a range of N numeric attributes and replaces
            them with N-1 numeric attributes, the values of which are the difference
            between consecutive attribute values from the original instance. eg:
            <br/><br/>Original attribute values<br/><br/> 0.1, 0.2, 0.3, 0.1, 0.3<br/><br/>New
            attribute values<br/><br/> 0.1, 0.1, -0.2, 0.2<br/><br/>The range of
            attributes used is taken in numeric order. That is, a range spec of 7-11,3-5 will
            use the attribute ordering 3,4,5,7,8,9,10,11 for the differences, NOT
            7,8,9,10,11,3,4,5.<br/><br/>Options:<br/><br/>-R
            &lt;index1,index2-index4,...&gt; = 	Specify list of columns to take the differences between.<br/>	First
            and last are valid indexes.<br/>	(default none)
            </summary>
        </member>
        <!-- Badly formed XML comment ignored for member "M:Ml2.Fltr.UnsupervisedAttributeFilters.InterquartileRange" -->
        <member name="M:Ml2.Fltr.UnsupervisedAttributeFilters.KernelFilter">
            <summary>
            Converts the given set of predictor variables into a kernel matrix. The
            class value remains unchangedm, as long as the preprocessing filter doesn't
            change it.<br/>By default, the data is preprocessed with the Center filter,
            but the user can choose any filter (NB: one must be careful that the
            filter does not alter the class attribute unintentionally). With
            weka.filters.AllFilter the preprocessing gets disabled.<br/><br/>For more information
            regarding preprocessing the data, see:<br/><br/>K.P. Bennett, M.J. Embrechts:
            An Optimization Perspective on Kernel Partial Least Squares Regression. In:
            Advances in Learning Theory: Methods, Models and Applications, 227-249,
            2003.<br/><br/>Options:<br/><br/>-D = 	Turns on output of debugging
            information.<br/>-no-checks = 	Turns off all checks - use with caution!<br/>	Turning
            them off assumes that data is purely numeric, doesn't<br/>	contain any
            missing values, and has a nominal class. Turning them<br/>	off also means that
            no header information will be stored if the<br/>	machine is linear.
            Finally, it also assumes that no instance has<br/>	a weight equal to
            0.<br/>	(default: checks on)<br/>-F &lt;filename&gt; = 	The file to initialize the
            filter with (optional).<br/>-C &lt;num&gt; = 	The class index for the file to
            initialize with,<br/>	First and last are valid (optional, default:
            last).<br/>-K &lt;classname and parameters&gt; = 	The Kernel to use.<br/>	(default:
            weka.classifiers.functions.supportVector.PolyKernel)<br/>-kernel-factor =
            	Defines a factor for the kernel.<br/>		- RBFKernel: a factor for
            gamma<br/>			Standardize: 1/(2*N)<br/>			Normalize..: 6/N<br/>	Available parameters
            are:<br/>		N for # of instances, A for # of attributes<br/>	(default:
            1)<br/>-P &lt;classname and parameters&gt; = 	The Filter used for preprocessing
            (use weka.filters.AllFilter<br/>	to disable preprocessing).<br/>	(default:
            weka.filters.unsupervised.attribute.Center)<br/><br/>Options specific to
            kernel weka.classifiers.functions.supportVector.PolyKernel: = <br/>-D =
            	Enables debugging output (if available) to be printed.<br/>	(default:
            off)<br/>-no-checks = 	Turns off all checks - use with caution!<br/>	(default: checks
            on)<br/>-C &lt;num&gt; = 	The size of the cache (a prime number), 0 for
            full cache and <br/>	-1 to turn it off.<br/>	(default: 250007)<br/>-E
            &lt;num&gt; = 	The Exponent to use.<br/>	(default: 1.0)<br/>-L = 	Use lower-order
            terms.<br/>	(default: no)<br/><br/>Options specific to preprocessing filter
            weka.filters.unsupervised.attribute.Center: = <br/>-unset-class-temporarily
            = 	Unsets the class index temporarily before the filter is<br/>	applied to
            the data.<br/>	(default: no)
            </summary>
        </member>
        <member name="M:Ml2.Fltr.UnsupervisedAttributeFilters.MakeIndicator">
            <summary>
            A filter that creates a new dataset with a boolean attribute replacing a
            nominal attribute. In the new dataset, a value of 1 is assigned to an
            instance that exhibits a particular range of attribute values, a 0 to an
            instance that doesn't. The boolean attribute is coded as numeric by
            default.<br/><br/>Options:<br/><br/>-C &lt;col&gt; = 	Sets the attribute index.<br/>-V
            &lt;index1,index2-index4,...&gt; = 	Specify the list of values to indicate.
            First and last are<br/>	valid indexes (default last)<br/>-N &lt;index&gt; =
            	Set if new boolean attribute nominal.
            </summary>
        </member>
        <member name="M:Ml2.Fltr.UnsupervisedAttributeFilters.MathExpression">
            <summary>
            Modify numeric attributes according to a given expression
            <br/><br/>Options:<br/><br/>-unset-class-temporarily = 	Unsets the class index temporarily
            before the filter is<br/>	applied to the data.<br/>	(default: no)<br/>-E
            &lt;expression&gt; = 	Specify the expression to apply. Eg.
            pow(A,6)/(MEAN+MAX)<br/>	Supported operators are +, -, *, /, pow, log,<br/>	abs, cos, exp,
            sqrt, tan, sin, ceil, floor, rint, (, ), <br/>	MEAN, MAX, MIN, SD, COUNT,
            SUM, SUMSQUARED, ifelse. The 'A'<br/>	letter refers to the value of the
            attribute being processed.<br/>	Other attribute values (numeric only) can be
            accessed through<br/>	the variables A1, A2, A3, ...<br/>-R
            &lt;index1,index2-index4,...&gt; = 	Specify list of columns to ignore. First and last are
            valid<br/>	indexes. (default none)<br/>-V = 	Invert matching sense (i.e. only
            modify specified columns)
            </summary>
        </member>
        <member name="M:Ml2.Fltr.UnsupervisedAttributeFilters.MergeManyValues">
            <summary>
            Merges many values of a nominal attribute into one
            value.<br/><br/>Options:<br/><br/>-C &lt;col&gt; = 	Sets the attribute index<br/>	(default:
            last)<br/>-L &lt;label&gt; = 	Sets the label of the newly merged
            classes<br/>	(default: 'merged')<br/>-R &lt;range&gt; = 	Sets the merge range. 'first and
            'last' are accepted as well.'<br/>	E.g.: first-5,7,9,20-last<br/>	(default:
            1,2)
            </summary>
        </member>
        <member name="M:Ml2.Fltr.UnsupervisedAttributeFilters.MergeTwoValues">
            <summary>
            Merges two values of a nominal attribute into one
            value.<br/><br/>Options:<br/><br/>-C &lt;col&gt; = 	Sets the attribute index (default
            last).<br/>-F &lt;value index&gt; = 	Sets the first value's index (default
            first).<br/>-S &lt;value index&gt; = 	Sets the second value's index (default last).
            </summary>
        </member>
        <member name="M:Ml2.Fltr.UnsupervisedAttributeFilters.NominalToBinary">
            <summary>
            Converts all nominal attributes into binary numeric attributes. An
            attribute with k values is transformed into k binary attributes if the class is
            nominal (using the one-attribute-per-value approach). Binary attributes are
            left binary, if option '-A' is not given.If the class is numeric, you might
            want to use the supervised version of this
            filter.<br/><br/>Options:<br/><br/>-N = 	Sets if binary attributes are to be coded as nominal ones.<br/>-A
            = 	For each nominal value a new attribute is created, <br/>	not only if
            there are more than 2 values.<br/>-R &lt;col1,col2-col4,...&gt; = 	Specifies
            list of columns to act on. First and last are <br/>	valid
            indexes.<br/>	(default: first-last)<br/>-V = 	Invert matching sense of column indexes.
            </summary>
        </member>
        <member name="M:Ml2.Fltr.UnsupervisedAttributeFilters.NominalToString">
            <summary>
            Converts a nominal attribute (that is, a set number of values) to string
            (that is, an unspecified number of values).<br/><br/>Options:<br/><br/>-C
            &lt;col&gt; = 	Sets the range of attributes to convert (default last).
            </summary>
        </member>
        <member name="M:Ml2.Fltr.UnsupervisedAttributeFilters.Normalize">
            <summary>
            Normalizes all numeric values in the given dataset (apart from the class
            attribute, if set). The resulting values are by default in [0,1] for the
            data used to compute the normalization intervals. But with the scale and
            translation parameters one can change that, e.g., with scale = 2.0 and
            translation = -1.0 you get values in the range
            [-1,+1].<br/><br/>Options:<br/><br/>-unset-class-temporarily = 	Unsets the class index temporarily before the
            filter is<br/>	applied to the data.<br/>	(default: no)<br/>-S &lt;num&gt; =
            	The scaling factor for the output range.<br/>	(default: 1.0)<br/>-T
            &lt;num&gt; = 	The translation of the output range.<br/>	(default: 0.0)
            </summary>
        </member>
        <member name="M:Ml2.Fltr.UnsupervisedAttributeFilters.NumericCleaner">
            <summary>
            A filter that 'cleanses' the numeric data from values that are too small,
            too big or very close to a certain value (e.g., 0) and sets these values
            to a pre-defined default.<br/><br/>Options:<br/><br/>-D = 	Turns on output
            of debugging information.<br/>-min &lt;double&gt; = 	The minimum threshold.
            (default -Double.MAX_VALUE)<br/>-min-default &lt;double&gt; = 	The
            replacement for values smaller than the minimum threshold.<br/>	(default
            -Double.MAX_VALUE)<br/>-max &lt;double&gt; = 	The maximum threshold. (default
            Double.MAX_VALUE)<br/>-max-default &lt;double&gt; = 	The replacement for values
            larger than the maximum threshold.<br/>	(default
            Double.MAX_VALUE)<br/>-closeto &lt;double&gt; = 	The number values are checked for closeness. (default
            0)<br/>-closeto-default &lt;double&gt; = 	The replacement for values that
            are close to '-closeto'.<br/>	(default 0)<br/>-closeto-tolerance
            &lt;double&gt; = 	The tolerance below which numbers are considered being close to
            <br/>	to each other. (default 1E-6)<br/>-decimals &lt;int&gt; = 	The number of
            decimals to round to, -1 means no rounding at all.<br/>	(default -1)<br/>-R
            &lt;col1,col2,...&gt; = 	The list of columns to cleanse, e.g., first-last
            or first-3,5-last.<br/>	(default first-last)<br/>-V = 	Inverts the matching
            sense.<br/>-include-class = 	Whether to include the class in the
            cleansing.<br/>	The class column will always be skipped, if this flag is
            not<br/>	present. (default no)
            </summary>
        </member>
        <member name="M:Ml2.Fltr.UnsupervisedAttributeFilters.NumericToBinary">
            <summary>
            Converts all numeric attributes into binary attributes (apart from the
            class attribute, if set): if the value of the numeric attribute is exactly
            zero, the value of the new attribute will be zero. If the value of the
            numeric attribute is missing, the value of the new attribute will be missing.
            Otherwise, the value of the new attribute will be one. The new attributes will
            be nominal.<br/><br/>Options:<br/><br/>-unset-class-temporarily = 	Unsets
            the class index temporarily before the filter is<br/>	applied to the
            data.<br/>	(default: no)
            </summary>
        </member>
        <member name="M:Ml2.Fltr.UnsupervisedAttributeFilters.NumericToNominal">
            <summary>
            A filter for turning numeric attributes into nominal ones. Unlike
            discretization, it just takes all numeric values and adds them to the list of
            nominal values of that attribute. Useful after CSV imports, to enforce certain
            attributes to become nominal, e.g., the class attribute, containing values
            from 1 to 5.<br/><br/>Options:<br/><br/>-R &lt;col1,col2-col4,...&gt; =
            	Specifies list of columns to Discretize. First and last are valid
            indexes.<br/>	(default: first-last)<br/>-V = 	Invert matching sense of column indexes.
            </summary>
        </member>
        <member name="M:Ml2.Fltr.UnsupervisedAttributeFilters.NumericTransform">
            <summary>
            Transforms numeric attributes using a given transformation
            method.<br/><br/>Options:<br/><br/>-R &lt;index1,index2-index4,...&gt; = 	Specify list of
            columns to transform. First and last are<br/>	valid indexes (default
            none). Non-numeric columns are <br/>	skipped.<br/>-V = 	Invert matching
            sense.<br/>-C &lt;string&gt; = 	Sets the class containing transformation
            method.<br/>	(default java.lang.Math)<br/>-M &lt;string&gt; = 	Sets the method.
            (default abs)
            </summary>
        </member>
        <member name="M:Ml2.Fltr.UnsupervisedAttributeFilters.Obfuscate">
            <summary>
            A simple instance filter that renames the relation, all attribute names
            and all nominal (and string) attribute values. For exchanging sensitive
            datasets. Currently doesn't like string or relational attributes.
            </summary>
        </member>
        <member name="M:Ml2.Fltr.UnsupervisedAttributeFilters.PKIDiscretize">
            <summary>
            Discretizes numeric attributes using equal frequency binning, where the
            number of bins is equal to the square root of the number of non-missing
            values.<br/><br/>For more information, see:<br/><br/>Ying Yang, Geoffrey I.
            Webb: Proportional k-Interval Discretization for Naive-Bayes Classifiers. In:
            12th European Conference on Machine Learning, 564-575,
            2001.<br/><br/>Options:<br/><br/>-unset-class-temporarily = 	Unsets the class index temporarily
            before the filter is<br/>	applied to the data.<br/>	(default: no)<br/>-R
            &lt;col1,col2-col4,...&gt; = 	Specifies list of columns to Discretize. First
            and last are valid indexes.<br/>	(default: first-last)<br/>-V = 	Invert
            matching sense of column indexes.<br/>-D = 	Output binary attributes for
            discretized attributes.
            </summary>
        </member>
        <member name="M:Ml2.Fltr.UnsupervisedAttributeFilters.PartitionMembership">
            <summary>
            A filter that uses a PartitionGenerator to generate partition membership
            values; filtered instances are composed of these values plus the class
            attribute (if set in the input data) and rendered as sparse
            instances.<br/><br/>Options:<br/><br/>-W &lt;name of partition generator&gt; = 	Full name of
            partition generator to use,
            e.g.:<br/>		weka.classifiers.trees.J48<br/>	Additional options after the '--'.<br/>	(default: weka.classifiers.trees.J48)
            </summary>
        </member>
        <member name="M:Ml2.Fltr.UnsupervisedAttributeFilters.PartitionedMultiFilter">
            <summary>
            A filter that applies filters on subsets of attributes and assembles the
            output into a new dataset. Attributes that are not covered by any of the
            ranges can be either retained or removed from the
            output.<br/><br/>Options:<br/><br/>-D = 	Turns on output of debugging information.<br/>-F
            &lt;classname [options]&gt; = 	A filter to apply (can be specified multiple
            times).<br/>-R &lt;range&gt; = 	An attribute range (can be specified multiple
            times).<br/>	For each filter a range must be supplied. 'first' and 'last'<br/>	are
            valid indices. 'inv(...)' around the range denotes an<br/>	inverted
            range.<br/>-U = 	Flag for leaving unused attributes out of the output, by
            default<br/>	these are included in the filter output.
            </summary>
        </member>
        <member name="M:Ml2.Fltr.UnsupervisedAttributeFilters.PrincipalComponents">
            <summary>
            Performs a principal components analysis and transformation of the
            data.<br/>Dimensionality reduction is accomplished by choosing enough
            eigenvectors to account for some percentage of the variance in the original data --
            default 0.95 (95%).<br/>Based on code of the attribute selection scheme
            'PrincipalComponents' by Mark Hall and Gabi
            Schmidberger.<br/><br/>Options:<br/><br/>-C = 	Center (rather than standardize) the<br/>	data and compute PCA
            using the covariance (rather<br/>	 than the correlation) matrix.<br/>-R
            &lt;num&gt; = 	Retain enough PC attributes to account<br/>	for this proportion
            of variance in the original data.<br/>	(default: 0.95)<br/>-A &lt;num&gt; =
            	Maximum number of attributes to include in <br/>	transformed attribute
            names.<br/>	(-1 = include all, default: 5)<br/>-M &lt;num&gt; = 	Maximum
            number of PC attributes to retain.<br/>	(-1 = include all, default: -1)
            </summary>
        </member>
        <member name="M:Ml2.Fltr.UnsupervisedAttributeFilters.RandomProjection">
            <summary>
            Reduces the dimensionality of the data by projecting it onto a lower
            dimensional subspace using a random matrix with columns of unit length (i.e. It
            will reduce the number of attributes in the data while preserving much of
            its variation like PCA, but at a much less computational cost).<br/>It
            first applies the NominalToBinary filter to convert all attributes to numeric
            before reducing the dimension. It preserves the class
            attribute.<br/><br/>For more information, see:<br/><br/>Dmitriy Fradkin, David Madigan:
            Experiments with random projections for machine learning. In: KDD '03: Proceedings
            of the ninth ACM SIGKDD international conference on Knowledge discovery and
            data mining, New York, NY, USA, 517-522, 003.<br/><br/>Options:<br/><br/>-N
            &lt;number&gt; = 	The number of dimensions (attributes) the data should be
            reduced to<br/>	(default 10; exclusive of the class attribute, if it is
            set).<br/>-D [SPARSE1|SPARSE2|GAUSSIAN] = 	The distribution to use for
            calculating the random matrix.<br/>	Sparse1 is:<br/>	 sqrt(3)*{-1 with prob(1/6),
            0 with prob(2/3), +1 with prob(1/6)}<br/>	Sparse2 is:<br/>	 {-1 with
            prob(1/2), +1 with prob(1/2)}<br/><br/>-P &lt;percent&gt; = 	The percentage of
            dimensions (attributes) the data should<br/>	be reduced to (exclusive of the
            class attribute, if it is set). This -N<br/>	option is ignored if this
            option is present or is greater<br/>	than zero.<br/>-M = 	Replace missing
            values using the ReplaceMissingValues filter<br/>-R &lt;num&gt; = 	The random
            seed for the random number generator used for<br/>	calculating the random
            matrix (default 42).
            </summary>
        </member>
        <!-- Badly formed XML comment ignored for member "M:Ml2.Fltr.UnsupervisedAttributeFilters.RandomSubset" -->
        <member name="M:Ml2.Fltr.UnsupervisedAttributeFilters.Remove">
            <summary>
            A filter that removes a range of attributes from the dataset. Will
            re-order the remaining attributes if invert matching sense is turned on and the
            attribute column indices are not specified in ascending
            order.<br/><br/>Options:<br/><br/>-R &lt;index1,index2-index4,...&gt; = 	Specify list of
            columns to delete. First and last are valid<br/>	indexes. (default none)<br/>-V =
            	Invert matching sense (i.e. only keep specified columns)
            </summary>
        </member>
        <member name="M:Ml2.Fltr.UnsupervisedAttributeFilters.RemoveByName">
            <summary>
            Removes attributes based on a regular expression matched against their
            names.<br/><br/>Options:<br/><br/>-D = 	Turns on output of debugging
            information.<br/>-E &lt;regular expression&gt; = 	The regular expression to match
            the attribute names against.<br/>	(default: ^.*id$)<br/>-V = 	Flag for
            inverting the matching sense. If set, attributes are kept<br/>	instead of
            deleted.<br/>	(default: off)
            </summary>
        </member>
        <member name="M:Ml2.Fltr.UnsupervisedAttributeFilters.RemoveType">
            <summary>
            Removes attributes of a given type.<br/><br/>Options:<br/><br/>-T
            &lt;nominal|numeric|string|date|relational&gt; = 	Attribute type to delete. Valid
            options are "nominal", <br/>	"numeric", "string", "date" and
            "relational".<br/>	(default "string")<br/>-V = 	Invert matching sense (i.e. only keep
            specified columns)
            </summary>
        </member>
        <member name="M:Ml2.Fltr.UnsupervisedAttributeFilters.RemoveUseless">
            <summary>
            This filter removes attributes that do not vary at all or that vary too
            much. All constant attributes are deleted automatically, along with any that
            exceed the maximum percentage of variance parameter. The maximum variance
            test is only applied to nominal attributes.<br/><br/>Options:<br/><br/>-M
            &lt;max variance %&gt; = 	Maximum variance percentage allowed (default 99)
            </summary>
        </member>
        <!-- Badly formed XML comment ignored for member "M:Ml2.Fltr.UnsupervisedAttributeFilters.RenameAttribute" -->
        <member name="M:Ml2.Fltr.UnsupervisedAttributeFilters.Reorder">
            <summary>
            A filter that generates output with a new order of the attributes. Useful
            if one wants to move an attribute to the end to use it as class attribute
            (e.g. with using "-R 2-last,1").<br/>But it's not only possible to change
            the order of all the attributes, but also to leave out attributes. E.g. if
            you have 10 attributes, you can generate the following output order:
            1,3,5,7,9,10 or 10,1-5.<br/>You can also duplicate attributes, e.g. for further
            processing later on: e.g. 1,1,1,4,4,4,2,2,2 where the second and the third
            column of each attribute are processed differently and the first one, i.e.
            the original one is kept.<br/>One can simply inverse the order of the
            attributes via 'last-first'.<br/>After appyling the filter, the index of the class
            attribute is the last attribute.<br/><br/>Options:<br/><br/>-R
            &lt;index1,index2-index4,...&gt; = 	Specify list of columns to copy. First and last
            are valid<br/>	indexes. (default first-last)
            </summary>
        </member>
        <member name="M:Ml2.Fltr.UnsupervisedAttributeFilters.ReplaceMissingValues">
            <summary>
            Replaces all missing values for nominal and numeric attributes in a
            dataset with the modes and means from the training
            data.<br/><br/>Options:<br/><br/>-unset-class-temporarily = 	Unsets the class index temporarily before
            the filter is<br/>	applied to the data.<br/>	(default: no)
            </summary>
        </member>
        <member name="M:Ml2.Fltr.UnsupervisedAttributeFilters.ReplaceMissingWithUserConstant">
            <summary>
            Replaces all missing values for nominal, string, numeric and date
            attributes in the dataset with user-supplied constant
            values.<br/><br/>Options:<br/><br/>-A &lt;index1,index2-index4,... | att-name1,att-name2,...&gt; =
            	Specify list of attributes to replace missing values for <br/>	(as weka range
            list of indices or a comma separated list of attribute
            names).<br/>	(default: consider all attributes)<br/>-N = 	Specify the replacement constant for
            nominal/string attributes<br/>-R = 	Specify the replacement constant for
            numeric attributes<br/>	(default: 0)<br/>-D = 	Specify the replacement
            constant for date attributes<br/>-F = 	Specify the date format for parsing the
            replacement date constant<br/>	(default:
            yyyy-MM-dd'T'HH:mm:ss)<br/>-unset-class-temporarily = 	Unsets the class index temporarily before the filter
            is<br/>	applied to the data.<br/>	(default: no)
            </summary>
        </member>
        <member name="M:Ml2.Fltr.UnsupervisedAttributeFilters.SortLabels">
            <summary>
            A simple filter for sorting the labels of nominal
            attributes.<br/><br/>Options:<br/><br/>-D = 	Turns on output of debugging information.<br/>-R
            &lt;index1,index2-index4,...&gt; = 	Specify list of string attributes to
            convert to words.<br/>	(default: select all relational attributes)<br/>-V =
            	Inverts the matching sense of the selection.<br/>-S &lt;CASE|NON-CASE&gt; =
            	Determines the type of sorting:<br/>	CASE = Case-sensitive<br/>	NON-CASE =
            Case-insensitive<br/>	(default: CASE)
            </summary>
        </member>
        <member name="M:Ml2.Fltr.UnsupervisedAttributeFilters.Standardize">
            <summary>
            Standardizes all numeric attributes in the given dataset to have zero
            mean and unit variance (apart from the class attribute, if
            set).<br/><br/>Options:<br/><br/>-unset-class-temporarily = 	Unsets the class index
            temporarily before the filter is<br/>	applied to the data.<br/>	(default: no)
            </summary>
        </member>
        <member name="M:Ml2.Fltr.UnsupervisedAttributeFilters.StringToNominal">
            <summary>
            Converts a range of string attributes (unspecified number of values) to
            nominal (set number of values). You should ensure that all string values
            that will appear are represented in the first batch of the
            data.<br/><br/>Options:<br/><br/>-R &lt;col&gt; = 	Sets the range of attribute indices
            (default last).<br/>-V &lt;col&gt; = 	Invert the range specified by -R.
            </summary>
        </member>
        <member name="M:Ml2.Fltr.UnsupervisedAttributeFilters.StringToWordVector">
            <summary>
            Converts String attributes into a set of attributes representing word
            occurrence (depending on the tokenizer) information from the text contained in
            the strings. The set of words (attributes) is determined by the first
            batch filtered (typically training data).<br/><br/>Options:<br/><br/>-C =
            	Output word counts rather than boolean word presence.<br/><br/>-R
            &lt;index1,index2-index4,...&gt; = 	Specify list of string attributes to convert to
            words (as weka Range).<br/>	(default: select all string attributes)<br/>-V =
            	Invert matching sense of column indexes.<br/>-P &lt;attribute name
            prefix&gt; = 	Specify a prefix for the created attribute names.<br/>	(default:
            "")<br/>-W &lt;number of words to keep&gt; = 	Specify approximate number of word
            fields to create.<br/>	Surplus words will be discarded..<br/>	(default:
            1000)<br/>-prune-rate &lt;rate as a percentage of dataset&gt; = 	Specify the
            rate (e.g., every 10% of the input dataset) at which to periodically prune
            the dictionary.<br/>	-W prunes after creating a full dictionary. You may
            not have enough memory for this approach.<br/>	(default: no periodic
            pruning)<br/>-T = 	Transform the word frequencies into log(1+fij)<br/>	where fij is
            the frequency of word i in jth document(instance).<br/><br/>-I =
            	Transform each word frequency into:<br/>	fij*log(num of Documents/num of documents
            containing word i)<br/>	 where fij if frequency of word i in jth
            document(instance)<br/>-N = 	Whether to 0=not normalize/1=normalize all
            data/2=normalize test data only<br/>	to average length of training documents (default
            0=don't normalize).<br/>-L = 	Convert all tokens to lowercase before adding
            to the dictionary.<br/>-S = 	Ignore words that are in the
            stoplist.<br/>-stemmer &lt;spec&gt; = 	The stemmering algorihtm (classname plus parameters)
            to use.<br/>-M &lt;int&gt; = 	The minimum term frequency (default =
            1).<br/>-O = 	If this is set, the maximum number of words and the <br/>	minimum
            term frequency is not enforced on a per-class <br/>	basis but based on the
            documents in all the classes <br/>	(even if a class attribute is
            set).<br/>-stopwords &lt;file&gt; = 	A file containing stopwords to override the default
            ones.<br/>	Using this option automatically sets the flag ('-S') to use
            the<br/>	stoplist if the file exists.<br/>	Format: one stopword per line,
            lines starting with '#'<br/>	are interpreted as comments and
            ignored.<br/>-tokenizer &lt;spec&gt; = 	The tokenizing algorihtm (classname plus parameters)
            to use.<br/>	(default: weka.core.tokenizers.WordTokenizer)
            </summary>
        </member>
        <member name="M:Ml2.Fltr.UnsupervisedAttributeFilters.SwapValues">
            <summary>
            Swaps two values of a nominal attribute.<br/><br/>Options:<br/><br/>-C
            &lt;col&gt; = 	Sets the attribute index (default last).<br/>-F &lt;value
            index&gt; = 	Sets the first value's index (default first).<br/>-S &lt;value
            index&gt; = 	Sets the second value's index (default last).
            </summary>
        </member>
        <member name="M:Ml2.Fltr.UnsupervisedAttributeFilters.TimeSeriesDelta">
            <summary>
            An instance filter that assumes instances form time-series data and
            replaces attribute values in the current instance with the difference between
            the current value and the equivalent attribute attribute value of some
            previous (or future) instance. For instances where the time-shifted value is
            unknown either the instance may be dropped, or missing values used. Skips the
            class attribute if it is set.<br/><br/>Options:<br/><br/>-R
            &lt;index1,index2-index4,...&gt; = 	Specify list of columns to translate in time. First
            and<br/>	last are valid indexes. (default none)<br/>-V = 	Invert matching
            sense (i.e. calculate for all non-specified columns)<br/>-I &lt;num&gt; = 	The
            number of instances forward to translate values<br/>	between. A negative
            number indicates taking values from<br/>	a past instance. (default -1)<br/>-M
            = 	For instances at the beginning or end of the dataset where<br/>	the
            translated values are not known, remove those instances<br/>	(default is to
            use missing values).
            </summary>
        </member>
        <member name="M:Ml2.Fltr.UnsupervisedAttributeFilters.TimeSeriesTranslate">
            <summary>
            An instance filter that assumes instances form time-series data and
            replaces attribute values in the current instance with the equivalent attribute
            values of some previous (or future) instance. For instances where the
            desired value is unknown either the instance may be dropped, or missing values
            used. Skips the class attribute if it is set.<br/><br/>Options:<br/><br/>-R
            &lt;index1,index2-index4,...&gt; = 	Specify list of columns to translate in
            time. First and<br/>	last are valid indexes. (default none)<br/>-V =
            	Invert matching sense (i.e. calculate for all non-specified columns)<br/>-I
            &lt;num&gt; = 	The number of instances forward to translate
            values<br/>	between. A negative number indicates taking values from<br/>	a past instance.
            (default -1)<br/>-M = 	For instances at the beginning or end of the dataset
            where<br/>	the translated values are not known, remove those
            instances<br/>	(default is to use missing values).
            </summary>
        </member>
        <member name="M:Ml2.Fltr.UnsupervisedInstanceFilters.NonSparseToSparse">
            <summary>
            An instance filter that converts all incoming instances into sparse
            format.<br/><br/>Options:<br/><br/>-M = 	Treat missing values as zero.<br/>-F =
            	Add a dummy first value for nominal attributes.
            </summary>
        </member>
        <member name="M:Ml2.Fltr.UnsupervisedInstanceFilters.Randomize">
            <summary>
            Randomly shuffles the order of instances passed through it. The random
            number generator is reset with the seed value whenever a new set of instances
            is passed in.<br/><br/>Options:<br/><br/>-S &lt;num&gt; = 	Specify the
            random number seed (default 42)
            </summary>
        </member>
        <member name="M:Ml2.Fltr.UnsupervisedInstanceFilters.RemoveFolds">
            <summary>
            This filter takes a dataset and outputs a specified fold for cross
            validation. If you want the folds to be stratified use the supervised
            version.<br/><br/>Options:<br/><br/>-V = 	Specifies if inverse of selection is to be
            output.<br/><br/>-N &lt;number of folds&gt; = 	Specifies number of folds
            dataset is split into. <br/>	(default 10)<br/><br/>-F &lt;fold&gt; =
            	Specifies which fold is selected. (default 1)<br/><br/>-S &lt;seed&gt; = 	Specifies
            random number seed. (default 0, no randomizing)<br/>
            </summary>
        </member>
        <member name="M:Ml2.Fltr.UnsupervisedInstanceFilters.RemoveFrequentValues">
            <summary>
            Determines which values (frequent or infrequent ones) of an (nominal)
            attribute are retained and filters the instances accordingly. In case of
            values with the same frequency, they are kept in the way they appear in the
            original instances object. E.g. if you have the values "1,2,3,4" with the
            frequencies "10,5,5,3" and you chose to keep the 2 most common values, the
            values "1,2" would be returned, since the value "2" comes before "3", even
            though they have the same frequency.<br/><br/>Options:<br/><br/>-C &lt;num&gt; =
            	Choose attribute to be used for selection.<br/>-N &lt;num&gt; = 	Number
            of values to retain for the sepcified attribute, <br/>	i.e. the ones with
            the most instances (default 2).<br/>-L = 	Instead of values with the most
            instances the ones with the <br/>	least are retained.<br/><br/>-H = 	When
            selecting on nominal attributes, removes header<br/>	references to excluded
            values.<br/>-V = 	Invert matching sense.
            </summary>
        </member>
        <!-- Badly formed XML comment ignored for member "M:Ml2.Fltr.UnsupervisedInstanceFilters.RemoveMisclassified" -->
        <member name="M:Ml2.Fltr.UnsupervisedInstanceFilters.RemovePercentage">
            <summary>
            A filter that removes a given percentage of a
            dataset.<br/><br/>Options:<br/><br/>-P &lt;percentage&gt; = 	Specifies percentage of instances to
            select. (default 50)<br/><br/>-V = 	Specifies if inverse of selection is to be
            output.<br/>
            </summary>
        </member>
        <member name="M:Ml2.Fltr.UnsupervisedInstanceFilters.RemoveRange">
            <summary>
            A filter that removes a given range of instances of a
            dataset.<br/><br/>Options:<br/><br/>-R &lt;inst1,inst2-inst4,...&gt; = 	Specifies list of
            instances to select. First and last<br/>	are valid indexes.
            (required)<br/><br/>-V = 	Specifies if inverse of selection is to be output.<br/>
            </summary>
        </member>
        <member name="M:Ml2.Fltr.UnsupervisedInstanceFilters.RemoveWithValues">
            <summary>
            Filters instances according to the value of an
            attribute.<br/><br/>Options:<br/><br/>-C &lt;num&gt; = 	Choose attribute to be used for
            selection.<br/>-S &lt;num&gt; = 	Numeric value to be used for selection on
            numeric<br/>	attribute.<br/>	Instances with values smaller than given value will<br/>	be
            selected. (default 0)<br/>-L &lt;index1,index2-index4,...&gt; = 	Range of
            label indices to be used for selection on<br/>	nominal
            attribute.<br/>	First and last are valid indexes. (default all values)<br/>-M = 	Missing values
            count as a match. This setting is<br/>	independent of the -V
            option.<br/>	(default missing values don't match)<br/>-V = 	Invert matching
            sense.<br/>-H = 	When selecting on nominal attributes, removes header<br/>	references
            to excluded values.<br/>-F = 	Do not apply the filter to instances that
            arrive after the first<br/>	(training) batch. The default is to apply the
            filter (i.e.<br/>	the filter may not return an instance if it matches the remove
            criteria)
            </summary>
        </member>
        <member name="M:Ml2.Fltr.UnsupervisedInstanceFilters.Resample">
            <summary>
            Produces a random subsample of a dataset using either sampling with
            replacement or without replacement. The original dataset must fit entirely in
            memory. The number of instances in the generated dataset may be specified.
            When used in batch mode, subsequent batches are NOT
            resampled.<br/><br/>Options:<br/><br/>-S &lt;num&gt; = 	Specify the random number seed (default
            1)<br/>-Z &lt;num&gt; = 	The size of the output dataset, as a percentage
            of<br/>	the input dataset (default 100)<br/>-no-replacement = 	Disables
            replacement of instances<br/>	(default: with replacement)<br/>-V = 	Inverts the
            selection - only available with '-no-replacement'.
            </summary>
        </member>
        <member name="M:Ml2.Fltr.UnsupervisedInstanceFilters.ReservoirSample">
            <summary>
            Produces a random subsample of a dataset using the reservoir sampling
            Algorithm "R" by Vitter. The original data set does not have to fit into main
            memory, but the reservoir does. <br/><br/>Options:<br/><br/>-S &lt;num&gt;
            = 	Specify the random number seed (default 1)<br/>-Z &lt;num&gt; = 	The
            size of the output dataset - number of instances<br/>	(default 100)
            </summary>
        </member>
        <member name="M:Ml2.Fltr.UnsupervisedInstanceFilters.SparseToNonSparse">
            <summary>
            An instance filter that converts all incoming sparse instances into
            non-sparse format.
            </summary>
        </member>
        <!-- Badly formed XML comment ignored for member "M:Ml2.Fltr.UnsupervisedInstanceFilters.SubsetByExpression" -->
    </members>
</doc>
